{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "737fb0c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: sentencepiece 0.2.1\n",
      "Uninstalling sentencepiece-0.2.1:\n",
      "  Successfully uninstalled sentencepiece-0.2.1\n",
      "Collecting sentencepiece==0.1.99\n",
      "  Downloading sentencepiece-0.1.99-cp310-cp310-win_amd64.whl.metadata (8.3 kB)\n",
      "Downloading sentencepiece-0.1.99-cp310-cp310-win_amd64.whl (977 kB)\n",
      "   ---------------------------------------- 0.0/977.5 kB ? eta -:--:--\n",
      "   --------------------- ------------------ 524.3/977.5 kB 5.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 977.5/977.5 kB 5.7 MB/s  0:00:00\n",
      "Installing collected packages: sentencepiece\n",
      "Successfully installed sentencepiece-0.1.99\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.2 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip uninstall -y sentencepiece\n",
    "!pip install sentencepiece==0.1.99\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e1076992",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Python executable: c:\\Users\\thong\\AppData\\Local\\Programs\\Python\\Python310\\python.exe\n",
      ">>> Python version: 3.10.11 (tags/v3.10.11:7d4cc5a, Apr  5 2023, 00:38:17) [MSC v.1929 64 bit (AMD64)]\n",
      ">>> SentencePiece loaded OK\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(\">>> Python executable:\", sys.executable)\n",
    "print(\">>> Python version:\", sys.version)\n",
    "try:\n",
    "    import sentencepiece as spm\n",
    "    print(\">>> SentencePiece loaded OK\")\n",
    "except:\n",
    "    print(\">>> SentencePiece NOT FOUND\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1bce4f06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "üìå Testing BARTpho\n",
      "==================================================\n",
      "Loading BARTpho from: C:\\Users\\thong\\workplace\\IUH\\4RD_YEAR_COLLEGE\\semester_1\\deep learnning\\CK\\kaagle_Truong\\results\\models\\bartpho\\final\n",
      "‚ùå Error: \n",
      "BartphoTokenizer requires the SentencePiece library but it was not found in your environment. Checkout the instructions on the\n",
      "installation page of its repo: https://github.com/google/sentencepiece#installation and follow the ones\n",
      "that match your environment. Please note that you may need to restart your runtime after installation.\n",
      "\n",
      "\n",
      "==================================================\n",
      "üìå Testing mT5\n",
      "==================================================\n",
      "Loading mT5 from: C:\\Users\\thong\\workplace\\IUH\\4RD_YEAR_COLLEGE\\semester_1\\deep learnning\\CK\\kaagle_Truong\\results\\models\\mt5-base\\final\n",
      "‚ùå Error: \n",
      "MT5Tokenizer requires the SentencePiece library but it was not found in your environment. Checkout the instructions on the\n",
      "installation page of its repo: https://github.com/google/sentencepiece#installation and follow the ones\n",
      "that match your environment. Please note that you may need to restart your runtime after installation.\n",
      "\n",
      "\n",
      "==================================================\n",
      "üìå Testing ViT5 Advanced\n",
      "==================================================\n",
      "Loading ViT5 from: C:\\Users\\thong\\workplace\\IUH\\4RD_YEAR_COLLEGE\\semester_1\\deep learnning\\CK\\kaagle_Truong\\results\\models\\vit5-base\\final\n",
      "‚ùå Error: \n",
      "T5Tokenizer requires the SentencePiece library but it was not found in your environment. Checkout the instructions on the\n",
      "installation page of its repo: https://github.com/google/sentencepiece#installation and follow the ones\n",
      "that match your environment. Please note that you may need to restart your runtime after installation.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "from transformers import (\n",
    "    T5ForConditionalGeneration, T5Tokenizer,\n",
    "    MT5ForConditionalGeneration, MT5Tokenizer,\n",
    "    AutoModelForSeq2SeqLM, AutoTokenizer\n",
    ")\n",
    "from peft import PeftModel\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "import os\n",
    "\n",
    "# ========================================\n",
    "# ƒê∆Ø·ªúNG D·∫™N ƒê√öNG THEO H√åNH C·ª¶A B·∫†N\n",
    "# ========================================\n",
    "MODEL_DIR = r\"C:\\Users\\thong\\workplace\\IUH\\4RD_YEAR_COLLEGE\\semester_1\\deep learnning\\CK\\kaagle_Truong\\results\\models\"\n",
    "\n",
    "BARTPHO_PATH = os.path.join(MODEL_DIR, \"bartpho\", \"final\")\n",
    "MT5_PATH = os.path.join(MODEL_DIR, \"mt5-base\", \"final\")\n",
    "VIT5_PATH = os.path.join(MODEL_DIR, \"vit5-base\", \"final\")   # <-- FIXED\n",
    "\n",
    "\n",
    "# ========================================\n",
    "# 1. LOAD BARTPHO (ƒê∆°n gi·∫£n nh·∫•t - ƒê√É ƒê·ª¶ FILE)\n",
    "# ========================================\n",
    "def load_bartpho(model_path):\n",
    "    \"\"\"Load BARTpho model t·ª´ local\"\"\"\n",
    "    print(f\"Loading BARTpho from: {model_path}\")\n",
    "    \n",
    "    # Ki·ªÉm tra th∆∞ m·ª•c t·ªìn t·∫°i\n",
    "    if not os.path.exists(model_path):\n",
    "        raise FileNotFoundError(f\"Kh√¥ng t√¨m th·∫•y th∆∞ m·ª•c: {model_path}\")\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path, local_files_only=True)\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "        model_path,\n",
    "        local_files_only=True,\n",
    "        torch_dtype=torch.float32  # Windows th∆∞·ªùng d√πng float32\n",
    "    )\n",
    "    \n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    print(f\"‚úÖ BARTpho loaded on {device}\")\n",
    "    return model, tokenizer\n",
    "\n",
    "\n",
    "# ========================================\n",
    "# 2. LOAD mT5-Base (ƒê√É ƒê·ª¶ FILE)\n",
    "# ========================================\n",
    "def load_mt5(model_path):\n",
    "    \"\"\"Load mT5 model t·ª´ local\"\"\"\n",
    "    print(f\"Loading mT5 from: {model_path}\")\n",
    "    \n",
    "    if not os.path.exists(model_path):\n",
    "        raise FileNotFoundError(f\"Kh√¥ng t√¨m th·∫•y th∆∞ m·ª•c: {model_path}\")\n",
    "    \n",
    "    tokenizer = MT5Tokenizer.from_pretrained(model_path, local_files_only=True)\n",
    "    model = MT5ForConditionalGeneration.from_pretrained(\n",
    "        model_path,\n",
    "        local_files_only=True,\n",
    "        torch_dtype=torch.float32\n",
    "    )\n",
    "    \n",
    "    # Resize n·∫øu vocab kh√¥ng kh·ªõp\n",
    "    if len(tokenizer) != model.config.vocab_size:\n",
    "        model.resize_token_embeddings(len(tokenizer))\n",
    "    \n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    print(f\"‚úÖ mT5 loaded on {device}\")\n",
    "    return model, tokenizer\n",
    "\n",
    "\n",
    "# ========================================\n",
    "# 3. LOAD ViT5 (C·∫¶N LOAD BASE + LoRA + Custom layers)\n",
    "# ========================================\n",
    "\n",
    "# Custom architecture classes (gi·ªØ nguy√™n nh∆∞ tr∆∞·ªõc)\n",
    "class CustomMultiHeadAttention(nn.Module):\n",
    "    def __init__(self, hidden_size, num_heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = hidden_size // num_heads\n",
    "        self.query = nn.Linear(hidden_size, hidden_size)\n",
    "        self.key = nn.Linear(hidden_size, hidden_size)\n",
    "        self.value = nn.Linear(hidden_size, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, hidden_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.scale = math.sqrt(self.head_dim)\n",
    "        \n",
    "    def forward(self, hidden_states, attention_mask=None):\n",
    "        batch_size, seq_length, _ = hidden_states.size()\n",
    "        Q = self.query(hidden_states).view(batch_size, seq_length, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        K = self.key(hidden_states).view(batch_size, seq_length, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        V = self.value(hidden_states).view(batch_size, seq_length, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / self.scale\n",
    "        if attention_mask is not None:\n",
    "            scores = scores.masked_fill(attention_mask == 0, -1e4)\n",
    "        attn_weights = torch.softmax(scores, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "        context = torch.matmul(attn_weights, V).transpose(1, 2).contiguous().view(batch_size, seq_length, self.hidden_size)\n",
    "        return self.out(context), attn_weights\n",
    "\n",
    "class BiLSTMLayer(nn.Module):\n",
    "    def __init__(self, hidden_size, num_layers=2, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(hidden_size, hidden_size // 2, num_layers, \n",
    "                           dropout=dropout if num_layers > 1 else 0, bidirectional=True, batch_first=True)\n",
    "        self.layer_norm = nn.LayerNorm(hidden_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    def forward(self, hidden_states):\n",
    "        lstm_out, _ = self.lstm(hidden_states)\n",
    "        return self.layer_norm(hidden_states + self.dropout(lstm_out))\n",
    "\n",
    "class RNNLayer(nn.Module):\n",
    "    def __init__(self, hidden_size, num_layers=1, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.rnn = nn.RNN(hidden_size, hidden_size, num_layers, \n",
    "                         dropout=dropout if num_layers > 1 else 0, batch_first=True)\n",
    "        self.layer_norm = nn.LayerNorm(hidden_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    def forward(self, hidden_states):\n",
    "        rnn_out, _ = self.rnn(hidden_states)\n",
    "        return self.layer_norm(hidden_states + self.dropout(rnn_out))\n",
    "\n",
    "class FeedForwardNetwork(nn.Module):\n",
    "    def __init__(self, hidden_size, ff_size=None, dropout=0.1):\n",
    "        super().__init__()\n",
    "        ff_size = ff_size or hidden_size * 4\n",
    "        self.fc1 = nn.Linear(hidden_size, ff_size)\n",
    "        self.fc2 = nn.Linear(ff_size, hidden_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.layer_norm = nn.LayerNorm(hidden_size)\n",
    "    def forward(self, hidden_states):\n",
    "        residual = hidden_states\n",
    "        x = torch.nn.functional.gelu(self.fc1(hidden_states))\n",
    "        x = self.dropout(self.fc2(self.dropout(x)))\n",
    "        return self.layer_norm(residual + x)\n",
    "\n",
    "class T5WithAdvancedArchitecture(nn.Module):\n",
    "    def __init__(self, base_model, hidden_size=768, num_heads=12):\n",
    "        super().__init__()\n",
    "        self.base_model = base_model\n",
    "        self.config = base_model.config\n",
    "        self.multi_head_attention = CustomMultiHeadAttention(hidden_size, num_heads)\n",
    "        self.bilstm = BiLSTMLayer(hidden_size, num_layers=2)\n",
    "        self.rnn = RNNLayer(hidden_size, num_layers=1)\n",
    "        self.ffn = FeedForwardNetwork(hidden_size)\n",
    "        self.layer_norm1 = nn.LayerNorm(hidden_size)\n",
    "        self.layer_norm3 = nn.LayerNorm(hidden_size)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.gate = nn.Linear(hidden_size * 3, 3)\n",
    "        \n",
    "    def generate(self, *args, **kwargs):\n",
    "        return self.base_model.generate(*args, **kwargs)\n",
    "\n",
    "\n",
    "def load_vit5_advanced(adapter_path):\n",
    "    \"\"\"\n",
    "    Load ViT5 v·ªõi LoRA adapters v√† custom layers\n",
    "    V√¨ b·∫°n ch·ªâ l∆∞u adapters, c·∫ßn download base model t·ª´ HuggingFace\n",
    "    \"\"\"\n",
    "    print(f\"Loading ViT5 from: {adapter_path}\")\n",
    "    \n",
    "    if not os.path.exists(adapter_path):\n",
    "        raise FileNotFoundError(f\"Kh√¥ng t√¨m th·∫•y th∆∞ m·ª•c: {adapter_path}\")\n",
    "    \n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    \n",
    "    # Step 1: Load tokenizer t·ª´ local (n·∫øu c√≥) ho·∫∑c t·ª´ HuggingFace\n",
    "    tokenizer_path = adapter_path\n",
    "    if os.path.exists(os.path.join(adapter_path, \"tokenizer_config.json\")):\n",
    "        tokenizer = T5Tokenizer.from_pretrained(adapter_path, local_files_only=True)\n",
    "    else:\n",
    "        print(\"   ‚ö†Ô∏è Tokenizer kh√¥ng c√≥ local, downloading t·ª´ VietAI/vit5-base...\")\n",
    "        tokenizer = T5Tokenizer.from_pretrained(\"VietAI/vit5-base\")\n",
    "        # Add special tokens\n",
    "        special_tokens = {\"additional_special_tokens\": [\"<TIKTOK>\", \"<FACEBOOK>\", \"<YOUTUBE>\", \"<COMMENT>\"]}\n",
    "        tokenizer.add_special_tokens(special_tokens)\n",
    "    \n",
    "    # Step 2: Load base model t·ª´ HuggingFace\n",
    "    print(\"   Loading base model VietAI/vit5-base...\")\n",
    "    base_model = T5ForConditionalGeneration.from_pretrained(\n",
    "        \"VietAI/vit5-base\",\n",
    "        torch_dtype=torch.float32\n",
    "    )\n",
    "    base_model.resize_token_embeddings(len(tokenizer))\n",
    "    \n",
    "    # Step 3: Load LoRA adapters\n",
    "    print(\"   Loading LoRA adapters...\")\n",
    "    base_model = PeftModel.from_pretrained(base_model, adapter_path, local_files_only=True)\n",
    "    base_model = base_model.merge_and_unload()  # Merge LoRA v√†o model\n",
    "    \n",
    "    # Step 4: Wrap v·ªõi custom architecture\n",
    "    model = T5WithAdvancedArchitecture(base_model, hidden_size=768, num_heads=12)\n",
    "    \n",
    "    # Step 5: Load custom layers\n",
    "    custom_layers_path = os.path.join(adapter_path, \"advanced_layers.pt\")\n",
    "    if os.path.exists(custom_layers_path):\n",
    "        print(\"   Loading custom layers (BiLSTM, RNN, Attention)...\")\n",
    "        state_dict = torch.load(custom_layers_path, map_location=device)\n",
    "        model.multi_head_attention.load_state_dict(state_dict['multi_head_attention'])\n",
    "        model.bilstm.load_state_dict(state_dict['bilstm'])\n",
    "        model.rnn.load_state_dict(state_dict['rnn'])\n",
    "        model.ffn.load_state_dict(state_dict['ffn'])\n",
    "        model.gate.load_state_dict(state_dict['gate'])\n",
    "    else:\n",
    "        print(\"   ‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y advanced_layers.pt!\")\n",
    "    \n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    print(f\"‚úÖ ViT5 Advanced loaded on {device}\")\n",
    "    return model, tokenizer\n",
    "\n",
    "\n",
    "# ========================================\n",
    "# PREDICT FUNCTION\n",
    "# ========================================\n",
    "def predict(model, tokenizer, text, max_length=64, num_beams=4):\n",
    "    \"\"\"Predict v·ªõi model\"\"\"\n",
    "    device = next(model.parameters()).device\n",
    "    \n",
    "    inputs = tokenizer(\n",
    "        text, \n",
    "        return_tensors=\"pt\", \n",
    "        max_length=128, \n",
    "        truncation=True,\n",
    "        padding=True\n",
    "    ).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            input_ids=inputs['input_ids'],\n",
    "            attention_mask=inputs['attention_mask'],\n",
    "            max_length=max_length,\n",
    "            num_beams=num_beams,\n",
    "            early_stopping=True,\n",
    "            no_repeat_ngram_size=2,\n",
    "        )\n",
    "    \n",
    "    result = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return result\n",
    "\n",
    "\n",
    "# ========================================\n",
    "# TEST\n",
    "# ========================================\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # Test texts\n",
    "    test_texts = [\n",
    "        \"<TIKTOK> video n√†y hay qu√° ƒëi m·∫•t\",\n",
    "        \"<FACEBOOK> s·∫£n ph·∫©m t·ªët l·∫Øm shop ∆°i\", \n",
    "    ]\n",
    "    \n",
    "    # --- Test BARTpho ---\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"üìå Testing BARTpho\")\n",
    "    print(\"=\"*50)\n",
    "    try:\n",
    "        model, tokenizer = load_bartpho(BARTPHO_PATH)\n",
    "        for text in test_texts:\n",
    "            result = predict(model, tokenizer, text)\n",
    "            print(f\"Input:  {text}\")\n",
    "            print(f\"Output: {result}\\n\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error: {e}\")\n",
    "    \n",
    "    # --- Test mT5 ---\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"üìå Testing mT5\")\n",
    "    print(\"=\"*50)\n",
    "    try:\n",
    "        model, tokenizer = load_mt5(MT5_PATH)\n",
    "        for text in test_texts:\n",
    "            result = predict(model, tokenizer, text)\n",
    "            print(f\"Input:  {text}\")\n",
    "            print(f\"Output: {result}\\n\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error: {e}\")\n",
    "    \n",
    "    # --- Test ViT5 Advanced ---\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"üìå Testing ViT5 Advanced\")\n",
    "    print(\"=\"*50)\n",
    "    try:\n",
    "        model, tokenizer = load_vit5_advanced(VIT5_PATH)\n",
    "        for text in test_texts:\n",
    "            result = predict(model, tokenizer, text)\n",
    "            print(f\"Input:  {text}\")\n",
    "            print(f\"Output: {result}\\n\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e208b291",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd58397",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Load model b·∫°n mu·ªën d√πng\n",
    "model, tokenizer = load_bartpho(\"/results/models/bartpho/final\")\n",
    "# HO·∫∂C\n",
    "model, tokenizer = load_mt5(\"/results/models/mt5-base/final\")\n",
    "# HO·∫∂C  \n",
    "model, tokenizer = load_vit5_advanced(\"/results/models/vit5-base/final\")\n",
    "# 2. Predict\n",
    "result = predict(model, tokenizer, \"<TIKTOK> video hay qu√°\")\n",
    "print(result)\n",
    "\n",
    "# 3. Predict nhi·ªÅu texts\n",
    "results = predict_batch(model, tokenizer, [\"text1\", \"text2\", \"text3\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49636c03",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
