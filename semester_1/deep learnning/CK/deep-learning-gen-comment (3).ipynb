{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":13824265,"sourceType":"datasetVersion","datasetId":8803740}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# ========================================\n# CELL 1: INSTALL & IMPORT\n# ========================================\n\n# !pip install -q transformers==4.36.0 peft==0.7.1 datasets==2.16.1 accelerate==0.25.0 sentencepiece bitsandbytes sacrebleu rouge-score\n\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom transformers import (\n    T5ForConditionalGeneration, \n    T5Tokenizer,\n    MT5ForConditionalGeneration,\n    MT5Tokenizer,\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    TrainingArguments, \n    Trainer,\n    DataCollatorForSeq2Seq,\n    DataCollatorForLanguageModeling\n)\nfrom transformers.modeling_outputs import BaseModelOutput, CausalLMOutputWithCrossAttentions\nfrom peft import LoraConfig, get_peft_model, TaskType, prepare_model_for_kbit_training\nfrom datasets import Dataset, load_from_disk\nimport os\nimport json\nimport math\nimport numpy as np\nimport warnings\nimport gc\nfrom datetime import datetime\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom tqdm.auto import tqdm\n\nwarnings.filterwarnings('ignore')\ntorch.cuda.empty_cache()\n\nprint(f\"‚úÖ PyTorch: {torch.__version__}\")\nprint(f\"‚úÖ CUDA Available: {torch.cuda.is_available()}\")\nif torch.cuda.is_available():\n    print(f\"‚úÖ GPU: {torch.cuda.get_device_name(0)}\")\n    print(f\"‚úÖ VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n    print(f\"‚úÖ Compute Capability: {torch.cuda.get_device_capability(0)}\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-11-25T11:45:33.785196Z","iopub.execute_input":"2025-11-25T11:45:33.785478Z","iopub.status.idle":"2025-11-25T11:45:33.793335Z","shell.execute_reply.started":"2025-11-25T11:45:33.785458Z","shell.execute_reply":"2025-11-25T11:45:33.792430Z"}},"outputs":[{"name":"stdout","text":"‚úÖ PyTorch: 2.6.0+cu124\n‚úÖ CUDA Available: True\n‚úÖ GPU: Tesla P100-PCIE-16GB\n‚úÖ VRAM: 17.06 GB\n‚úÖ Compute Capability: (6, 0)\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"# ========================================\n# CELL 2: GLOBAL CONFIGURATION\n# ========================================\n\n# ========================================\n# CELL 2: GLOBAL CONFIGURATION (UPDATED)\n# ========================================\n\nclass Config:\n    # ‚úÖ TH√äM TEST MODE - B·∫≠t ƒë·ªÉ test nhanh\n    TEST_MODE = True  # ‚Üê SET TRUE ƒê·ªÇ TEST\n    \n    # Paths\n    DATA_DIR = \"/kaggle/input/processed-data\"\n    OUTPUT_DIR = \"/kaggle/working/models\"\n    \n    # ‚úÖ Training params - Dynamic based on TEST_MODE\n    if TEST_MODE:\n        print(\"‚ö° TEST MODE ENABLED - Fast Training\")\n        NUM_EPOCHS = 3  # Ch·ªâ 1 epoch\n        MAX_INPUT_LENGTH = 128  # Gi·∫£m t·ª´ 256\n        MAX_TARGET_LENGTH = 64   # Gi·∫£m t·ª´ 128\n        TRAIN_SAMPLES = 100000     # Ch·ªâ l·∫•y 1k samples\n        VAL_SAMPLES = 200\n        TEST_SAMPLES = 100\n        EVAL_STEPS = 50          # Eval s·ªõm h∆°n\n        SAVE_STEPS = 50\n        LOGGING_STEPS = 10\n    else:\n        print(\"üî• FULL TRAINING MODE\")\n        NUM_EPOCHS = 3\n        MAX_INPUT_LENGTH = 256\n        MAX_TARGET_LENGTH = 128\n        TRAIN_SAMPLES = None     # D√πng h·∫øt data\n        VAL_SAMPLES = None\n        TEST_SAMPLES = None\n        EVAL_STEPS = 200\n        SAVE_STEPS = 200\n        LOGGING_STEPS = 50\n    \n    # Model configs\n    MODELS = {\n        \"vit5-base\": {\n            \"name\": \"VietAI/vit5-base\",\n            \"type\": \"t5-advanced\",\n            \"batch_size\": 32 if TEST_MODE else 16,  # ‚úÖ TƒÉng batch trong test\n            \"learning_rate\": 2e-4 if TEST_MODE else 1e-4,  # ‚úÖ LR cao h∆°n\n            \"gradient_accumulation\": 1 if TEST_MODE else 2,  # ‚úÖ B·ªè accumulation\n            \"lora_r\": 16 if TEST_MODE else 32,  # ‚úÖ LoRA nh·ªè h∆°n\n            \"lora_alpha\": 32 if TEST_MODE else 64,\n        },\n        \n        \"gpt-neo-vi\": {\n            \"name\": \"VietAI/gpt-neo-1.3B-vietnamese-news\",\n            \"type\": \"gpt\",\n            \"batch_size\": 8 if TEST_MODE else 4,\n            \"learning_rate\": 1e-4 if TEST_MODE else 5e-5,\n            \"gradient_accumulation\": 2 if TEST_MODE else 8,\n            \"lora_r\": 8 if TEST_MODE else 16,\n            \"lora_alpha\": 16 if TEST_MODE else 32,\n        },\n        \n        \"mt5-base\": {\n            \"name\": \"google/mt5-base\",\n            \"type\": \"mt5-simple\",\n            \"batch_size\": 24 if TEST_MODE else 12,\n            \"learning_rate\": 1e-4 if TEST_MODE else 5e-5,\n            \"gradient_accumulation\": 1 if TEST_MODE else 3,\n            \"lora_r\": 16 if TEST_MODE else 32,\n            \"lora_alpha\": 32 if TEST_MODE else 64,\n        }\n    }\n\nconfig = Config()\nos.makedirs(config.OUTPUT_DIR, exist_ok=True)\n\nprint(\"=\"*80)\nprint(\"üìã COMPARISON: 3 DIFFERENT MODELS\")\nprint(\"=\"*80)\nfor name, cfg in config.MODELS.items():\n    print(f\"\\n{name}:\")\n    print(f\"  Model: {cfg['name']}\")\n    print(f\"  Type: {cfg['type']}\")\n    print(f\"  Batch: {cfg['batch_size']} √ó {cfg['gradient_accumulation']} = {cfg['batch_size'] * cfg['gradient_accumulation']}\")\nprint(\"=\"*80)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-25T11:45:33.794659Z","iopub.execute_input":"2025-11-25T11:45:33.794894Z","iopub.status.idle":"2025-11-25T11:45:33.812212Z","shell.execute_reply.started":"2025-11-25T11:45:33.794877Z","shell.execute_reply":"2025-11-25T11:45:33.811459Z"}},"outputs":[{"name":"stdout","text":"‚ö° TEST MODE ENABLED - Fast Training\n================================================================================\nüìã COMPARISON: 3 DIFFERENT MODELS\n================================================================================\n\nvit5-base:\n  Model: VietAI/vit5-base\n  Type: t5-advanced\n  Batch: 32 √ó 1 = 32\n\ngpt-neo-vi:\n  Model: VietAI/gpt-neo-1.3B-vietnamese-news\n  Type: gpt\n  Batch: 8 √ó 2 = 16\n\nmt5-base:\n  Model: google/mt5-base\n  Type: mt5-simple\n  Batch: 24 √ó 1 = 24\n================================================================================\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"# ========================================\n# CELL 3: LOAD DATA (CHUNG CHO C·∫¢ 3 MODELS)\n# ========================================\n\ndef load_data():\n    print(\"=\"*70)\n    print(\"üìÇ LOADING DATA\")\n    print(\"=\"*70)\n    \n    train_df = pd.read_csv(f\"{config.DATA_DIR}/train.csv\")\n    val_df = pd.read_csv(f\"{config.DATA_DIR}/val.csv\")\n    test_df = pd.read_csv(f\"{config.DATA_DIR}/test.csv\")\n    \n    # Clean\n    for df in [train_df, val_df, test_df]:\n        df.dropna(subset=['input_text', 'target_text'], inplace=True)\n        df['input_text'] = df['input_text'].str.strip()\n        df['target_text'] = df['target_text'].str.strip()\n    \n    # ‚úÖ SAMPLE DATA IN TEST MODE\n    if config.TEST_MODE:\n        train_df = train_df.head(config.TRAIN_SAMPLES)\n        val_df = val_df.head(config.VAL_SAMPLES)\n        test_df = test_df.head(config.TEST_SAMPLES)\n        print(\"‚ö° Using sampled data for fast testing\")\n    \n    print(f\"‚úÖ Train: {len(train_df):,}\")\n    print(f\"‚úÖ Val: {len(val_df):,}\")\n    print(f\"‚úÖ Test: {len(test_df):,}\")\n    \n    return train_df, val_df, test_df\n\ntrain_df, val_df, test_df = load_data()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-25T11:45:33.813009Z","iopub.execute_input":"2025-11-25T11:45:33.815518Z","iopub.status.idle":"2025-11-25T11:45:37.161522Z","shell.execute_reply.started":"2025-11-25T11:45:33.815498Z","shell.execute_reply":"2025-11-25T11:45:37.160732Z"}},"outputs":[{"name":"stdout","text":"======================================================================\nüìÇ LOADING DATA\n======================================================================\n‚ö° Using sampled data for fast testing\n‚úÖ Train: 100,000\n‚úÖ Val: 200\n‚úÖ Test: 100\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"# ========================================\n# CELL 4: CUSTOM ADVANCED ARCHITECTURE (CODE C≈®)\n# ========================================\n\nimport math\n\nclass CustomMultiHeadAttention(nn.Module):\n    \"\"\"Multi-Head Attention v·ªõi Scaled Dot-Product\"\"\"\n    def __init__(self, hidden_size, num_heads, dropout=0.1):\n        super().__init__()\n        self.hidden_size = hidden_size\n        self.num_heads = num_heads\n        self.head_dim = hidden_size // num_heads\n        \n        assert self.head_dim * num_heads == hidden_size\n        \n        self.query = nn.Linear(hidden_size, hidden_size)\n        self.key = nn.Linear(hidden_size, hidden_size)\n        self.value = nn.Linear(hidden_size, hidden_size)\n        self.out = nn.Linear(hidden_size, hidden_size)\n        \n        self.dropout = nn.Dropout(dropout)\n        self.scale = math.sqrt(self.head_dim)\n        \n    def forward(self, hidden_states, attention_mask=None):\n        batch_size, seq_length, _ = hidden_states.size()\n        \n        Q = self.query(hidden_states)\n        K = self.key(hidden_states)\n        V = self.value(hidden_states)\n        \n        Q = Q.view(batch_size, seq_length, self.num_heads, self.head_dim).transpose(1, 2)\n        K = K.view(batch_size, seq_length, self.num_heads, self.head_dim).transpose(1, 2)\n        V = V.view(batch_size, seq_length, self.num_heads, self.head_dim).transpose(1, 2)\n        \n        scores = torch.matmul(Q, K.transpose(-2, -1)) / self.scale\n        \n        if attention_mask is not None:\n            scores = scores.masked_fill(attention_mask == 0, -1e4)\n        \n        attn_weights = torch.softmax(scores, dim=-1)\n        attn_weights = self.dropout(attn_weights)\n        \n        context = torch.matmul(attn_weights, V)\n        context = context.transpose(1, 2).contiguous().view(batch_size, seq_length, self.hidden_size)\n        output = self.out(context)\n        \n        return output, attn_weights\n\n\nclass BiLSTMLayer(nn.Module):\n    \"\"\"Bidirectional LSTM ƒë·ªÉ capture context hai chi·ªÅu\"\"\"\n    def __init__(self, hidden_size, num_layers=2, dropout=0.1):\n        super().__init__()\n        self.lstm = nn.LSTM(\n            input_size=hidden_size,\n            hidden_size=hidden_size // 2,\n            num_layers=num_layers,\n            dropout=dropout if num_layers > 1 else 0,\n            bidirectional=True,\n            batch_first=True\n        )\n        self.layer_norm = nn.LayerNorm(hidden_size)\n        self.dropout = nn.Dropout(dropout)\n        \n    def forward(self, hidden_states):\n        lstm_out, _ = self.lstm(hidden_states)\n        output = self.layer_norm(hidden_states + self.dropout(lstm_out))\n        return output\n\n\nclass RNNLayer(nn.Module):\n    \"\"\"Simple RNN layer\"\"\"\n    def __init__(self, hidden_size, num_layers=1, dropout=0.1):\n        super().__init__()\n        self.rnn = nn.RNN(\n            input_size=hidden_size,\n            hidden_size=hidden_size,\n            num_layers=num_layers,\n            dropout=dropout if num_layers > 1 else 0,\n            batch_first=True\n        )\n        self.layer_norm = nn.LayerNorm(hidden_size)\n        self.dropout = nn.Dropout(dropout)\n        \n    def forward(self, hidden_states):\n        rnn_out, _ = self.rnn(hidden_states)\n        output = self.layer_norm(hidden_states + self.dropout(rnn_out))\n        return output\n\n\nclass FeedForwardNetwork(nn.Module):\n    \"\"\"Position-wise Feed Forward Network\"\"\"\n    def __init__(self, hidden_size, ff_size=None, dropout=0.1):\n        super().__init__()\n        ff_size = ff_size or hidden_size * 4\n        \n        self.fc1 = nn.Linear(hidden_size, ff_size)\n        self.fc2 = nn.Linear(ff_size, hidden_size)\n        self.dropout = nn.Dropout(dropout)\n        self.layer_norm = nn.LayerNorm(hidden_size)\n        \n    def forward(self, hidden_states):\n        residual = hidden_states\n        x = F.gelu(self.fc1(hidden_states))\n        x = self.dropout(x)\n        x = self.fc2(x)\n        x = self.dropout(x)\n        output = self.layer_norm(residual + x)\n        return output\n\n\nclass T5WithAdvancedArchitecture(nn.Module):\n    \"\"\"\n    CODE C≈®: T5 Model k·∫øt h·ª£p:\n    - Multi-Head Attention (Transformer)\n    - Bidirectional LSTM\n    - RNN\n    - Feed Forward Network\n    \"\"\"\n    def __init__(self, base_model, hidden_size=768, num_heads=12):\n        super().__init__()\n        self.base_model = base_model\n        self.config = base_model.config\n        \n        self.multi_head_attention = CustomMultiHeadAttention(hidden_size, num_heads, dropout=0.1)\n        self.bilstm = BiLSTMLayer(hidden_size, num_layers=2, dropout=0.1)\n        self.rnn = RNNLayer(hidden_size, num_layers=1, dropout=0.1)\n        self.ffn = FeedForwardNetwork(hidden_size, dropout=0.1)\n        \n        self.layer_norm1 = nn.LayerNorm(hidden_size)\n        self.layer_norm2 = nn.LayerNorm(hidden_size)\n        self.layer_norm3 = nn.LayerNorm(hidden_size)\n        self.dropout = nn.Dropout(0.1)\n        \n        self.gate = nn.Linear(hidden_size * 3, 3)\n        \n        print(\"‚úÖ ViT5-Base Advanced Architecture:\")\n        print(\"   - Multi-Head Attention\")\n        print(\"   - BiLSTM (2 layers)\")\n        print(\"   - RNN (1 layer)\")\n        print(\"   - FFN + Gating\")\n        \n    def forward(self, input_ids=None, attention_mask=None, labels=None, \n                decoder_input_ids=None, decoder_attention_mask=None, **kwargs):\n        \n        if \"num_items_in_batch\" in kwargs:\n            kwargs.pop(\"num_items_in_batch\")\n        \n        if decoder_input_ids is None and labels is not None:\n            decoder_input_ids = self.base_model._shift_right(labels)\n        \n        encoder_outputs = self.base_model.encoder(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            output_hidden_states=True,\n            return_dict=True\n        )\n        \n        encoder_hidden = encoder_outputs.last_hidden_state\n        \n        attn_output, _ = self.multi_head_attention(\n            encoder_hidden,\n            attention_mask=attention_mask.unsqueeze(1).unsqueeze(2) if attention_mask is not None else None\n        )\n        attn_output = self.layer_norm1(encoder_hidden + self.dropout(attn_output))\n        \n        lstm_output = self.bilstm(attn_output)\n        rnn_output = self.rnn(lstm_output)\n        \n        combined = torch.cat([attn_output, lstm_output, rnn_output], dim=-1)\n        gates = torch.softmax(self.gate(combined), dim=-1)\n        ensemble_output = (\n            gates[:, :, 0:1] * attn_output +\n            gates[:, :, 1:2] * lstm_output +\n            gates[:, :, 2:3] * rnn_output\n        )\n        \n        ensemble_output = self.ffn(ensemble_output)\n        final_output = self.layer_norm3(encoder_hidden + ensemble_output)\n        \n        new_encoder_outputs = BaseModelOutput(\n            last_hidden_state=final_output,\n            hidden_states=encoder_outputs.hidden_states\n        )\n        \n        outputs = self.base_model(\n            encoder_outputs=new_encoder_outputs,\n            attention_mask=attention_mask,\n            decoder_input_ids=decoder_input_ids,\n            decoder_attention_mask=decoder_attention_mask,\n            labels=labels,\n            return_dict=True\n        )\n        \n        return outputs\n    \n    def generate(self, *args, **kwargs):\n        return self.base_model.generate(*args, **kwargs)\n    \n    def prepare_inputs_for_generation(self, *args, **kwargs):\n        return self.base_model.prepare_inputs_for_generation(*args, **kwargs)\n\n\nclass CustomTrainer(Trainer):\n    \"\"\"Custom Trainer - CODE C≈®\"\"\"\n    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n        outputs = model(**inputs)\n        loss = outputs.loss\n        return (loss, outputs) if return_outputs else loss\n\n\nprint(\"‚úÖ Advanced Architecture Classes Defined (CODE C≈®)\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-25T11:45:37.164889Z","iopub.execute_input":"2025-11-25T11:45:37.165100Z","iopub.status.idle":"2025-11-25T11:45:37.187157Z","shell.execute_reply.started":"2025-11-25T11:45:37.165085Z","shell.execute_reply":"2025-11-25T11:45:37.186372Z"}},"outputs":[{"name":"stdout","text":"‚úÖ Advanced Architecture Classes Defined (CODE C≈®)\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"# ========================================\n# CELL 5: SETUP & TRAIN ViT5-Base (CODE C≈®)\n# ========================================\n\ndef setup_vit5_base():\n    \"\"\"Setup ViT5-Base v·ªõi Advanced Architecture - CODE C≈®\"\"\"\n    \n    model_config = config.MODELS[\"vit5-base\"]\n    \n    print(\"\\n\" + \"=\"*70)\n    print(\"üîß MODEL 1: ViT5-Base (CODE C≈®)\")\n    print(\"=\"*70)\n    \n    # Tokenizer\n    tokenizer = T5Tokenizer.from_pretrained(\n        model_config['name'],\n        model_max_length=512,\n        legacy=False\n    )\n    special_tokens = {\"additional_special_tokens\": [\"<TIKTOK>\", \"<FACEBOOK>\", \"<YOUTUBE>\", \"<COMMENT>\"]}\n    tokenizer.add_special_tokens(special_tokens)\n    \n    # Base model\n    base_model = T5ForConditionalGeneration.from_pretrained(\n        model_config['name'],\n        torch_dtype=torch.bfloat16,\n        low_cpu_mem_usage=True\n    )\n    base_model.resize_token_embeddings(len(tokenizer))\n    \n    # Advanced Architecture\n    model = T5WithAdvancedArchitecture(\n        base_model=base_model,\n        hidden_size=768,\n        num_heads=12\n    )\n    \n    # LoRA\n    peft_config = LoraConfig(\n        task_type=TaskType.SEQ_2_SEQ_LM,\n        r=model_config['lora_r'],\n        lora_alpha=model_config['lora_alpha'],\n        lora_dropout=0.05,\n        target_modules=[\"q\", \"v\", \"k\", \"o\", \"wi_0\", \"wi_1\", \"wo\"],\n        inference_mode=False,\n        bias=\"none\"\n    )\n    model.base_model = get_peft_model(model.base_model, peft_config)\n    model = model.to(\"cuda\")\n    \n    model.base_model.print_trainable_parameters()\n    \n    return model, tokenizer\n\n\ndef tokenize_t5(tokenizer, train_df, val_df, test_df):\n    \"\"\"Tokenize cho T5 - CODE C≈®\"\"\"\n    \n    def tokenize_function(examples):\n        model_inputs = tokenizer(\n            examples['input_text'],\n            max_length=config.MAX_INPUT_LENGTH,\n            truncation=True,\n            padding=\"max_length\"\n        )\n        \n        with tokenizer.as_target_tokenizer():\n            labels = tokenizer(\n                examples['target_text'],\n                max_length=config.MAX_TARGET_LENGTH,\n                truncation=True,\n                padding=\"max_length\"\n            )\n        \n        model_inputs[\"labels\"] = labels[\"input_ids\"]\n        return model_inputs\n    \n    train_dataset = Dataset.from_pandas(train_df[['input_text', 'target_text']])\n    val_dataset = Dataset.from_pandas(val_df[['input_text', 'target_text']])\n    test_dataset = Dataset.from_pandas(test_df[['input_text', 'target_text']])\n    \n    tokenized_train = train_dataset.map(tokenize_function, batched=True, remove_columns=train_dataset.column_names)\n    tokenized_val = val_dataset.map(tokenize_function, batched=True, remove_columns=val_dataset.column_names)\n    tokenized_test = test_dataset.map(tokenize_function, batched=True, remove_columns=test_dataset.column_names)\n\n    # üî•üî•üî• THIS IS THE FIX üî•üî•üî•\n    tokenized_train.set_format(type=\"torch\")\n    tokenized_val.set_format(type=\"torch\")\n    tokenized_test.set_format(type=\"torch\")\n    \n    print(f\"‚úÖ Tokenized: Train={len(tokenized_train)}, Val={len(tokenized_val)}, Test={len(tokenized_test)}\")\n    \n    return tokenized_train, tokenized_val, tokenized_test\n\n\ndef train_vit5_base(model, tokenizer, train_dataset, val_dataset):\n    \"\"\"Train ViT5-Base - CODE C≈®\"\"\"\n    \n    model_config = config.MODELS[\"vit5-base\"]\n    output_dir = f\"{config.OUTPUT_DIR}/vit5-base\"\n    os.makedirs(output_dir, exist_ok=True)\n    \n    print(\"\\nüöÄ Training ViT5-Base...\")\n    \n    data_collator = DataCollatorForSeq2Seq(\n        tokenizer=tokenizer,\n        model=model,\n        padding=True,\n        pad_to_multiple_of=8\n    )\n    \n    training_args = TrainingArguments(\n        output_dir=output_dir,\n        num_train_epochs=config.NUM_EPOCHS,\n        per_device_train_batch_size=model_config['batch_size'],\n        per_device_eval_batch_size=model_config['batch_size'] * 2,\n        gradient_accumulation_steps=model_config['gradient_accumulation'],\n        learning_rate=model_config['learning_rate'],\n        weight_decay=0.01,\n        max_grad_norm=1.0,\n        warmup_ratio=0.1,\n        bf16=True,\n        \n        # ‚úÖ FIX: TH√äM do_eval=True\n        do_eval=True,                      # ‚Üê TH√äM D√íNG N√ÄY\n        eval_strategy=\"steps\",             # ‚Üê TH√äM D√íNG N√ÄY (ho·∫∑c d√πng do_eval=True l√† ƒë·ªß)\n        \n        logging_steps=50,\n        eval_steps=200,\n        save_steps=200,\n        save_total_limit=2,\n        load_best_model_at_end=True,\n        metric_for_best_model=\"eval_loss\",\n        report_to=\"none\",\n        remove_unused_columns=False,\n        save_safetensors=False,\n    )\n    \n    trainer = CustomTrainer(\n        model=model,\n        args=training_args,\n        train_dataset=train_dataset,\n        eval_dataset=val_dataset,\n        tokenizer=tokenizer,\n        data_collator=data_collator,\n    )\n    \n    train_result = trainer.train()\n    \n    # Save\n    model_to_save = model.module if hasattr(model, 'module') else model\n    model_to_save.base_model.save_pretrained(f\"{output_dir}/final\")\n    \n    torch.save({\n        'multi_head_attention': model_to_save.multi_head_attention.state_dict(),\n        'bilstm': model_to_save.bilstm.state_dict(),\n        'rnn': model_to_save.rnn.state_dict(),\n        'ffn': model_to_save.ffn.state_dict(),\n        'gate': model_to_save.gate.state_dict(),\n    }, f\"{output_dir}/final/advanced_layers.pt\")\n    \n    tokenizer.save_pretrained(f\"{output_dir}/final\")\n    \n    print(f\"\\n‚úÖ Training completed!\")\n    print(f\"‚è±Ô∏è  Time: {train_result.metrics['train_runtime']/60:.1f} min\")\n    print(f\"üìä Loss: {train_result.metrics['train_loss']:.4f}\")\n    \n    return trainer, train_result\n\n\n# CH·∫†Y MODEL 1\nmodel1, tokenizer1 = setup_vit5_base()\ntokenized_train1, tokenized_val1, tokenized_test1 = tokenize_t5(tokenizer1, train_df, val_df, test_df)\ntrainer1, result1 = train_vit5_base(model1, tokenizer1, tokenized_train1, tokenized_val1)\n\nprint(\"\\n‚úÖ Model 1 (ViT5-Base) Ready for Evaluation\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-25T11:45:37.187921Z","iopub.execute_input":"2025-11-25T11:45:37.188100Z"}},"outputs":[{"name":"stdout","text":"\n======================================================================\nüîß MODEL 1: ViT5-Base (CODE C≈®)\n======================================================================\n‚úÖ ViT5-Base Advanced Architecture:\n   - Multi-Head Attention\n   - BiLSTM (2 layers)\n   - RNN (1 layer)\n   - FFN + Gating\ntrainable params: 5,013,504 || all params: 230,967,552 || trainable%: 2.1707\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/100000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4eb3b54166c54b5586982c17f5566f29"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/200 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2559274d8ae34088ab5f2649df52598a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/100 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3bb0e9ab78194528b1ee6ddfcfc11ad4"}},"metadata":{}},{"name":"stdout","text":"‚úÖ Tokenized: Train=100000, Val=200, Test=100\n\nüöÄ Training ViT5-Base...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='4601' max='9375' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [4601/9375 1:20:35 < 1:23:39, 0.95 it/s, Epoch 1.47/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>200</td>\n      <td>1.357700</td>\n      <td>1.328246</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>1.121100</td>\n      <td>1.167356</td>\n    </tr>\n    <tr>\n      <td>600</td>\n      <td>1.077900</td>\n      <td>1.147151</td>\n    </tr>\n    <tr>\n      <td>800</td>\n      <td>1.070300</td>\n      <td>1.132430</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>1.068100</td>\n      <td>1.122214</td>\n    </tr>\n    <tr>\n      <td>1200</td>\n      <td>1.056500</td>\n      <td>1.109577</td>\n    </tr>\n    <tr>\n      <td>1400</td>\n      <td>1.028200</td>\n      <td>1.115761</td>\n    </tr>\n    <tr>\n      <td>1600</td>\n      <td>1.064100</td>\n      <td>1.103978</td>\n    </tr>\n    <tr>\n      <td>1800</td>\n      <td>1.043700</td>\n      <td>1.103338</td>\n    </tr>\n    <tr>\n      <td>2000</td>\n      <td>1.041600</td>\n      <td>1.100631</td>\n    </tr>\n    <tr>\n      <td>2200</td>\n      <td>1.019100</td>\n      <td>1.100316</td>\n    </tr>\n    <tr>\n      <td>2400</td>\n      <td>1.043200</td>\n      <td>1.097531</td>\n    </tr>\n    <tr>\n      <td>2600</td>\n      <td>1.040000</td>\n      <td>1.088252</td>\n    </tr>\n    <tr>\n      <td>2800</td>\n      <td>1.031500</td>\n      <td>1.087965</td>\n    </tr>\n    <tr>\n      <td>3000</td>\n      <td>0.994000</td>\n      <td>1.085302</td>\n    </tr>\n    <tr>\n      <td>3200</td>\n      <td>1.008800</td>\n      <td>1.088613</td>\n    </tr>\n    <tr>\n      <td>3400</td>\n      <td>0.983400</td>\n      <td>1.082980</td>\n    </tr>\n    <tr>\n      <td>3600</td>\n      <td>1.007100</td>\n      <td>1.090481</td>\n    </tr>\n    <tr>\n      <td>3800</td>\n      <td>1.000700</td>\n      <td>1.083306</td>\n    </tr>\n    <tr>\n      <td>4000</td>\n      <td>1.003400</td>\n      <td>1.085601</td>\n    </tr>\n    <tr>\n      <td>4200</td>\n      <td>1.015900</td>\n      <td>1.078752</td>\n    </tr>\n    <tr>\n      <td>4400</td>\n      <td>1.032700</td>\n      <td>1.076319</td>\n    </tr>\n  </tbody>\n</table><p>\n    <div>\n      \n      <progress value='2' max='4' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [2/4 00:00 < 00:01, 1.37 it/s]\n    </div>\n    "},"metadata":{}}],"execution_count":null},{"cell_type":"code","source":"# ========================================\n# CELL 6: MODEL 2 - GPT-Neo Vietnamese (M·ªöI)\n# ========================================\n\ndef setup_gpt_neo():\n    \"\"\"Setup GPT-Neo Vietnamese - DECODER-ONLY\"\"\"\n    \n    model_config = config.MODELS[\"gpt-neo-vi\"]\n    \n    print(\"\\n\" + \"=\"*70)\n    print(\"üîß MODEL 2: GPT-Neo-1.3B Vietnamese (M·ªöI)\")\n    print(\"=\"*70)\n    \n    # Tokenizer\n    tokenizer = AutoTokenizer.from_pretrained(model_config['name'])\n    if tokenizer.pad_token is None:\n        tokenizer.pad_token = tokenizer.eos_token\n    \n    # Model\n    model = AutoModelForCausalLM.from_pretrained(\n        model_config['name'],\n        torch_dtype=torch.bfloat16,\n        low_cpu_mem_usage=True\n    )\n    model.config.pad_token_id = tokenizer.pad_token_id\n    \n    # LoRA\n    peft_config = LoraConfig(\n        task_type=TaskType.CAUSAL_LM,\n        r=model_config['lora_r'],\n        lora_alpha=model_config['lora_alpha'],\n        lora_dropout=0.05,\n        target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"out_proj\", \"c_fc\", \"c_proj\"],\n        inference_mode=False,\n        bias=\"none\"\n    )\n    model = get_peft_model(model, peft_config)\n    model = model.to(\"cuda\")\n    \n    model.print_trainable_parameters()\n    print(\"‚úÖ GPT-Neo: Decoder-Only Architecture\")\n    \n    return model, tokenizer\n\n\ndef tokenize_gpt(tokenizer, train_df, val_df, test_df):\n    \"\"\"Tokenize cho GPT - Causal LM\"\"\"\n    \n    def tokenize_function(examples):\n        # Format: input + target\n        texts = [\n            f\"{inp} {tokenizer.eos_token} {tgt} {tokenizer.eos_token}\"\n            for inp, tgt in zip(examples['input_text'], examples['target_text'])\n        ]\n        \n        model_inputs = tokenizer(\n            texts,\n            max_length=config.MAX_INPUT_LENGTH + config.MAX_TARGET_LENGTH,\n            truncation=True,\n            padding=\"max_length\"\n        )\n        \n        model_inputs[\"labels\"] = model_inputs[\"input_ids\"].copy()\n        \n        return model_inputs\n    \n    train_dataset = Dataset.from_pandas(train_df[['input_text', 'target_text']])\n    val_dataset = Dataset.from_pandas(val_df[['input_text', 'target_text']])\n    test_dataset = Dataset.from_pandas(test_df[['input_text', 'target_text']])\n    \n    tokenized_train = train_dataset.map(tokenize_function, batched=True, remove_columns=train_dataset.column_names)\n    tokenized_val = val_dataset.map(tokenize_function, batched=True, remove_columns=val_dataset.column_names)\n    tokenized_test = test_dataset.map(tokenize_function, batched=True, remove_columns=test_dataset.column_names)\n    \n    print(f\"‚úÖ Tokenized: Train={len(tokenized_train)}, Val={len(tokenized_val)}, Test={len(tokenized_test)}\")\n    \n    return tokenized_train, tokenized_val, tokenized_test\n\n\ndef train_gpt_neo(model, tokenizer, train_dataset, val_dataset):\n    \"\"\"Train GPT-Neo\"\"\"\n    \n    model_config = config.MODELS[\"gpt-neo-vi\"]\n    output_dir = f\"{config.OUTPUT_DIR}/gpt-neo-vi\"\n    os.makedirs(output_dir, exist_ok=True)\n    \n    print(\"\\nüöÄ Training GPT-Neo...\")\n    \n    data_collator = DataCollatorForLanguageModeling(\n        tokenizer=tokenizer,\n        mlm=False\n    )\n    \n    training_args = TrainingArguments(\n        output_dir=output_dir,\n        num_train_epochs=config.NUM_EPOCHS,\n        per_device_train_batch_size=model_config['batch_size'],\n        per_device_eval_batch_size=model_config['batch_size'],\n        gradient_accumulation_steps=model_config['gradient_accumulation'],\n        learning_rate=model_config['learning_rate'],\n        weight_decay=0.01,\n        max_grad_norm=1.0,\n        warmup_ratio=0.1,\n        bf16=True,\n        \n        do_eval=True,              # ‚úÖ TH√äM\n        eval_strategy=\"steps\",     # ‚úÖ TH√äM\n        \n        logging_steps=50,\n        eval_steps=200,\n        save_steps=200,\n        save_total_limit=2,\n        load_best_model_at_end=True,\n        metric_for_best_model=\"eval_loss\",\n        report_to=\"none\",\n        save_safetensors=False,\n    )\n    \n    trainer = Trainer(\n        model=model,\n        args=training_args,\n        train_dataset=train_dataset,\n        eval_dataset=val_dataset,\n        tokenizer=tokenizer,\n        data_collator=data_collator,\n    )\n    \n    train_result = trainer.train()\n    \n    # Save\n    model.save_pretrained(f\"{output_dir}/final\")\n    tokenizer.save_pretrained(f\"{output_dir}/final\")\n    \n    print(f\"\\n‚úÖ Training completed!\")\n    print(f\"‚è±Ô∏è  Time: {train_result.metrics['train_runtime']/60:.1f} min\")\n    print(f\"üìä Loss: {train_result.metrics['train_loss']:.4f}\")\n    \n    return trainer, train_result\n\n\n# CH·∫†Y MODEL 2\nmodel2, tokenizer2 = setup_gpt_neo()\ntokenized_train2, tokenized_val2, tokenized_test2 = tokenize_gpt(tokenizer2, train_df, val_df, test_df)\ntrainer2, result2 = train_gpt_neo(model2, tokenizer2, tokenized_train2, tokenized_val2)\n\n# Cleanup\ndel model2, trainer2\ntorch.cuda.empty_cache()\ngc.collect()\n\nprint(\"\\n‚úÖ Model 2 (GPT-Neo) Completed\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ========================================\n# CELL 7: MODEL 3 - mT5-Base (M·ªöI - SIMPLE)\n# ========================================\n\ndef setup_mt5_base():\n    \"\"\"Setup mT5-Base - ƒê∆†N GI·∫¢N (kh√¥ng d√πng advanced arch)\"\"\"\n    \n    model_config = config.MODELS[\"mt5-base\"]\n    \n    print(\"\\n\" + \"=\"*70)\n    print(\"üîß MODEL 3: mT5-Base (M·ªöI - SIMPLE)\")\n    print(\"=\"*70)\n    \n    # Tokenizer\n    tokenizer = MT5Tokenizer.from_pretrained(\n        model_config['name'],\n        model_max_length=512,\n        legacy=False\n    )\n    special_tokens = {\"additional_special_tokens\": [\"<TIKTOK>\", \"<FACEBOOK>\", \"<YOUTUBE>\", \"<COMMENT>\"]}\n    tokenizer.add_special_tokens(special_tokens)\n    \n    # Model - VANILLA (kh√¥ng wrap)\n    model = MT5ForConditionalGeneration.from_pretrained(\n        model_config['name'],\n        torch_dtype=torch.bfloat16,\n        low_cpu_mem_usage=True\n    )\n    model.resize_token_embeddings(len(tokenizer))\n    \n    # LoRA\n    peft_config = LoraConfig(\n        task_type=TaskType.SEQ_2_SEQ_LM,\n        r=model_config['lora_r'],\n        lora_alpha=model_config['lora_alpha'],\n        lora_dropout=0.05,\n        target_modules=[\"q\", \"v\", \"k\", \"o\", \"wi_0\", \"wi_1\", \"wo\"],\n        inference_mode=False,\n        bias=\"none\"\n    )\n    model = get_peft_model(model, peft_config)\n    model = model.to(\"cuda\")\n    \n    model.print_trainable_parameters()\n    print(\"‚úÖ mT5-Base: Vanilla Encoder-Decoder (No Custom Layers)\")\n    \n    return model, tokenizer\n\n\ndef train_mt5_base(model, tokenizer, train_dataset, val_dataset):\n    \"\"\"Train mT5-Base\"\"\"\n    \n    model_config = config.MODELS[\"mt5-base\"]\n    output_dir = f\"{config.OUTPUT_DIR}/mt5-base\"\n    os.makedirs(output_dir, exist_ok=True)\n    \n    print(\"\\nüöÄ Training mT5-Base...\")\n    \n    data_collator = DataCollatorForSeq2Seq(\n        tokenizer=tokenizer,\n        model=model,\n        padding=True,\n        pad_to_multiple_of=8\n    )\n    \n    training_args = TrainingArguments(\n        output_dir=output_dir,\n        num_train_epochs=config.NUM_EPOCHS,\n        per_device_train_batch_size=model_config['batch_size'],\n        per_device_eval_batch_size=model_config['batch_size'],\n        gradient_accumulation_steps=model_config['gradient_accumulation'],\n        learning_rate=model_config['learning_rate'],\n        weight_decay=0.01,\n        warmup_ratio=0.1,\n        bf16=True,\n        eval_strategy=\"steps\",\n        save_strategy=\"steps\",\n        logging_steps=50,\n        eval_steps=200,\n        save_steps=200,\n        save_total_limit=2,\n        load_best_model_at_end=True,\n        metric_for_best_model=\"eval_loss\",\n        report_to=\"none\",\n    )\n    \n    trainer = Trainer(\n        model=model,\n        args=training_args,\n        train_dataset=train_dataset,\n        eval_dataset=val_dataset,\n        tokenizer=tokenizer,\n        data_collator=data_collator,\n    )\n    \n    train_result = trainer.train()\n    \n    # üî• SAVE STRATEGY M·ªöI\n    print(\"üî• Merging LoRA and saving...\")\n    merged_model = model.merge_and_unload()\n    \n    save_path = f\"{output_dir}/final\"\n    merged_model.save_pretrained(save_path)\n    tokenizer.save_pretrained(save_path)\n    \n    # ‚úÖ L∆ØU TH√äM CONFIG V·ªÄ VOCAB SIZE\n    with open(f\"{save_path}/model_info.json\", \"w\") as f:\n        json.dump({\n            \"vocab_size\": len(tokenizer),\n            \"model_vocab_size\": merged_model.config.vocab_size,\n            \"num_special_tokens\": len(tokenizer) - merged_model.config.vocab_size\n        }, f, indent=2)\n    \n    print(f\"\\n‚úÖ Model saved to: {save_path}\")\n    print(f\"   Vocab size: {len(tokenizer)}\")\n    print(f\"‚è±Ô∏è  Training time: {train_result.metrics['train_runtime']/60:.1f} min\")\n    print(f\"üìä Train loss: {train_result.metrics['train_loss']:.4f}\")\n    \n    return trainer, train_result\n\n\n\n# CH·∫†Y MODEL 3\nmodel3, tokenizer3 = setup_mt5_base()\ntokenized_train3, tokenized_val3, tokenized_test3 = tokenize_t5(tokenizer3, train_df, val_df, test_df)  # D√πng chung tokenize_t5\ntrainer3, result3 = train_mt5_base(model3, tokenizer3, tokenized_train3, tokenized_val3)\n\n# Cleanup\ndel model3, trainer3\ntorch.cuda.empty_cache()\ngc.collect()\n\nprint(\"\\n‚úÖ Model 3 (mT5-Base) Completed\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# %%\n# ========================================\n# CELL 8: EVALUATION - ALL 3 MODELS (FIXED)\n# ========================================\n\ndef evaluate_t5_model(model, tokenizer, test_dataset, model_name):\n    \"\"\"Evaluate T5/mT5\"\"\"\n    \n    print(f\"\\n{'='*70}\")\n    print(f\"üìä EVALUATING: {model_name}\")\n    print(f\"{'='*70}\")\n    \n    model.eval()\n    predictions = []\n    references = []\n    \n    from torch.utils.data import DataLoader\n    dataloader = DataLoader(test_dataset, batch_size=8)\n    \n    total_time = 0\n    \n    with torch.no_grad():\n        for batch in tqdm(dataloader, desc=\"Generating\"):\n            input_ids = batch['input_ids'].to(\"cuda\")\n            attention_mask = batch['attention_mask'].to(\"cuda\")\n    \n            start_time = datetime.now()\n            outputs = model.generate(\n                input_ids=input_ids,\n                attention_mask=attention_mask,\n                max_length=config.MAX_TARGET_LENGTH,\n                num_beams=4,\n                early_stopping=True\n            )\n            total_time += (datetime.now() - start_time).total_seconds()\n    \n            preds = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n            refs = tokenizer.batch_decode(batch['labels'], skip_special_tokens=True)\n    \n            predictions.extend(preds)\n            references.extend(refs)\n    \n    # BLEU\n    from collections import Counter\n    def simple_bleu(pred, ref):\n        pred_words = pred.split()\n        ref_words = ref.split()\n        if len(pred_words) == 0:\n            return 0.0\n        matches = sum((Counter(pred_words) & Counter(ref_words)).values())\n        return matches / len(pred_words)\n    \n    bleu_scores = [simple_bleu(p, r) for p, r in zip(predictions, references)]\n    avg_bleu = sum(bleu_scores) / len(bleu_scores) if bleu_scores else 0.0\n    avg_time = total_time / len(test_dataset)\n    \n    print(f\"‚úÖ BLEU: {avg_bleu:.4f}\")\n    print(f\"‚úÖ Inference Time: {avg_time:.4f}s/sample\")\n    \n    return {\n        \"bleu\": avg_bleu,\n        \"inference_time\": avg_time,\n        \"predictions\": predictions[:10],\n        \"references\": references[:10]\n    }\n\n\ndef evaluate_gpt_model(model, tokenizer, test_df, model_name):\n    \"\"\"Evaluate GPT\"\"\"\n    \n    print(f\"\\n{'='*70}\")\n    print(f\"üìä EVALUATING: {model_name}\")\n    print(f\"{'='*70}\")\n    \n    model.eval()\n    predictions = []\n    references = test_df['target_text'].tolist()[:100]\n    \n    total_time = 0\n    \n    with torch.no_grad():\n        for idx, row in tqdm(test_df.head(100).iterrows(), total=100, desc=\"Generating\"):\n            input_text = row['input_text']\n            \n            inputs = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\n            \n            start_time = datetime.now()\n            outputs = model.generate(\n                **inputs,\n                max_new_tokens=config.MAX_TARGET_LENGTH,\n                num_beams=2,\n                early_stopping=True,\n                pad_token_id=tokenizer.pad_token_id,\n                eos_token_id=tokenizer.eos_token_id\n            )\n            total_time += (datetime.now() - start_time).total_seconds()\n            \n            input_length = inputs['input_ids'].shape[1]\n            generated = outputs[0][input_length:]\n            pred_text = tokenizer.decode(generated, skip_special_tokens=True)\n            predictions.append(pred_text)\n    \n    # BLEU\n    from collections import Counter\n    def simple_bleu(pred, ref):\n        pred_words = pred.split()\n        ref_words = ref.split()\n        if len(pred_words) == 0:\n            return 0.0\n        matches = sum((Counter(pred_words) & Counter(ref_words)).values())\n        return matches / len(pred_words)\n    \n    bleu_scores = [simple_bleu(p, r) for p, r in zip(predictions, references)]\n    avg_bleu = sum(bleu_scores) / len(bleu_scores) if bleu_scores else 0.0\n    avg_time = total_time / len(predictions)\n    \n    print(f\"‚úÖ BLEU: {avg_bleu:.4f}\")\n    print(f\"‚úÖ Inference Time: {avg_time:.4f}s/sample\")\n    \n    return {\n        \"bleu\": avg_bleu,\n        \"inference_time\": avg_time,\n        \"predictions\": predictions[:10],\n        \"references\": references[:10]\n    }\n\n\n# ========================================\n# üî• HELPER: LOAD MODEL ƒê√öNG C√ÅCH\n# ========================================\n\ndef load_mt5_for_eval(model_path):\n    \"\"\"\n    Load mT5 model v·ªõi ƒë√∫ng vocab size\n    \"\"\"\n    print(f\"\\nüîß Loading mT5 from: {model_path}\")\n    \n    # 1Ô∏è‚É£ Load tokenizer\n    tokenizer = MT5Tokenizer.from_pretrained(model_path)\n    \n    # 2Ô∏è‚É£ Load model\n    model = MT5ForConditionalGeneration.from_pretrained(\n        model_path,\n        torch_dtype=torch.bfloat16\n    ).to(\"cuda\")\n    \n    # 3Ô∏è‚É£ Check vocab size\n    print(f\"   Tokenizer vocab: {len(tokenizer)}\")\n    print(f\"   Model vocab: {model.config.vocab_size}\")\n    \n    # 4Ô∏è‚É£ N·∫øu mismatch ‚Üí Resize (safety check)\n    if len(tokenizer) != model.config.vocab_size:\n        print(f\"   ‚ö†Ô∏è  Vocab mismatch! Resizing model embeddings...\")\n        model.resize_token_embeddings(len(tokenizer))\n    \n    print(f\"   ‚úÖ Model loaded successfully!\")\n    \n    return model, tokenizer\n\n\ndef load_vit5_for_eval(model_path):\n    \"\"\"\n    Load ViT5 (v·ªõi custom architecture) ƒë·ªÉ eval\n    \"\"\"\n    print(f\"\\nüîß Loading ViT5 from: {model_path}\")\n    \n    # 1Ô∏è‚É£ Load tokenizer\n    tokenizer = T5Tokenizer.from_pretrained(model_path)\n    \n    # 2Ô∏è‚É£ Load base model\n    from peft import PeftModel\n    base_model = T5ForConditionalGeneration.from_pretrained(\n        model_path,\n        torch_dtype=torch.bfloat16\n    )\n    \n    # 3Ô∏è‚É£ Wrap v·ªõi custom architecture\n    model = T5WithAdvancedArchitecture(\n        base_model=base_model,\n        hidden_size=768,\n        num_heads=12\n    )\n    \n    # 4Ô∏è‚É£ Load custom layers\n    custom_layers_path = f\"{model_path}/advanced_layers.pt\"\n    if os.path.exists(custom_layers_path):\n        print(f\"   Loading custom layers...\")\n        state_dict = torch.load(custom_layers_path, map_location=\"cuda\")\n        model.multi_head_attention.load_state_dict(state_dict['multi_head_attention'])\n        model.bilstm.load_state_dict(state_dict['bilstm'])\n        model.rnn.load_state_dict(state_dict['rnn'])\n        model.ffn.load_state_dict(state_dict['ffn'])\n        model.gate.load_state_dict(state_dict['gate'])\n    \n    model = model.to(\"cuda\")\n    print(f\"   ‚úÖ ViT5 loaded successfully!\")\n    \n    return model, tokenizer\n\n\ndef load_gpt_for_eval(model_path):\n    \"\"\"Load GPT-Neo\"\"\"\n    print(f\"\\nüîß Loading GPT-Neo from: {model_path}\")\n    \n    tokenizer = AutoTokenizer.from_pretrained(model_path)\n    model = AutoModelForCausalLM.from_pretrained(\n        model_path,\n        torch_dtype=torch.bfloat16\n    ).to(\"cuda\")\n    \n    print(f\"   ‚úÖ GPT-Neo loaded successfully!\")\n    return model, tokenizer\n\n\n# ========================================\n# üöÄ EVALUATE ALL MODELS\n# ========================================\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"üìä EVALUATING ALL 3 MODELS\")\nprint(\"=\"*70)\n\n# Model 1: ViT5-Base (ƒëang c√≥ s·∫µn trong memory)\nprint(\"\\n\" + \"=\"*70)\nprint(\"MODEL 1: ViT5-Base\")\nprint(\"=\"*70)\nmetrics1 = evaluate_t5_model(model1, tokenizer1, tokenized_test1, \"ViT5-Base\")\n\n# Model 2: GPT-Neo (load l·∫°i)\nprint(\"\\n\" + \"=\"*70)\nprint(\"MODEL 2: GPT-Neo\")\nprint(\"=\"*70)\nmodel2, tokenizer2_reload = load_gpt_for_eval(f\"{config.OUTPUT_DIR}/gpt-neo-vi/final\")\nmetrics2 = evaluate_gpt_model(model2, tokenizer2_reload, test_df, \"GPT-Neo\")\ndel model2, tokenizer2_reload\ntorch.cuda.empty_cache()\n\n# Model 3: mT5-Base (load ƒê√öNG C√ÅCH)\nprint(\"\\n\" + \"=\"*70)\nprint(\"MODEL 3: mT5-Base\")\nprint(\"=\"*70)\nmodel3, tokenizer3_reload = load_mt5_for_eval(f\"{config.OUTPUT_DIR}/mt5-base/final\")\nmetrics3 = evaluate_t5_model(model3, tokenizer3_reload, tokenized_test3, \"mT5-Base\")\ndel model3, tokenizer3_reload\ntorch.cuda.empty_cache()\n\nprint(\"\\n‚úÖ All Evaluations Complete!\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ========================================\n# CELL 9: FINAL COMPARISON & VISUALIZATION\n# ========================================\n\n# Compile results\nresults_df = pd.DataFrame({\n    \"Model\": [\"ViT5-Base\", \"GPT-Neo-1.3B\", \"mT5-Base\"],\n    \"Architecture\": [\n        \"T5 Enc-Dec + Advanced\",\n        \"GPT Decoder-Only\",\n        \"mT5 Enc-Dec Vanilla\"\n    ],\n    \"Params\": [\"220M\", \"1.3B\", \"580M\"],\n    \"Train Loss\": [\n        result1.metrics['train_loss'],\n        result2.metrics['train_loss'],\n        result3.metrics['train_loss']\n    ],\n    \"BLEU Score\": [\n        metrics1['bleu'],\n        metrics2['bleu'],\n        metrics3['bleu']\n    ],\n    \"Inference Time (s)\": [\n        metrics1['inference_time'],\n        metrics2['inference_time'],\n        metrics3['inference_time']\n    ],\n    \"Training Time (min)\": [\n        result1.metrics['train_runtime'] / 60,\n        result2.metrics['train_runtime'] / 60,\n        result3.metrics['train_runtime'] / 60\n    ]\n})\n\nprint(\"\\n\" + \"=\"*100)\nprint(\"üèÜ FINAL COMPARISON - 3 DIFFERENT MODELS\")\nprint(\"=\"*100)\nprint(results_df.to_string(index=False))\nprint(\"=\"*100)\n\n# Save\nresults_df.to_csv(f\"{config.OUTPUT_DIR}/comparison_results.csv\", index=False)\n\n# Best model\nbest_idx = results_df['BLEU Score'].idxmax()\nprint(f\"\\nü•á BEST MODEL: {results_df.loc[best_idx, 'Model']}\")\nprint(f\"   Architecture: {results_df.loc[best_idx, 'Architecture']}\")\nprint(f\"   BLEU: {results_df.loc[best_idx, 'BLEU Score']:.4f}\")\n\n# Sample predictions\nprint(\"\\n\" + \"=\"*100)\nprint(\"üìù SAMPLE PREDICTIONS\")\nprint(\"=\"*100)\n\nfor i in range(min(5, len(metrics1['predictions']))):\n    print(f\"\\n{'‚îÄ'*100}\")\n    print(f\"Example {i+1}\")\n    print(f\"{'‚îÄ'*100}\")\n    print(f\"Reference:\\n  {metrics1['references'][i]}\")\n    print(f\"\\nViT5-Base (Advanced):\\n  {metrics1['predictions'][i]}\")\n    print(f\"\\nGPT-Neo (Decoder):\\n  {metrics2['predictions'][i]}\")\n    print(f\"\\nmT5-Base (Vanilla):\\n  {metrics3['predictions'][i]}\")\n\n# Visualization\nplt.style.use('seaborn-v0_8-darkgrid')\nfig, axes = plt.subplots(2, 2, figsize=(16, 12))\n\ncolors = ['#3498db', '#e74c3c', '#2ecc71']\n\n# 1. BLEU\nax1 = axes[0, 0]\nbars = ax1.bar(results_df['Model'], results_df['BLEU Score'], color=colors)\nax1.set_title('BLEU Score', fontsize=14, fontweight='bold')\nax1.set_ylabel('BLEU')\nfor bar in bars:\n    height = bar.get_height()\n    ax1.text(bar.get_x() + bar.get_width()/2., height,\n             f'{height:.4f}', ha='center', va='bottom', fontsize=10)\n\n# 2. Loss\nax2 = axes[0, 1]\nbars = ax2.bar(results_df['Model'], results_df['Train Loss'], color=colors)\nax2.set_title('Training Loss', fontsize=14, fontweight='bold')\nax2.set_ylabel('Loss')\nfor bar in bars:\n    height = bar.get_height()\n    ax2.text(bar.get_x() + bar.get_width()/2., height,\n             f'{height:.4f}', ha='center', va='bottom', fontsize=10)\n\n# 3. Inference Speed\nax3 = axes[1, 0]\nbars = ax3.bar(results_df['Model'], results_df['Inference Time (s)'], color=colors)\nax3.set_title('Inference Speed', fontsize=14, fontweight='bold')\nax3.set_ylabel('Time (s)')\nfor bar in bars:\n    height = bar.get_height()\n    ax3.text(bar.get_x() + bar.get_width()/2., height,\n             f'{height:.4f}s', ha='center', va='bottom', fontsize=10)\n\n# 4. Training Time\nax4 = axes[1, 1]\nbars = ax4.bar(results_df['Model'], results_df['Training Time (min)'], color=colors)\nax4.set_title('Training Duration', fontsize=14, fontweight='bold')\nax4.set_ylabel('Time (min)')\nfor bar in bars:\n    height = bar.get_height()\n    ax4.text(bar.get_x() + bar.get_width()/2., height,\n             f'{height:.1f}m', ha='center', va='bottom', fontsize=10)\n\nplt.tight_layout()\nplt.savefig(f\"{config.OUTPUT_DIR}/comparison_charts.png\", dpi=300, bbox_inches='tight')\nplt.show()\n\nprint(f\"\\n‚úÖ Charts saved!\")\nprint(f\"\\nüéâ ALL DONE! Results in: {config.OUTPUT_DIR}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}