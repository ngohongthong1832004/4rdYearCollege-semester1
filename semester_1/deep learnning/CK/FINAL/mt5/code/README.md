# PhÃ¢n tÃ­ch Chi tiáº¿t Code mT5 + LoRA

## SÆ¡ Ä‘á»“ Tá»•ng quan Pipeline

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  INPUT â†’ TOKENIZER â†’ EMBEDDING â†’ ENCODER â†’ DECODER â†’ LM HEAD â†’ OUTPUT      â”‚
â”‚    â†“         â†“           â†“          â†“          â†“         â†“         â†“       â”‚
â”‚  String   [B,256]    [B,256,768] [B,256,768] [B,64,768] [B,64,V]  Loss     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Pipeline Ä‘Æ¡n giáº£n hÆ¡n - KHÃ”NG cÃ³ Advanced Layers
Chá»‰ cÃ³: mT5 base + LoRA adapters
```

---

## 1. INPUT (Raw Text)

**Vá»‹ trÃ­ trong code:** `load_processed_data()`

```python
def load_processed_data(self, data_dir="./processed_data"):
    print("ğŸ“¥ Loading dataset...")

    train_df = pd.read_csv(f"{data_dir}/train.csv")
    val_df = pd.read_csv(f"{data_dir}/val.csv")
    test_df = pd.read_csv(f"{data_dir}/test.csv")

    # Loáº¡i bá» rows cÃ³ NaN
    train_df = train_df.dropna(subset=["input_text", "target_text"])
    val_df = val_df.dropna(subset=["input_text", "target_text"])

    return train_df, val_df, test_df
```

| Thuá»™c tÃ­nh | GiÃ¡ trá»‹ |
|------------|---------|
| **Kiá»ƒu dá»¯ liá»‡u** | `String` |
| **Columns** | `['input_text', 'target_text']` |
| **VÃ­ dá»¥ input** | `"Video nÃ y hay quÃ¡ <TIKTOK>"` |
| **VÃ­ dá»¥ target** | `"Cáº£m Æ¡n báº¡n Ä‘Ã£ chia sáº»!"` |

---

## 2. TOKENIZER

**Vá»‹ trÃ­ trong code:** `setup_tokenizer()` vÃ  `tokenize_data()`

```python
# ===== SETUP TOKENIZER =====
def setup_tokenizer(self):
    print("Loading tokenizer...")

    # Check náº¿u Ä‘Ã£ cÃ³ tokenizer saved
    saved_tok_dir = f"{self.output_dir}/tokenizer"
    if os.path.exists(saved_tok_dir):
        print(f"Loading tokenizer from {saved_tok_dir}")
        self.tokenizer = AutoTokenizer.from_pretrained(saved_tok_dir)
        return self.tokenizer

    # Táº¡o má»›i tá»« mT5
    print("Creating new tokenizer from base mT5")
    tokenizer = AutoTokenizer.from_pretrained(
        self.model_name,        # "google/mt5-base"
        model_max_length=512,
    )

    original_size = len(tokenizer)  # ~250,000
    print(f"Original vocab: {original_size}")

    # ThÃªm 4 special tokens
    tokenizer.add_special_tokens({
        "additional_special_tokens": ["<TIKTOK>", "<FACEBOOK>", "<YOUTUBE>", "<COMMENT>"]
    })

    new_size = len(tokenizer)  # ~250,004
    print(f"New vocab: {new_size} (added {new_size - original_size} tokens)")

    # Save tokenizer
    tokenizer.save_pretrained(saved_tok_dir)
    self.tokenizer = tokenizer
    return tokenizer
```

```python
# ===== TOKENIZE DATA =====
def tokenize_data(self, train_df, val_df, max_input=256, max_target=64):
    print("âœï¸ Tokenizing dataset...")

    train_ds = Dataset.from_pandas(train_df[["input_text", "target_text"]])
    val_ds = Dataset.from_pandas(val_df[["input_text", "target_text"]])

    def encode(ex):
        # Tokenize input
        inputs = self.tokenizer(
            ex["input_text"],
            truncation=True,
            max_length=max_input,   # 256
            padding=False,
        )

        # Tokenize target
        with self.tokenizer.as_target_tokenizer():
            labels = self.tokenizer(
                ex["target_text"],
                truncation=True,
                max_length=max_target,  # 64
                padding=False,
            )
        
        inputs["labels"] = labels["input_ids"]
        return inputs

    train_tok = train_ds.map(encode, batched=True, remove_columns=["input_text", "target_text"])
    val_tok = val_ds.map(encode, batched=True, remove_columns=["input_text", "target_text"])

    return train_tok, val_tok
```

| Input/Output | Shape | Giáº£i thÃ­ch |
|--------------|-------|------------|
| **Input** | `String` | Raw text |
| **Output (input_ids)** | `[batch_size, seq_len]` = `[16, 256]` | Token IDs |
| **Output (labels)** | `[batch_size, target_len]` = `[16, 64]` | Target token IDs |
| **Vocab size** | `250,004` | mT5 vocab + 4 special tokens |

**VÃ­ dá»¥:**
```
"Video nÃ y hay <TIKTOK>" â†’ [1234, 567, 89, 250001, 1]  (token IDs)
                                            â†‘
                                     special token ID
```

---

## 3. EMBEDDING

**Vá»‹ trÃ­ trong code:** Náº±m **bÃªn trong** `MT5ForConditionalGeneration`, Ä‘Æ°á»£c xá»­ lÃ½ trong `setup_model()`

```python
def setup_model(self):
    print("ğŸ”§ Loading mT5 base model...")

    model = MT5ForConditionalGeneration.from_pretrained(
        self.model_name,           # "google/mt5-base"
        torch_dtype=torch.bfloat16,
    )

    print(f"Original embedding size: {model.get_input_embeddings().weight.shape}")
    # Output: [250112, 768] (mT5 original vocab)
    
    # âœ… RESIZE EMBEDDING cho special tokens má»›i
    model.resize_token_embeddings(len(self.tokenizer))
    # New shape: [250004, 768]
    
    # âœ… KHá»I Táº O EMBEDDING CHO 4 TOKEN Má»šI
    with torch.no_grad():
        input_embeddings = model.get_input_embeddings()
        output_embeddings = model.get_output_embeddings()
        
        # Láº¥y mean cá»§a táº¥t cáº£ embeddings cÅ©
        input_embeddings_avg = input_embeddings.weight[:-4].mean(dim=0)
        # Shape: [768] - vector trung bÃ¬nh
        
        output_embeddings_avg = output_embeddings.weight[:-4].mean(dim=0)
        # Shape: [768]
        
        # GÃ¡n mean cho 4 token má»›i
        for i in range(4):
            input_embeddings.weight[-4 + i] = input_embeddings_avg
            output_embeddings.weight[-4 + i] = output_embeddings_avg
    
    print(f"New embedding size: {model.get_input_embeddings().weight.shape}")
    # Output: [250004, 768]
```

**Embedding Layer (áº©n bÃªn trong mT5):**
```python
# Pseudo-code cá»§a mT5 internal:
class MT5Stack:
    def __init__(self):
        self.embed_tokens = nn.Embedding(vocab_size, d_model)
        # Shape: [250004, 768]
    
    def forward(self, input_ids):
        # input_ids: [batch, seq_len] = [16, 256]
        hidden_states = self.embed_tokens(input_ids)
        # hidden_states: [batch, seq_len, d_model] = [16, 256, 768]
        return hidden_states
```

| Thuá»™c tÃ­nh | Shape | Giáº£i thÃ­ch |
|------------|-------|------------|
| **Input** | `[16, 256]` | Token IDs |
| **Embedding Matrix** | `[250004, 768]` | Vocab Ã— Hidden |
| **Output** | `[16, 256, 768]` | Embedded vectors |
| **4 new tokens** | Initialized vá»›i mean | `<TIKTOK>`, `<FACEBOOK>`, `<YOUTUBE>`, `<COMMENT>` |

---

## 4. mT5 ENCODER (+ LoRA)

**Vá»‹ trÃ­ trong code:** BÃªn trong `MT5ForConditionalGeneration`, LoRA Ä‘Æ°á»£c apply trong `setup_model()`

```python
def setup_model(self):
    # ... sau pháº§n embedding ...

    # âœ… LORA CONFIG
    peft_cfg = LoraConfig(
        task_type=TaskType.SEQ_2_SEQ_LM,
        r=16,                    # LoRA rank
        lora_alpha=32,           # Scaling factor
        lora_dropout=0.1,
        target_modules=["q", "k", "v", "o", "wi_0", "wi_1", "wo"],
        inference_mode=False,
        modules_to_save=None,    # KHÃ”NG train embedding
    )

    # Apply LoRA vÃ o model
    model = get_peft_model(model, peft_cfg)
    model.print_trainable_parameters()
    # Output: trainable params: ~1.5M (0.6% of 250M)

    model = model.to("cuda")
    self.model = model
    return self.model
```

**Cáº¥u trÃºc mT5 Encoder (12 layers):**

```
Input [16, 256, 768]
        â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Layer 1-12 (má»—i layer cÃ³):                                 â”‚
â”‚                                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  Self-Attention                                      â”‚   â”‚
â”‚  â”‚  â”œâ”€ Q = W_q Ã— input + LoRA_A_q Ã— LoRA_B_q Ã— input   â”‚   â”‚
â”‚  â”‚  â”‚   W_q: [768, 768], LoRA_A: [768, 16], LoRA_B: [16, 768]
â”‚  â”‚  â”œâ”€ K = W_k Ã— input + LoRA_A_k Ã— LoRA_B_k Ã— input   â”‚   â”‚
â”‚  â”‚  â”œâ”€ V = W_v Ã— input + LoRA_A_v Ã— LoRA_B_v Ã— input   â”‚   â”‚
â”‚  â”‚  â””â”€ O = W_o Ã— attn_out + LoRA_A_o Ã— LoRA_B_o Ã— attn_out
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                         â†“                                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  LayerNorm                                           â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                         â†“                                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  Feed-Forward Network (FFN)                          â”‚   â”‚
â”‚  â”‚  â”œâ”€ wi_0: [768, 2048] + LoRA  (gate)                â”‚   â”‚
â”‚  â”‚  â”œâ”€ wi_1: [768, 2048] + LoRA  (up projection)       â”‚   â”‚
â”‚  â”‚  â””â”€ wo:   [2048, 768] + LoRA  (down projection)     â”‚   â”‚
â”‚  â”‚                                                      â”‚   â”‚
â”‚  â”‚  Computation: wo(gelu(wi_0(x)) * wi_1(x))           â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                         â†“                                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  LayerNorm                                           â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â†“
Output [16, 256, 768]
```

**LoRA Math:**
```
Original:  Y = W Ã— X           where W: [768, 768]
With LoRA: Y = W Ã— X + (A Ã— B) Ã— X
           where A: [768, 16], B: [16, 768]
           
Trainable params per module = 768 Ã— 16 + 16 Ã— 768 = 24,576
```

| Thuá»™c tÃ­nh | Shape/Value |
|------------|-------------|
| **Input** | `[16, 256, 768]` |
| **Output** | `[16, 256, 768]` |
| **Num layers** | 12 |
| **Hidden size** | 768 |
| **FFN size** | 2048 |
| **Attention heads** | 12 |
| **LoRA rank (r)** | 16 |
| **LoRA alpha** | 32 |
| **Target modules** | q, k, v, o, wi_0, wi_1, wo |

---

## 5. mT5 DECODER (+ LoRA)

**Vá»‹ trÃ­ trong code:** Tá»± Ä‘á»™ng Ä‘Æ°á»£c gá»i khi forward model vá»›i labels

```python
# Khi training, Trainer tá»± Ä‘á»™ng gá»i:
outputs = model(
    input_ids=input_ids,           # [16, 256]
    attention_mask=attention_mask,  # [16, 256]
    labels=labels,                  # [16, 64]
)

# BÃªn trong model:
# 1. Encoder xá»­ lÃ½ input_ids â†’ encoder_hidden_states [16, 256, 768]
# 2. Decoder nháº­n labels (shifted) â†’ decoder_hidden_states [16, 64, 768]
# 3. LM Head â†’ logits [16, 64, 250004]
# 4. CrossEntropyLoss(logits, labels) â†’ loss
```

**Cáº¥u trÃºc mT5 Decoder (12 layers):**

```
labels [16, 64]
        â†“
    Shift Right â†’ decoder_input_ids [16, 64]
    (thÃªm BOS token á»Ÿ Ä‘áº§u)
        â†“
    Decoder Embedding [16, 64, 768]
        â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Layer 1-12 (má»—i layer cÃ³):                                 â”‚
â”‚                                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  Masked Self-Attention + LoRA                        â”‚   â”‚
â”‚  â”‚  (chá»‰ attend to previous tokens)                     â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                         â†“                                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  Cross-Attention + LoRA                              â”‚   â”‚
â”‚  â”‚  Q: from decoder [16, 64, 768]                       â”‚   â”‚
â”‚  â”‚  K, V: from encoder [16, 256, 768]                   â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                         â†“                                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  FFN + LoRA                                          â”‚   â”‚
â”‚  â”‚  [768 â†’ 2048 â†’ 768]                                  â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â†“
decoder_hidden_states [16, 64, 768]
```

| Thuá»™c tÃ­nh | Shape/Value |
|------------|-------------|
| **Input (labels)** | `[16, 64]` |
| **decoder_input_ids** | `[16, 64]` (shifted right) |
| **Decoder embedding** | `[16, 64, 768]` |
| **Cross-attention to** | `[16, 256, 768]` (encoder output) |
| **Output** | `[16, 64, 768]` |

---

## 6. LM HEAD (Language Model Head)

**Vá»‹ trÃ­ trong code:** BÃªn trong `MT5ForConditionalGeneration`

```python
# Pseudo-code cá»§a mT5 internal:
class MT5ForConditionalGeneration:
    def __init__(self):
        # LM Head thÆ°á»ng lÃ  tied vá»›i embedding (share weights)
        self.lm_head = nn.Linear(d_model, vocab_size, bias=False)
        # Shape: [768, 250004]
        # Hoáº·c tied: self.lm_head.weight = self.shared.weight
    
    def forward(self, ...):
        # decoder_hidden: [16, 64, 768]
        
        # Project to vocab size
        lm_logits = self.lm_head(decoder_hidden)
        # lm_logits: [16, 64, 250004]
        
        return lm_logits
```

| Thuá»™c tÃ­nh | Shape |
|------------|-------|
| **Input** | `[16, 64, 768]` |
| **LM Head weights** | `[768, 250004]` (tied vá»›i embedding) |
| **Output (logits)** | `[16, 64, 250004]` |

---

## 7. OUTPUT (Loss Computation)

**Vá»‹ trÃ­ trong code:** Tá»± Ä‘á»™ng trong `Trainer` vÃ  bÃªn trong model

```python
# BÃªn trong MT5ForConditionalGeneration.forward():
def forward(self, input_ids, attention_mask, labels, ...):
    # ... encoder, decoder ...
    
    lm_logits = self.lm_head(decoder_outputs)  # [16, 64, 250004]
    
    loss = None
    if labels is not None:
        loss_fct = nn.CrossEntropyLoss(ignore_index=-100)
        
        # Flatten
        loss = loss_fct(
            lm_logits.view(-1, self.config.vocab_size),  # [16*64, 250004] = [1024, 250004]
            labels.view(-1)                               # [1024]
        )
    
    return Seq2SeqLMOutput(
        loss=loss,
        logits=lm_logits,
        ...
    )
```

```python
# Training trong train():
def train(self, train_tok, val_tok, epochs=4, batch=16, lr=3e-4):
    # DataCollator xá»­ lÃ½ padding
    collator = DataCollatorForSeq2Seq(
        tokenizer=self.tokenizer,
        model=self.model,
        pad_to_multiple_of=8,
    )

    # Training Arguments
    args = TrainingArguments(
        output_dir=self.output_dir,
        per_device_train_batch_size=batch,           # 16
        gradient_accumulation_steps=8,                # Effective batch = 128
        learning_rate=lr,                             # 3e-4
        lr_scheduler_type="cosine",
        warmup_steps=1000,
        num_train_epochs=epochs,                      # 4
        max_grad_norm=0.5,
        bf16=True,
        # ...
    )

    trainer = Trainer(
        model=self.model,
        args=args,
        train_dataset=train_tok,
        eval_dataset=val_tok,
        tokenizer=self.tokenizer,
        data_collator=collator,
        # ...
    )

    result = trainer.train()
    return result
```

| Thuá»™c tÃ­nh | Value |
|------------|-------|
| **Loss function** | `CrossEntropyLoss(ignore_index=-100)` |
| **Logits shape** | `[16, 64, 250004]` â†’ flatten `[1024, 250004]` |
| **Labels shape** | `[16, 64]` â†’ flatten `[1024]` |
| **Output** | scalar loss |

---

## 8. FULL PIPELINE DIAGRAM

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                          FULL mT5 + LoRA PIPELINE                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                              â”‚
â”‚  "Video nÃ y hay <TIKTOK>"                                                   â”‚
â”‚           â”‚                                                                  â”‚
â”‚           â–¼                                                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                                        â”‚
â”‚  â”‚    TOKENIZER    â”‚  String â†’ [16, 256] (token IDs)                        â”‚
â”‚  â”‚   (AutoTokenizer)â”‚  Vocab: 250,004 (mT5 + 4 special)                     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                                        â”‚
â”‚           â”‚                                                                  â”‚
â”‚           â–¼                                                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                                        â”‚
â”‚  â”‚    EMBEDDING    â”‚  [16, 256] â†’ [16, 256, 768]                            â”‚
â”‚  â”‚   (inside mT5)  â”‚  Matrix: [250004, 768]                                 â”‚
â”‚  â”‚                 â”‚  4 new tokens: initialized with mean                   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                                        â”‚
â”‚           â”‚                                                                  â”‚
â”‚           â–¼                                                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚                      mT5 ENCODER (12 layers)                         â”‚    â”‚
â”‚  â”‚                                                                      â”‚    â”‚
â”‚  â”‚   Input: [16, 256, 768]                                             â”‚    â”‚
â”‚  â”‚                                                                      â”‚    â”‚
â”‚  â”‚   Each layer:                                                        â”‚    â”‚
â”‚  â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚    â”‚
â”‚  â”‚   â”‚  Self-Attention                                             â”‚    â”‚    â”‚
â”‚  â”‚   â”‚  Q, K, V, O: [768, 768] + LoRA [768,16]Ã—[16,768]           â”‚    â”‚    â”‚
â”‚  â”‚   â”‚  12 heads Ã— 64 dim = 768                                    â”‚    â”‚    â”‚
â”‚  â”‚   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤    â”‚    â”‚
â”‚  â”‚   â”‚  FFN                                                        â”‚    â”‚    â”‚
â”‚  â”‚   â”‚  wi_0, wi_1: [768, 2048] + LoRA                            â”‚    â”‚    â”‚
â”‚  â”‚   â”‚  wo: [2048, 768] + LoRA                                    â”‚    â”‚    â”‚
â”‚  â”‚   â”‚  Activation: GELU                                          â”‚    â”‚    â”‚
â”‚  â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚    â”‚
â”‚  â”‚                                                                      â”‚    â”‚
â”‚  â”‚   Output: [16, 256, 768]                                            â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                â”‚                                             â”‚
â”‚                                â–¼                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚                      mT5 DECODER (12 layers)                         â”‚    â”‚
â”‚  â”‚                                                                      â”‚    â”‚
â”‚  â”‚   labels [16, 64] â†’ shift right â†’ decoder_input_ids [16, 64]        â”‚    â”‚
â”‚  â”‚                         â†“                                            â”‚    â”‚
â”‚  â”‚   Decoder Embedding: [16, 64, 768]                                  â”‚    â”‚
â”‚  â”‚                         â†“                                            â”‚    â”‚
â”‚  â”‚   Each layer:                                                        â”‚    â”‚
â”‚  â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚    â”‚
â”‚  â”‚   â”‚  Masked Self-Attention + LoRA                               â”‚    â”‚    â”‚
â”‚  â”‚   â”‚  (causal: only attend to past tokens)                       â”‚    â”‚    â”‚
â”‚  â”‚   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤    â”‚    â”‚
â”‚  â”‚   â”‚  Cross-Attention + LoRA                                     â”‚    â”‚    â”‚
â”‚  â”‚   â”‚  Q: from decoder [16, 64, 768]                              â”‚    â”‚    â”‚
â”‚  â”‚   â”‚  K, V: from encoder [16, 256, 768]                          â”‚    â”‚    â”‚
â”‚  â”‚   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤    â”‚    â”‚
â”‚  â”‚   â”‚  FFN + LoRA                                                 â”‚    â”‚    â”‚
â”‚  â”‚   â”‚  [768 â†’ 2048 â†’ 768]                                         â”‚    â”‚    â”‚
â”‚  â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚    â”‚
â”‚  â”‚                                                                      â”‚    â”‚
â”‚  â”‚   Output: [16, 64, 768]                                             â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                â”‚                                             â”‚
â”‚                                â–¼                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚                          LM HEAD                                     â”‚    â”‚
â”‚  â”‚                                                                      â”‚    â”‚
â”‚  â”‚   Linear: [768, 250004] (tied with embedding)                       â”‚    â”‚
â”‚  â”‚   Input:  [16, 64, 768]                                             â”‚    â”‚
â”‚  â”‚   Output: [16, 64, 250004] (logits)                                 â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                â”‚                                             â”‚
â”‚                                â–¼                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚                          OUTPUT                                      â”‚    â”‚
â”‚  â”‚                                                                      â”‚    â”‚
â”‚  â”‚   Training:                                                          â”‚    â”‚
â”‚  â”‚   â””â”€ CrossEntropyLoss(logits.view(-1, 250004), labels.view(-1))     â”‚    â”‚
â”‚  â”‚   â””â”€ [1024, 250004] vs [1024] â†’ scalar loss                         â”‚    â”‚
â”‚  â”‚                                                                      â”‚    â”‚
â”‚  â”‚   Inference:                                                         â”‚    â”‚
â”‚  â”‚   â””â”€ argmax(logits) â†’ token IDs                                     â”‚    â”‚
â”‚  â”‚   â””â”€ tokenizer.decode() â†’ "Cáº£m Æ¡n báº¡n Ä‘Ã£ chia sáº»!"                  â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 9. Báº£ng Tá»•ng há»£p Táº¥t cáº£ Shapes

| # | Component | Code Location | Input Shape | Output Shape | Weights Shape |
|---|-----------|---------------|-------------|--------------|---------------|
| 1 | **Input** | `load_processed_data()` | String | String | - |
| 2 | **Tokenizer** | `setup_tokenizer()`, `tokenize_data()` | String | `[16, 256]` | Vocab: 250,004 |
| 3 | **Embedding** | Inside mT5 (`setup_model()`) | `[16, 256]` | `[16, 256, 768]` | `[250004, 768]` |
| 4 | **Encoder** | Inside mT5 | `[16, 256, 768]` | `[16, 256, 768]` | ~125M + LoRA |
| 5 | **Decoder** | Inside mT5 | `[16, 64]` + encoder | `[16, 64, 768]` | ~125M + LoRA |
| 6 | **LM Head** | Inside mT5 | `[16, 64, 768]` | `[16, 64, 250004]` | tied w/ embed |
| 7 | **Loss** | Inside mT5 | `[1024, 250004]` + `[1024]` | scalar | - |

---

## 10. LoRA Parameters Summary

**Target modules vá»›i LoRA:**

| Module | Original Shape | LoRA A | LoRA B | Trainable per module |
|--------|---------------|--------|--------|---------------------|
| `q` | `[768, 768]` | `[768, 16]` | `[16, 768]` | 24,576 |
| `k` | `[768, 768]` | `[768, 16]` | `[16, 768]` | 24,576 |
| `v` | `[768, 768]` | `[768, 16]` | `[16, 768]` | 24,576 |
| `o` | `[768, 768]` | `[768, 16]` | `[16, 768]` | 24,576 |
| `wi_0` | `[768, 2048]` | `[768, 16]` | `[16, 2048]` | 45,056 |
| `wi_1` | `[768, 2048]` | `[768, 16]` | `[16, 2048]` | 45,056 |
| `wo` | `[2048, 768]` | `[2048, 16]` | `[16, 768]` | 45,056 |

**Per layer:** ~233K params
**12 Encoder + 12 Decoder = 24 layers:** ~5.6M trainable params

```
Total LoRA trainable: ~1.5M (sau khi PEFT tá»‘i Æ°u)
Total model params: ~250M
Trainable ratio: ~0.6%
```

---

## 11. Training Config Summary

| Config | Value | Code Location |
|--------|-------|---------------|
| **Batch size** | 16 | `train(..., batch=16)` |
| **Gradient accumulation** | 8 | `gradient_accumulation_steps=8` |
| **Effective batch** | 128 | 16 Ã— 8 |
| **Learning rate** | 3e-4 | `train(..., lr=3e-4)` |
| **Scheduler** | cosine | `lr_scheduler_type="cosine"` |
| **Warmup steps** | 1000 | `warmup_steps=1000` |
| **Epochs** | 4 | `train(..., epochs=4)` |
| **Max input length** | 256 | `tokenize_data(..., max_input=256)` |
| **Max target length** | 64 | `tokenize_data(..., max_target=64)` |
| **Precision** | bf16 | `bf16=True` |
| **Grad clipping** | 0.5 | `max_grad_norm=0.5` |
| **Weight decay** | 0.01 | `weight_decay=0.01` |
| **Optimizer** | AdamW | `optim="adamw_torch"` |

---

## 12. So sÃ¡nh vá»›i Code Advanced (tÃ³m táº¯t)

| Aspect | **mT5 + LoRA (Code nÃ y)** | **ViT5 + Advanced** |
|--------|--------------------------|---------------------|
| **Pipeline** | Simple: Enc â†’ Dec | Complex: Enc â†’ Advanced â†’ Dec |
| **Advanced Layers** | âŒ KhÃ´ng cÃ³ | âœ… Attn + BiLSTM + RNN + Gate + FFN |
| **Trainable params** | ~1.5M | ~18-20M |
| **Vocab size** | 250,004 | ~32,004 |
| **NgÃ´n ngá»¯ tá»‘i Æ°u** | Multilingual | Tiáº¿ng Viá»‡t |
| **Training speed** | Nhanh hÆ¡n | Cháº­m hÆ¡n |
| **Memory usage** | Ãt hÆ¡n | Nhiá»u hÆ¡n |