{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":13824265,"sourceType":"datasetVersion","datasetId":8803740}],"dockerImageVersionId":31192,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# ========================================\n# CELL 1: INSTALL & IMPORT\n# ========================================\n\n# !pip install -q transformers==4.36.0 peft==0.7.1 datasets==2.16.1 accelerate==0.25.0 sentencepiece bitsandbytes sacrebleu rouge-score\n\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom transformers import (\n    T5ForConditionalGeneration, \n    T5Tokenizer,\n    MT5ForConditionalGeneration,\n    MT5Tokenizer,\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    TrainingArguments, \n    Trainer,\n    DataCollatorForSeq2Seq,\n    DataCollatorForLanguageModeling\n)\nfrom transformers.modeling_outputs import BaseModelOutput, CausalLMOutputWithCrossAttentions\nfrom peft import LoraConfig, get_peft_model, TaskType, prepare_model_for_kbit_training\nfrom datasets import Dataset, load_from_disk\nimport os\nimport json\nimport math\nimport numpy as np\nimport warnings\nimport gc\nfrom datetime import datetime\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom tqdm.auto import tqdm\n\nwarnings.filterwarnings('ignore')\ntorch.cuda.empty_cache()\n\nprint(f\"‚úÖ PyTorch: {torch.__version__}\")\nprint(f\"‚úÖ CUDA Available: {torch.cuda.is_available()}\")\nif torch.cuda.is_available():\n    print(f\"‚úÖ GPU: {torch.cuda.get_device_name(0)}\")\n    print(f\"‚úÖ VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n    print(f\"‚úÖ Compute Capability: {torch.cuda.get_device_capability(0)}\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-11-27T01:38:27.831683Z","iopub.execute_input":"2025-11-27T01:38:27.831911Z","iopub.status.idle":"2025-11-27T01:39:02.880975Z","shell.execute_reply.started":"2025-11-27T01:38:27.831883Z","shell.execute_reply":"2025-11-27T01:39:02.880203Z"}},"outputs":[{"name":"stderr","text":"2025-11-27 01:38:43.013617: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1764207523.211623      47 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1764207523.268932      47 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"name":"stdout","text":"‚úÖ PyTorch: 2.6.0+cu124\n‚úÖ CUDA Available: True\n‚úÖ GPU: Tesla P100-PCIE-16GB\n‚úÖ VRAM: 17.06 GB\n‚úÖ Compute Capability: (6, 0)\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# ========================================\n# CELL 2: GLOBAL CONFIGURATION\n# ========================================\n\n# ========================================\n# CELL 2: GLOBAL CONFIGURATION (UPDATED)\n# ========================================\n\nclass Config:\n    # ‚úÖ TH√äM TEST MODE - B·∫≠t ƒë·ªÉ test nhanh\n    TEST_MODE = True  # ‚Üê SET TRUE ƒê·ªÇ TEST\n    \n    # Paths\n    DATA_DIR = \"/kaggle/input/processed-data\"\n    OUTPUT_DIR = \"/kaggle/working/models\"\n    \n    # ‚úÖ Training params - Dynamic based on TEST_MODE\n    if TEST_MODE:\n        print(\"‚ö° TEST MODE ENABLED - Fast Training\")\n        NUM_EPOCHS = 1  # Ch·ªâ 1 epoch\n        MAX_INPUT_LENGTH = 128  # Gi·∫£m t·ª´ 256\n        MAX_TARGET_LENGTH = 64   # Gi·∫£m t·ª´ 128\n        TRAIN_SAMPLES = 100     # Ch·ªâ l·∫•y 1k samples\n        VAL_SAMPLES = 200\n        TEST_SAMPLES = 100\n        EVAL_STEPS = 50          # Eval s·ªõm h∆°n\n        SAVE_STEPS = 50\n        LOGGING_STEPS = 10\n    else:\n        print(\"üî• FULL TRAINING MODE\")\n        NUM_EPOCHS = 3\n        MAX_INPUT_LENGTH = 256\n        MAX_TARGET_LENGTH = 128\n        TRAIN_SAMPLES = None     # D√πng h·∫øt data\n        VAL_SAMPLES = None\n        TEST_SAMPLES = None\n        EVAL_STEPS = 200\n        SAVE_STEPS = 200\n        LOGGING_STEPS = 50\n    \n    # Model configs\n    MODELS = {\n        \"vit5-base\": {\n            \"name\": \"VietAI/vit5-base\",\n            \"type\": \"t5-advanced\",\n            \"batch_size\": 32 if TEST_MODE else 16,  # ‚úÖ TƒÉng batch trong test\n            \"learning_rate\": 2e-4 if TEST_MODE else 1e-4,  # ‚úÖ LR cao h∆°n\n            \"gradient_accumulation\": 1 if TEST_MODE else 2,  # ‚úÖ B·ªè accumulation\n            \"lora_r\": 16 if TEST_MODE else 32,  # ‚úÖ LoRA nh·ªè h∆°n\n            \"lora_alpha\": 32 if TEST_MODE else 64,\n        },\n        \n        \"gpt-neo-vi\": {\n            \"name\": \"VietAI/gpt-neo-1.3B-vietnamese-news\",\n            \"type\": \"gpt\",\n            \"batch_size\": 8 if TEST_MODE else 4,\n            \"learning_rate\": 1e-4 if TEST_MODE else 5e-5,\n            \"gradient_accumulation\": 2 if TEST_MODE else 8,\n            \"lora_r\": 8 if TEST_MODE else 16,\n            \"lora_alpha\": 16 if TEST_MODE else 32,\n        },\n        \n        \"mt5-base\": {\n            \"name\": \"google/mt5-base\",\n            \"type\": \"mt5-simple\",\n            \"batch_size\": 24 if TEST_MODE else 12,\n            \"learning_rate\": 1e-4 if TEST_MODE else 5e-5,\n            \"gradient_accumulation\": 1 if TEST_MODE else 3,\n            \"lora_r\": 16 if TEST_MODE else 32,\n            \"lora_alpha\": 32 if TEST_MODE else 64,\n        }\n    }\n\nconfig = Config()\nos.makedirs(config.OUTPUT_DIR, exist_ok=True)\n\nprint(\"=\"*80)\nprint(\"üìã COMPARISON: 3 DIFFERENT MODELS\")\nprint(\"=\"*80)\nfor name, cfg in config.MODELS.items():\n    print(f\"\\n{name}:\")\n    print(f\"  Model: {cfg['name']}\")\n    print(f\"  Type: {cfg['type']}\")\n    print(f\"  Batch: {cfg['batch_size']} √ó {cfg['gradient_accumulation']} = {cfg['batch_size'] * cfg['gradient_accumulation']}\")\nprint(\"=\"*80)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-27T01:39:02.882857Z","iopub.execute_input":"2025-11-27T01:39:02.883369Z","iopub.status.idle":"2025-11-27T01:39:02.893213Z","shell.execute_reply.started":"2025-11-27T01:39:02.883350Z","shell.execute_reply":"2025-11-27T01:39:02.892495Z"}},"outputs":[{"name":"stdout","text":"‚ö° TEST MODE ENABLED - Fast Training\n================================================================================\nüìã COMPARISON: 3 DIFFERENT MODELS\n================================================================================\n\nvit5-base:\n  Model: VietAI/vit5-base\n  Type: t5-advanced\n  Batch: 32 √ó 1 = 32\n\ngpt-neo-vi:\n  Model: VietAI/gpt-neo-1.3B-vietnamese-news\n  Type: gpt\n  Batch: 8 √ó 2 = 16\n\nmt5-base:\n  Model: google/mt5-base\n  Type: mt5-simple\n  Batch: 24 √ó 1 = 24\n================================================================================\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# ========================================\n# CELL 3: LOAD DATA (CHUNG CHO C·∫¢ 3 MODELS)\n# ========================================\n\ndef load_data():\n    print(\"=\"*70)\n    print(\"üìÇ LOADING DATA\")\n    print(\"=\"*70)\n    \n    train_df = pd.read_csv(f\"{config.DATA_DIR}/train.csv\")\n    val_df = pd.read_csv(f\"{config.DATA_DIR}/val.csv\")\n    test_df = pd.read_csv(f\"{config.DATA_DIR}/test.csv\")\n    \n    # Clean\n    for df in [train_df, val_df, test_df]:\n        df.dropna(subset=['input_text', 'target_text'], inplace=True)\n        df['input_text'] = df['input_text'].str.strip()\n        df['target_text'] = df['target_text'].str.strip()\n    \n    # ‚úÖ SAMPLE DATA IN TEST MODE\n    if config.TEST_MODE:\n        train_df = train_df.head(config.TRAIN_SAMPLES)\n        val_df = val_df.head(config.VAL_SAMPLES)\n        test_df = test_df.head(config.TEST_SAMPLES)\n        print(\"‚ö° Using sampled data for fast testing\")\n    \n    print(f\"‚úÖ Train: {len(train_df):,}\")\n    print(f\"‚úÖ Val: {len(val_df):,}\")\n    print(f\"‚úÖ Test: {len(test_df):,}\")\n    \n    return train_df, val_df, test_df\n\ntrain_df, val_df, test_df = load_data()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-27T01:39:02.893973Z","iopub.execute_input":"2025-11-27T01:39:02.894229Z","iopub.status.idle":"2025-11-27T01:39:06.549078Z","shell.execute_reply.started":"2025-11-27T01:39:02.894209Z","shell.execute_reply":"2025-11-27T01:39:06.548327Z"}},"outputs":[{"name":"stdout","text":"======================================================================\nüìÇ LOADING DATA\n======================================================================\n‚ö° Using sampled data for fast testing\n‚úÖ Train: 100\n‚úÖ Val: 200\n‚úÖ Test: 100\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# ========================================\n# CELL 4: CUSTOM ADVANCED ARCHITECTURE (CODE C≈®)\n# ========================================\n\nimport math\n\nclass CustomMultiHeadAttention(nn.Module):\n    \"\"\"Multi-Head Attention v·ªõi Scaled Dot-Product\"\"\"\n    def __init__(self, hidden_size, num_heads, dropout=0.1):\n        super().__init__()\n        self.hidden_size = hidden_size\n        self.num_heads = num_heads\n        self.head_dim = hidden_size // num_heads\n        \n        assert self.head_dim * num_heads == hidden_size\n        \n        self.query = nn.Linear(hidden_size, hidden_size)\n        self.key = nn.Linear(hidden_size, hidden_size)\n        self.value = nn.Linear(hidden_size, hidden_size)\n        self.out = nn.Linear(hidden_size, hidden_size)\n        \n        self.dropout = nn.Dropout(dropout)\n        self.scale = math.sqrt(self.head_dim)\n        \n    def forward(self, hidden_states, attention_mask=None):\n        batch_size, seq_length, _ = hidden_states.size()\n        \n        Q = self.query(hidden_states)\n        K = self.key(hidden_states)\n        V = self.value(hidden_states)\n        \n        Q = Q.view(batch_size, seq_length, self.num_heads, self.head_dim).transpose(1, 2)\n        K = K.view(batch_size, seq_length, self.num_heads, self.head_dim).transpose(1, 2)\n        V = V.view(batch_size, seq_length, self.num_heads, self.head_dim).transpose(1, 2)\n        \n        scores = torch.matmul(Q, K.transpose(-2, -1)) / self.scale\n        \n        if attention_mask is not None:\n            scores = scores.masked_fill(attention_mask == 0, -1e4)\n        \n        attn_weights = torch.softmax(scores, dim=-1)\n        attn_weights = self.dropout(attn_weights)\n        \n        context = torch.matmul(attn_weights, V)\n        context = context.transpose(1, 2).contiguous().view(batch_size, seq_length, self.hidden_size)\n        output = self.out(context)\n        \n        return output, attn_weights\n\n\nclass BiLSTMLayer(nn.Module):\n    \"\"\"Bidirectional LSTM ƒë·ªÉ capture context hai chi·ªÅu\"\"\"\n    def __init__(self, hidden_size, num_layers=2, dropout=0.1):\n        super().__init__()\n        self.lstm = nn.LSTM(\n            input_size=hidden_size,\n            hidden_size=hidden_size // 2,\n            num_layers=num_layers,\n            dropout=dropout if num_layers > 1 else 0,\n            bidirectional=True,\n            batch_first=True\n        )\n        self.layer_norm = nn.LayerNorm(hidden_size)\n        self.dropout = nn.Dropout(dropout)\n        \n    def forward(self, hidden_states):\n        lstm_out, _ = self.lstm(hidden_states)\n        output = self.layer_norm(hidden_states + self.dropout(lstm_out))\n        return output\n\n\nclass RNNLayer(nn.Module):\n    \"\"\"Simple RNN layer\"\"\"\n    def __init__(self, hidden_size, num_layers=1, dropout=0.1):\n        super().__init__()\n        self.rnn = nn.RNN(\n            input_size=hidden_size,\n            hidden_size=hidden_size,\n            num_layers=num_layers,\n            dropout=dropout if num_layers > 1 else 0,\n            batch_first=True\n        )\n        self.layer_norm = nn.LayerNorm(hidden_size)\n        self.dropout = nn.Dropout(dropout)\n        \n    def forward(self, hidden_states):\n        rnn_out, _ = self.rnn(hidden_states)\n        output = self.layer_norm(hidden_states + self.dropout(rnn_out))\n        return output\n\n\nclass FeedForwardNetwork(nn.Module):\n    \"\"\"Position-wise Feed Forward Network\"\"\"\n    def __init__(self, hidden_size, ff_size=None, dropout=0.1):\n        super().__init__()\n        ff_size = ff_size or hidden_size * 4\n        \n        self.fc1 = nn.Linear(hidden_size, ff_size)\n        self.fc2 = nn.Linear(ff_size, hidden_size)\n        self.dropout = nn.Dropout(dropout)\n        self.layer_norm = nn.LayerNorm(hidden_size)\n        \n    def forward(self, hidden_states):\n        residual = hidden_states\n        x = F.gelu(self.fc1(hidden_states))\n        x = self.dropout(x)\n        x = self.fc2(x)\n        x = self.dropout(x)\n        output = self.layer_norm(residual + x)\n        return output\n\n\nclass T5WithAdvancedArchitecture(nn.Module):\n    \"\"\"\n    CODE C≈®: T5 Model k·∫øt h·ª£p:\n    - Multi-Head Attention (Transformer)\n    - Bidirectional LSTM\n    - RNN\n    - Feed Forward Network\n    \"\"\"\n    def __init__(self, base_model, hidden_size=768, num_heads=12):\n        super().__init__()\n        self.base_model = base_model\n        self.config = base_model.config\n        \n        self.multi_head_attention = CustomMultiHeadAttention(hidden_size, num_heads, dropout=0.1)\n        self.bilstm = BiLSTMLayer(hidden_size, num_layers=2, dropout=0.1)\n        self.rnn = RNNLayer(hidden_size, num_layers=1, dropout=0.1)\n        self.ffn = FeedForwardNetwork(hidden_size, dropout=0.1)\n        \n        self.layer_norm1 = nn.LayerNorm(hidden_size)\n        self.layer_norm2 = nn.LayerNorm(hidden_size)\n        self.layer_norm3 = nn.LayerNorm(hidden_size)\n        self.dropout = nn.Dropout(0.1)\n        \n        self.gate = nn.Linear(hidden_size * 3, 3)\n        \n        print(\"‚úÖ ViT5-Base Advanced Architecture:\")\n        print(\"   - Multi-Head Attention\")\n        print(\"   - BiLSTM (2 layers)\")\n        print(\"   - RNN (1 layer)\")\n        print(\"   - FFN + Gating\")\n        \n    def forward(self, input_ids=None, attention_mask=None, labels=None, \n                decoder_input_ids=None, decoder_attention_mask=None, **kwargs):\n        \n        if \"num_items_in_batch\" in kwargs:\n            kwargs.pop(\"num_items_in_batch\")\n        \n        if decoder_input_ids is None and labels is not None:\n            decoder_input_ids = self.base_model._shift_right(labels)\n        \n        encoder_outputs = self.base_model.encoder(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            output_hidden_states=True,\n            return_dict=True\n        )\n        \n        encoder_hidden = encoder_outputs.last_hidden_state\n        \n        attn_output, _ = self.multi_head_attention(\n            encoder_hidden,\n            attention_mask=attention_mask.unsqueeze(1).unsqueeze(2) if attention_mask is not None else None\n        )\n        attn_output = self.layer_norm1(encoder_hidden + self.dropout(attn_output))\n        \n        lstm_output = self.bilstm(attn_output)\n        rnn_output = self.rnn(lstm_output)\n        \n        combined = torch.cat([attn_output, lstm_output, rnn_output], dim=-1)\n        gates = torch.softmax(self.gate(combined), dim=-1)\n        ensemble_output = (\n            gates[:, :, 0:1] * attn_output +\n            gates[:, :, 1:2] * lstm_output +\n            gates[:, :, 2:3] * rnn_output\n        )\n        \n        ensemble_output = self.ffn(ensemble_output)\n        final_output = self.layer_norm3(encoder_hidden + ensemble_output)\n        \n        new_encoder_outputs = BaseModelOutput(\n            last_hidden_state=final_output,\n            hidden_states=encoder_outputs.hidden_states\n        )\n        \n        outputs = self.base_model(\n            encoder_outputs=new_encoder_outputs,\n            attention_mask=attention_mask,\n            decoder_input_ids=decoder_input_ids,\n            decoder_attention_mask=decoder_attention_mask,\n            labels=labels,\n            return_dict=True\n        )\n        \n        return outputs\n    \n    def generate(self, *args, **kwargs):\n        return self.base_model.generate(*args, **kwargs)\n    \n    def prepare_inputs_for_generation(self, *args, **kwargs):\n        return self.base_model.prepare_inputs_for_generation(*args, **kwargs)\n\n\nclass CustomTrainer(Trainer):\n    \"\"\"Custom Trainer - CODE C≈®\"\"\"\n    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n        outputs = model(**inputs)\n        loss = outputs.loss\n        return (loss, outputs) if return_outputs else loss\n\n\nprint(\"‚úÖ Advanced Architecture Classes Defined (CODE C≈®)\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-27T01:39:06.549844Z","iopub.execute_input":"2025-11-27T01:39:06.550088Z","iopub.status.idle":"2025-11-27T01:39:06.573098Z","shell.execute_reply.started":"2025-11-27T01:39:06.550067Z","shell.execute_reply":"2025-11-27T01:39:06.572276Z"}},"outputs":[{"name":"stdout","text":"‚úÖ Advanced Architecture Classes Defined (CODE C≈®)\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# ========================================\n# CELL 5: SETUP & TRAIN ViT5-Base (CODE C≈®)\n# ========================================\n\ndef setup_vit5_base():\n    \"\"\"Setup ViT5-Base v·ªõi Advanced Architecture - CODE C≈®\"\"\"\n    \n    model_config = config.MODELS[\"vit5-base\"]\n    \n    print(\"\\n\" + \"=\"*70)\n    print(\"üîß MODEL 1: ViT5-Base (CODE C≈®)\")\n    print(\"=\"*70)\n    \n    # Tokenizer\n    tokenizer = T5Tokenizer.from_pretrained(\n        model_config['name'],\n        model_max_length=512,\n        legacy=False\n    )\n    special_tokens = {\"additional_special_tokens\": [\"<TIKTOK>\", \"<FACEBOOK>\", \"<YOUTUBE>\", \"<COMMENT>\"]}\n    tokenizer.add_special_tokens(special_tokens)\n    \n    # Base model\n    base_model = T5ForConditionalGeneration.from_pretrained(\n        model_config['name'],\n        torch_dtype=torch.bfloat16,\n        low_cpu_mem_usage=True\n    )\n    base_model.resize_token_embeddings(len(tokenizer))\n    \n    # Advanced Architecture\n    model = T5WithAdvancedArchitecture(\n        base_model=base_model,\n        hidden_size=768,\n        num_heads=12\n    )\n    \n    # LoRA\n    peft_config = LoraConfig(\n        task_type=TaskType.SEQ_2_SEQ_LM,\n        r=model_config['lora_r'],\n        lora_alpha=model_config['lora_alpha'],\n        lora_dropout=0.05,\n        target_modules=[\"q\", \"v\", \"k\", \"o\", \"wi_0\", \"wi_1\", \"wo\"],\n        inference_mode=False,\n        bias=\"none\"\n    )\n    model.base_model = get_peft_model(model.base_model, peft_config)\n    model = model.to(\"cuda\")\n    \n    model.base_model.print_trainable_parameters()\n    \n    return model, tokenizer\n\n\ndef tokenize_t5(tokenizer, train_df, val_df, test_df):\n    \"\"\"Tokenize cho T5 - CODE C≈®\"\"\"\n    \n    def tokenize_function(examples):\n        model_inputs = tokenizer(\n            examples['input_text'],\n            max_length=config.MAX_INPUT_LENGTH,\n            truncation=True,\n            padding=\"max_length\"\n        )\n        \n        with tokenizer.as_target_tokenizer():\n            labels = tokenizer(\n                examples['target_text'],\n                max_length=config.MAX_TARGET_LENGTH,\n                truncation=True,\n                padding=\"max_length\"\n            )\n        \n        model_inputs[\"labels\"] = labels[\"input_ids\"]\n        return model_inputs\n    \n    train_dataset = Dataset.from_pandas(train_df[['input_text', 'target_text']])\n    val_dataset = Dataset.from_pandas(val_df[['input_text', 'target_text']])\n    test_dataset = Dataset.from_pandas(test_df[['input_text', 'target_text']])\n    \n    tokenized_train = train_dataset.map(tokenize_function, batched=True, remove_columns=train_dataset.column_names)\n    tokenized_val = val_dataset.map(tokenize_function, batched=True, remove_columns=val_dataset.column_names)\n    tokenized_test = test_dataset.map(tokenize_function, batched=True, remove_columns=test_dataset.column_names)\n\n    # üî•üî•üî• THIS IS THE FIX üî•üî•üî•\n    tokenized_train.set_format(type=\"torch\")\n    tokenized_val.set_format(type=\"torch\")\n    tokenized_test.set_format(type=\"torch\")\n    \n    print(f\"‚úÖ Tokenized: Train={len(tokenized_train)}, Val={len(tokenized_val)}, Test={len(tokenized_test)}\")\n    \n    return tokenized_train, tokenized_val, tokenized_test\n\n\ndef train_vit5_base(model, tokenizer, train_dataset, val_dataset):\n    \"\"\"Train ViT5-Base - CODE C≈®\"\"\"\n    \n    model_config = config.MODELS[\"vit5-base\"]\n    output_dir = f\"{config.OUTPUT_DIR}/vit5-base\"\n    os.makedirs(output_dir, exist_ok=True)\n    \n    print(\"\\nüöÄ Training ViT5-Base...\")\n    \n    data_collator = DataCollatorForSeq2Seq(\n        tokenizer=tokenizer,\n        model=model,\n        padding=True,\n        pad_to_multiple_of=8\n    )\n    \n    training_args = TrainingArguments(\n        output_dir=output_dir,\n        num_train_epochs=config.NUM_EPOCHS,\n        per_device_train_batch_size=model_config['batch_size'],\n        per_device_eval_batch_size=model_config['batch_size'] * 2,\n        gradient_accumulation_steps=model_config['gradient_accumulation'],\n        learning_rate=model_config['learning_rate'],\n        weight_decay=0.01,\n        max_grad_norm=1.0,\n        warmup_ratio=0.1,\n        bf16=True,\n        \n        # ‚úÖ FIX: TH√äM do_eval=True\n        do_eval=True,                      # ‚Üê TH√äM D√íNG N√ÄY\n        eval_strategy=\"steps\",             # ‚Üê TH√äM D√íNG N√ÄY (ho·∫∑c d√πng do_eval=True l√† ƒë·ªß)\n        \n        logging_steps=50,\n        eval_steps=200,\n        save_steps=200,\n        save_total_limit=2,\n        load_best_model_at_end=True,\n        metric_for_best_model=\"eval_loss\",\n        report_to=\"none\",\n        remove_unused_columns=False,\n        save_safetensors=False,\n    )\n    \n    trainer = CustomTrainer(\n        model=model,\n        args=training_args,\n        train_dataset=train_dataset,\n        eval_dataset=val_dataset,\n        tokenizer=tokenizer,\n        data_collator=data_collator,\n    )\n    \n    train_result = trainer.train()\n    \n    # Save\n    model_to_save = model.module if hasattr(model, 'module') else model\n    model_to_save.base_model.save_pretrained(f\"{output_dir}/final\")\n    \n    torch.save({\n        'multi_head_attention': model_to_save.multi_head_attention.state_dict(),\n        'bilstm': model_to_save.bilstm.state_dict(),\n        'rnn': model_to_save.rnn.state_dict(),\n        'ffn': model_to_save.ffn.state_dict(),\n        'gate': model_to_save.gate.state_dict(),\n    }, f\"{output_dir}/final/advanced_layers.pt\")\n    \n    tokenizer.save_pretrained(f\"{output_dir}/final\")\n    \n    print(f\"\\n‚úÖ Training completed!\")\n    print(f\"‚è±Ô∏è  Time: {train_result.metrics['train_runtime']/60:.1f} min\")\n    print(f\"üìä Loss: {train_result.metrics['train_loss']:.4f}\")\n    \n    return trainer, train_result\n\n\n# CH·∫†Y MODEL 1\nmodel1, tokenizer1 = setup_vit5_base()\ntokenized_train1, tokenized_val1, tokenized_test1 = tokenize_t5(tokenizer1, train_df, val_df, test_df)\ntrainer1, result1 = train_vit5_base(model1, tokenizer1, tokenized_train1, tokenized_val1)\n\nprint(\"\\n‚úÖ Model 1 (ViT5-Base) Ready for Evaluation\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-27T01:39:06.573856Z","iopub.execute_input":"2025-11-27T01:39:06.574099Z","iopub.status.idle":"2025-11-27T01:43:13.931184Z","shell.execute_reply.started":"2025-11-27T01:39:06.574073Z","shell.execute_reply":"2025-11-27T01:43:13.929535Z"}},"outputs":[{"name":"stdout","text":"\n======================================================================\nüîß MODEL 1: ViT5-Base (CODE C≈®)\n======================================================================\n","output_type":"stream"},{"name":"stderr","text":"'(MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /VietAI/vit5-base/resolve/main/tokenizer_config.json (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x7acb43a2c050>: Failed to resolve \\'huggingface.co\\' ([Errno -3] Temporary failure in name resolution)\"))'), '(Request ID: ec0efa82-fa64-43c7-bb58-c54048b3160e)')' thrown while requesting HEAD https://huggingface.co/VietAI/vit5-base/resolve/main/tokenizer_config.json\nWARNING:huggingface_hub.utils._http:'(MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /VietAI/vit5-base/resolve/main/tokenizer_config.json (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x7acb43a2c050>: Failed to resolve \\'huggingface.co\\' ([Errno -3] Temporary failure in name resolution)\"))'), '(Request ID: ec0efa82-fa64-43c7-bb58-c54048b3160e)')' thrown while requesting HEAD https://huggingface.co/VietAI/vit5-base/resolve/main/tokenizer_config.json\nRetrying in 1s [Retry 1/5].\nWARNING:huggingface_hub.utils._http:Retrying in 1s [Retry 1/5].\n'(MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /VietAI/vit5-base/resolve/main/tokenizer_config.json (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x7acb43d71090>: Failed to resolve \\'huggingface.co\\' ([Errno -3] Temporary failure in name resolution)\"))'), '(Request ID: cac8de6d-51d0-4af0-be5d-1a2a1f00a27d)')' thrown while requesting HEAD https://huggingface.co/VietAI/vit5-base/resolve/main/tokenizer_config.json\nWARNING:huggingface_hub.utils._http:'(MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /VietAI/vit5-base/resolve/main/tokenizer_config.json (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x7acb43d71090>: Failed to resolve \\'huggingface.co\\' ([Errno -3] Temporary failure in name resolution)\"))'), '(Request ID: cac8de6d-51d0-4af0-be5d-1a2a1f00a27d)')' thrown while requesting HEAD https://huggingface.co/VietAI/vit5-base/resolve/main/tokenizer_config.json\nRetrying in 2s [Retry 2/5].\nWARNING:huggingface_hub.utils._http:Retrying in 2s [Retry 2/5].\n'(MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /VietAI/vit5-base/resolve/main/tokenizer_config.json (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x7acca4056010>: Failed to resolve \\'huggingface.co\\' ([Errno -3] Temporary failure in name resolution)\"))'), '(Request ID: b1d13596-b7da-4812-9ec1-f6cc3870459c)')' thrown while requesting HEAD https://huggingface.co/VietAI/vit5-base/resolve/main/tokenizer_config.json\nWARNING:huggingface_hub.utils._http:'(MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /VietAI/vit5-base/resolve/main/tokenizer_config.json (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x7acca4056010>: Failed to resolve \\'huggingface.co\\' ([Errno -3] Temporary failure in name resolution)\"))'), '(Request ID: b1d13596-b7da-4812-9ec1-f6cc3870459c)')' thrown while requesting HEAD https://huggingface.co/VietAI/vit5-base/resolve/main/tokenizer_config.json\nRetrying in 4s [Retry 3/5].\nWARNING:huggingface_hub.utils._http:Retrying in 4s [Retry 3/5].\n'(MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /VietAI/vit5-base/resolve/main/tokenizer_config.json (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x7acb43d4eb90>: Failed to resolve \\'huggingface.co\\' ([Errno -3] Temporary failure in name resolution)\"))'), '(Request ID: 85c3bb11-857b-4411-a44f-d29881859a7d)')' thrown while requesting HEAD https://huggingface.co/VietAI/vit5-base/resolve/main/tokenizer_config.json\nWARNING:huggingface_hub.utils._http:'(MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /VietAI/vit5-base/resolve/main/tokenizer_config.json (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x7acb43d4eb90>: Failed to resolve \\'huggingface.co\\' ([Errno -3] Temporary failure in name resolution)\"))'), '(Request ID: 85c3bb11-857b-4411-a44f-d29881859a7d)')' thrown while requesting HEAD https://huggingface.co/VietAI/vit5-base/resolve/main/tokenizer_config.json\nRetrying in 8s [Retry 4/5].\nWARNING:huggingface_hub.utils._http:Retrying in 8s [Retry 4/5].\n'(MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /VietAI/vit5-base/resolve/main/tokenizer_config.json (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x7acb449e96d0>: Failed to resolve \\'huggingface.co\\' ([Errno -3] Temporary failure in name resolution)\"))'), '(Request ID: 860e37bb-c94c-4b69-924d-13241a0678a0)')' thrown while requesting HEAD https://huggingface.co/VietAI/vit5-base/resolve/main/tokenizer_config.json\nWARNING:huggingface_hub.utils._http:'(MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /VietAI/vit5-base/resolve/main/tokenizer_config.json (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x7acb449e96d0>: Failed to resolve \\'huggingface.co\\' ([Errno -3] Temporary failure in name resolution)\"))'), '(Request ID: 860e37bb-c94c-4b69-924d-13241a0678a0)')' thrown while requesting HEAD https://huggingface.co/VietAI/vit5-base/resolve/main/tokenizer_config.json\nRetrying in 8s [Retry 5/5].\nWARNING:huggingface_hub.utils._http:Retrying in 8s [Retry 5/5].\n'(MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /VietAI/vit5-base/resolve/main/tokenizer_config.json (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x7acb43c98410>: Failed to resolve \\'huggingface.co\\' ([Errno -3] Temporary failure in name resolution)\"))'), '(Request ID: 10201138-c659-4c84-871c-143d8956194d)')' thrown while requesting HEAD https://huggingface.co/VietAI/vit5-base/resolve/main/tokenizer_config.json\nWARNING:huggingface_hub.utils._http:'(MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /VietAI/vit5-base/resolve/main/tokenizer_config.json (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x7acb43c98410>: Failed to resolve \\'huggingface.co\\' ([Errno -3] Temporary failure in name resolution)\"))'), '(Request ID: 10201138-c659-4c84-871c-143d8956194d)')' thrown while requesting HEAD https://huggingface.co/VietAI/vit5-base/resolve/main/tokenizer_config.json\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mgaierror\u001b[0m                                  Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/urllib3/connection.py\u001b[0m in \u001b[0;36m_new_conn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    197\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 198\u001b[0;31m             sock = connection.create_connection(\n\u001b[0m\u001b[1;32m    199\u001b[0m                 \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dns_host\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mport\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/urllib3/util/connection.py\u001b[0m in \u001b[0;36mcreate_connection\u001b[0;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mres\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetaddrinfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mport\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfamily\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSOCK_STREAM\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m         \u001b[0maf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msocktype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproto\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcanonname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msa\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/socket.py\u001b[0m in \u001b[0;36mgetaddrinfo\u001b[0;34m(host, port, family, type, proto, flags)\u001b[0m\n\u001b[1;32m    973\u001b[0m     \u001b[0maddrlist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 974\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mres\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_socket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetaddrinfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mport\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfamily\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproto\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    975\u001b[0m         \u001b[0maf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msocktype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproto\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcanonname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msa\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mgaierror\u001b[0m: [Errno -3] Temporary failure in name resolution","\nThe above exception was the direct cause of the following exception:\n","\u001b[0;31mNameResolutionError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    786\u001b[0m             \u001b[0;31m# Make the request on the HTTPConnection object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 787\u001b[0;31m             response = self._make_request(\n\u001b[0m\u001b[1;32m    788\u001b[0m                 \u001b[0mconn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    487\u001b[0m                 \u001b[0mnew_e\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_wrap_proxy_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_e\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproxy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscheme\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 488\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mnew_e\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    489\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    463\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 464\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_conn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    465\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mSocketTimeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBaseSSLError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_validate_conn\u001b[0;34m(self, conn)\u001b[0m\n\u001b[1;32m   1092\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_closed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1093\u001b[0;31m             \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1094\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/urllib3/connection.py\u001b[0m in \u001b[0;36mconnect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    752\u001b[0m             \u001b[0msock\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msocket\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0mssl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSSLSocket\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 753\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msock\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msock\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_new_conn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    754\u001b[0m             \u001b[0mserver_hostname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhost\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/urllib3/connection.py\u001b[0m in \u001b[0;36m_new_conn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    204\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgaierror\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 205\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mNameResolutionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    206\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mSocketTimeout\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameResolutionError\u001b[0m: <urllib3.connection.HTTPSConnection object at 0x7acb43c980d0>: Failed to resolve 'huggingface.co' ([Errno -3] Temporary failure in name resolution)","\nThe above exception was the direct cause of the following exception:\n","\u001b[0;31mMaxRetryError\u001b[0m                             Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/requests/adapters.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    643\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 644\u001b[0;31m             resp = conn.urlopen(\n\u001b[0m\u001b[1;32m    645\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    840\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 841\u001b[0;31m             retries = retries.increment(\n\u001b[0m\u001b[1;32m    842\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnew_e\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_pool\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_stacktrace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/urllib3/util/retry.py\u001b[0m in \u001b[0;36mincrement\u001b[0;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[1;32m    518\u001b[0m             \u001b[0mreason\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0merror\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mResponseError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcause\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 519\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mMaxRetryError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_pool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreason\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mreason\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    520\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mMaxRetryError\u001b[0m: HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /api/models/VietAI/vit5-base/tree/main/additional_chat_templates?recursive=False&expand=False (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x7acb43c980d0>: Failed to resolve 'huggingface.co' ([Errno -3] Temporary failure in name resolution)\"))","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mConnectionError\u001b[0m                           Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_47/2522525680.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[0;31m# CH·∫†Y MODEL 1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m \u001b[0mmodel1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msetup_vit5_base\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m \u001b[0mtokenized_train1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenized_val1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenized_test1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenize_t5\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[0mtrainer1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_vit5_base\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenized_train1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenized_val1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_47/2522525680.py\u001b[0m in \u001b[0;36msetup_vit5_base\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;31m# Tokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     tokenizer = T5Tokenizer.from_pretrained(\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0mmodel_config\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'name'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mmodel_max_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, trust_remote_code, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   1955\u001b[0m                                 )\n\u001b[1;32m   1956\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1957\u001b[0;31m                         for template in list_repo_templates(\n\u001b[0m\u001b[1;32m   1958\u001b[0m                             \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1959\u001b[0m                             \u001b[0mlocal_files_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlocal_files_only\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/hub.py\u001b[0m in \u001b[0;36mlist_repo_templates\u001b[0;34m(repo_id, local_files_only, revision, cache_dir)\u001b[0m\n\u001b[1;32m    159\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mlocal_files_only\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m             return [\n\u001b[0m\u001b[1;32m    162\u001b[0m                 \u001b[0mentry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremoveprefix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{CHAT_TEMPLATE_DIR}/\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m                 for entry in list_repo_tree(\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/hub.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    159\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mlocal_files_only\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m             return [\n\u001b[0m\u001b[1;32m    162\u001b[0m                 \u001b[0mentry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremoveprefix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{CHAT_TEMPLATE_DIR}/\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m                 for entry in list_repo_tree(\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/hf_api.py\u001b[0m in \u001b[0;36mlist_repo_tree\u001b[0;34m(self, repo_id, path_in_repo, recursive, expand, revision, repo_type, token)\u001b[0m\n\u001b[1;32m   3189\u001b[0m         \u001b[0mencoded_path_in_repo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mquote\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_in_repo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msafe\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpath_in_repo\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3190\u001b[0m         \u001b[0mtree_url\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"{self.endpoint}/api/{repo_type}s/{repo_id}/tree/{revision}{encoded_path_in_repo}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3191\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mpath_info\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpaginate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtree_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"recursive\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mrecursive\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"expand\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mexpand\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3192\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mRepoFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mpath_info\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpath_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"type\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"file\"\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mRepoFolder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mpath_info\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_pagination.py\u001b[0m in \u001b[0;36mpaginate\u001b[0;34m(path, params, headers)\u001b[0m\n\u001b[1;32m     34\u001b[0m     \"\"\"\n\u001b[1;32m     35\u001b[0m     \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m     \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m     \u001b[0mhf_raise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/requests/sessions.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, url, **kwargs)\u001b[0m\n\u001b[1;32m    600\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    601\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetdefault\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"allow_redirects\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 602\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"GET\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    603\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/requests/sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    587\u001b[0m         }\n\u001b[1;32m    588\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 589\u001b[0;31m         \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    590\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    591\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/requests/sessions.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    701\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    702\u001b[0m         \u001b[0;31m# Send the request\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 703\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madapter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    704\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    705\u001b[0m         \u001b[0;31m# Total elapsed time of the request (approximately)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_http.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, *args, **kwargs)\u001b[0m\n\u001b[1;32m     93\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Send: {_curlify(request)}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRequestException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m             \u001b[0mrequest_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_AMZN_TRACE_ID\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/requests/adapters.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    675\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mSSLError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    676\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 677\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    678\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    679\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mClosedPoolError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mConnectionError\u001b[0m: (MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /api/models/VietAI/vit5-base/tree/main/additional_chat_templates?recursive=False&expand=False (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x7acb43c980d0>: Failed to resolve \\'huggingface.co\\' ([Errno -3] Temporary failure in name resolution)\"))'), '(Request ID: 197e6b06-13fe-487d-ace8-c45c58677c3c)')"],"ename":"ConnectionError","evalue":"(MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /api/models/VietAI/vit5-base/tree/main/additional_chat_templates?recursive=False&expand=False (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x7acb43c980d0>: Failed to resolve \\'huggingface.co\\' ([Errno -3] Temporary failure in name resolution)\"))'), '(Request ID: 197e6b06-13fe-487d-ace8-c45c58677c3c)')","output_type":"error"}],"execution_count":5},{"cell_type":"code","source":"# ========================================\n# CELL 6: MODEL 2 - GPT-Neo Vietnamese (M·ªöI)\n# ========================================\n\ndef setup_gpt_neo():\n    \"\"\"Setup GPT-Neo Vietnamese - DECODER-ONLY\"\"\"\n    \n    model_config = config.MODELS[\"gpt-neo-vi\"]\n    \n    print(\"\\n\" + \"=\"*70)\n    print(\"üîß MODEL 2: GPT-Neo-1.3B Vietnamese (M·ªöI)\")\n    print(\"=\"*70)\n    \n    # Tokenizer\n    tokenizer = AutoTokenizer.from_pretrained(model_config['name'])\n    if tokenizer.pad_token is None:\n        tokenizer.pad_token = tokenizer.eos_token\n    \n    # Model\n    model = AutoModelForCausalLM.from_pretrained(\n        model_config['name'],\n        torch_dtype=torch.bfloat16,\n        low_cpu_mem_usage=True\n    )\n    model.config.pad_token_id = tokenizer.pad_token_id\n    \n    # LoRA\n    peft_config = LoraConfig(\n        task_type=TaskType.CAUSAL_LM,\n        r=model_config['lora_r'],\n        lora_alpha=model_config['lora_alpha'],\n        lora_dropout=0.05,\n        target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"out_proj\", \"c_fc\", \"c_proj\"],\n        inference_mode=False,\n        bias=\"none\"\n    )\n    model = get_peft_model(model, peft_config)\n    model = model.to(\"cuda\")\n    \n    model.print_trainable_parameters()\n    print(\"‚úÖ GPT-Neo: Decoder-Only Architecture\")\n    \n    return model, tokenizer\n\n\ndef tokenize_gpt(tokenizer, train_df, val_df, test_df):\n    \"\"\"Tokenize cho GPT - Causal LM\"\"\"\n    \n    def tokenize_function(examples):\n        # Format: input + target\n        texts = [\n            f\"{inp} {tokenizer.eos_token} {tgt} {tokenizer.eos_token}\"\n            for inp, tgt in zip(examples['input_text'], examples['target_text'])\n        ]\n        \n        model_inputs = tokenizer(\n            texts,\n            max_length=config.MAX_INPUT_LENGTH + config.MAX_TARGET_LENGTH,\n            truncation=True,\n            padding=\"max_length\"\n        )\n        \n        model_inputs[\"labels\"] = model_inputs[\"input_ids\"].copy()\n        \n        return model_inputs\n    \n    train_dataset = Dataset.from_pandas(train_df[['input_text', 'target_text']])\n    val_dataset = Dataset.from_pandas(val_df[['input_text', 'target_text']])\n    test_dataset = Dataset.from_pandas(test_df[['input_text', 'target_text']])\n    \n    tokenized_train = train_dataset.map(tokenize_function, batched=True, remove_columns=train_dataset.column_names)\n    tokenized_val = val_dataset.map(tokenize_function, batched=True, remove_columns=val_dataset.column_names)\n    tokenized_test = test_dataset.map(tokenize_function, batched=True, remove_columns=test_dataset.column_names)\n    \n    print(f\"‚úÖ Tokenized: Train={len(tokenized_train)}, Val={len(tokenized_val)}, Test={len(tokenized_test)}\")\n    \n    return tokenized_train, tokenized_val, tokenized_test\n\n\ndef train_gpt_neo(model, tokenizer, train_dataset, val_dataset):\n    \"\"\"Train GPT-Neo\"\"\"\n    \n    model_config = config.MODELS[\"gpt-neo-vi\"]\n    output_dir = f\"{config.OUTPUT_DIR}/gpt-neo-vi\"\n    os.makedirs(output_dir, exist_ok=True)\n    \n    print(\"\\nüöÄ Training GPT-Neo...\")\n    \n    data_collator = DataCollatorForLanguageModeling(\n        tokenizer=tokenizer,\n        mlm=False\n    )\n    \n    training_args = TrainingArguments(\n        output_dir=output_dir,\n        num_train_epochs=config.NUM_EPOCHS,\n        per_device_train_batch_size=model_config['batch_size'],\n        per_device_eval_batch_size=model_config['batch_size'],\n        gradient_accumulation_steps=model_config['gradient_accumulation'],\n        learning_rate=model_config['learning_rate'],\n        weight_decay=0.01,\n        max_grad_norm=1.0,\n        warmup_ratio=0.1,\n        bf16=True,\n        \n        do_eval=True,              # ‚úÖ TH√äM\n        eval_strategy=\"steps\",     # ‚úÖ TH√äM\n        \n        logging_steps=50,\n        eval_steps=200,\n        save_steps=200,\n        save_total_limit=2,\n        load_best_model_at_end=True,\n        metric_for_best_model=\"eval_loss\",\n        report_to=\"none\",\n        save_safetensors=False,\n    )\n    \n    trainer = Trainer(\n        model=model,\n        args=training_args,\n        train_dataset=train_dataset,\n        eval_dataset=val_dataset,\n        tokenizer=tokenizer,\n        data_collator=data_collator,\n    )\n    \n    train_result = trainer.train()\n    \n    # Save\n    model.save_pretrained(f\"{output_dir}/final\")\n    tokenizer.save_pretrained(f\"{output_dir}/final\")\n    \n    print(f\"\\n‚úÖ Training completed!\")\n    print(f\"‚è±Ô∏è  Time: {train_result.metrics['train_runtime']/60:.1f} min\")\n    print(f\"üìä Loss: {train_result.metrics['train_loss']:.4f}\")\n    \n    return trainer, train_result\n\n\n# CH·∫†Y MODEL 2\nmodel2, tokenizer2 = setup_gpt_neo()\ntokenized_train2, tokenized_val2, tokenized_test2 = tokenize_gpt(tokenizer2, train_df, val_df, test_df)\ntrainer2, result2 = train_gpt_neo(model2, tokenizer2, tokenized_train2, tokenized_val2)\n\n# Cleanup\ndel model2, trainer2\ntorch.cuda.empty_cache()\ngc.collect()\n\nprint(\"\\n‚úÖ Model 2 (GPT-Neo) Completed\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-27T01:43:13.932030Z","iopub.status.idle":"2025-11-27T01:43:13.932291Z","shell.execute_reply.started":"2025-11-27T01:43:13.932174Z","shell.execute_reply":"2025-11-27T01:43:13.932184Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ========================================\n# CELL 7: MODEL 3 - mT5-Base (M·ªöI - SIMPLE)\n# ========================================\n\ndef setup_mt5_base():\n    \"\"\"Setup mT5-Base - ƒê∆†N GI·∫¢N (kh√¥ng d√πng advanced arch)\"\"\"\n    \n    model_config = config.MODELS[\"mt5-base\"]\n    \n    print(\"\\n\" + \"=\"*70)\n    print(\"üîß MODEL 3: mT5-Base (M·ªöI - SIMPLE)\")\n    print(\"=\"*70)\n    \n    # Tokenizer\n    tokenizer = MT5Tokenizer.from_pretrained(\n        model_config['name'],\n        model_max_length=512,\n        legacy=False\n    )\n    special_tokens = {\"additional_special_tokens\": [\"<TIKTOK>\", \"<FACEBOOK>\", \"<YOUTUBE>\", \"<COMMENT>\"]}\n    tokenizer.add_special_tokens(special_tokens)\n    \n    # Model - VANILLA (kh√¥ng wrap)\n    model = MT5ForConditionalGeneration.from_pretrained(\n        model_config['name'],\n        torch_dtype=torch.bfloat16,\n        low_cpu_mem_usage=True\n    )\n    model.resize_token_embeddings(len(tokenizer))\n    \n    # LoRA\n    peft_config = LoraConfig(\n        task_type=TaskType.SEQ_2_SEQ_LM,\n        r=model_config['lora_r'],\n        lora_alpha=model_config['lora_alpha'],\n        lora_dropout=0.05,\n        target_modules=[\"q\", \"v\", \"k\", \"o\", \"wi_0\", \"wi_1\", \"wo\"],\n        inference_mode=False,\n        bias=\"none\"\n    )\n    model = get_peft_model(model, peft_config)\n    model = model.to(\"cuda\")\n    \n    model.print_trainable_parameters()\n    print(\"‚úÖ mT5-Base: Vanilla Encoder-Decoder (No Custom Layers)\")\n    \n    return model, tokenizer\n\n\ndef train_mt5_base(model, tokenizer, train_dataset, val_dataset):\n    \"\"\"Train mT5-Base\"\"\"\n    \n    model_config = config.MODELS[\"mt5-base\"]\n    output_dir = f\"{config.OUTPUT_DIR}/mt5-base\"\n    os.makedirs(output_dir, exist_ok=True)\n    \n    print(\"\\nüöÄ Training mT5-Base...\")\n    \n    data_collator = DataCollatorForSeq2Seq(\n        tokenizer=tokenizer,\n        model=model,\n        padding=True,\n        pad_to_multiple_of=8\n    )\n    \n    training_args = TrainingArguments(\n        output_dir=output_dir,\n        num_train_epochs=config.NUM_EPOCHS,\n        per_device_train_batch_size=model_config['batch_size'],\n        per_device_eval_batch_size=model_config['batch_size'],\n        gradient_accumulation_steps=model_config['gradient_accumulation'],\n        learning_rate=model_config['learning_rate'],\n        weight_decay=0.01,\n        warmup_ratio=0.1,\n        bf16=True,\n        eval_strategy=\"steps\",\n        save_strategy=\"steps\",\n        logging_steps=50,\n        eval_steps=200,\n        save_steps=200,\n        save_total_limit=2,\n        load_best_model_at_end=True,\n        metric_for_best_model=\"eval_loss\",\n        report_to=\"none\",\n    )\n    \n    trainer = Trainer(\n        model=model,\n        args=training_args,\n        train_dataset=train_dataset,\n        eval_dataset=val_dataset,\n        tokenizer=tokenizer,\n        data_collator=data_collator,\n    )\n    \n    train_result = trainer.train()\n    \n    # üî• SAVE STRATEGY M·ªöI\n    print(\"üî• Merging LoRA and saving...\")\n    merged_model = model.merge_and_unload()\n    \n    save_path = f\"{output_dir}/final\"\n    merged_model.save_pretrained(save_path)\n    tokenizer.save_pretrained(save_path)\n    \n    # ‚úÖ L∆ØU TH√äM CONFIG V·ªÄ VOCAB SIZE\n    with open(f\"{save_path}/model_info.json\", \"w\") as f:\n        json.dump({\n            \"vocab_size\": len(tokenizer),\n            \"model_vocab_size\": merged_model.config.vocab_size,\n            \"num_special_tokens\": len(tokenizer) - merged_model.config.vocab_size\n        }, f, indent=2)\n    \n    print(f\"\\n‚úÖ Model saved to: {save_path}\")\n    print(f\"   Vocab size: {len(tokenizer)}\")\n    print(f\"‚è±Ô∏è  Training time: {train_result.metrics['train_runtime']/60:.1f} min\")\n    print(f\"üìä Train loss: {train_result.metrics['train_loss']:.4f}\")\n    \n    return trainer, train_result\n\n\n\n# CH·∫†Y MODEL 3\nmodel3, tokenizer3 = setup_mt5_base()\ntokenized_train3, tokenized_val3, tokenized_test3 = tokenize_t5(tokenizer3, train_df, val_df, test_df)  # D√πng chung tokenize_t5\ntrainer3, result3 = train_mt5_base(model3, tokenizer3, tokenized_train3, tokenized_val3)\n\n# Cleanup\ndel model3, trainer3\ntorch.cuda.empty_cache()\ngc.collect()\n\nprint(\"\\n‚úÖ Model 3 (mT5-Base) Completed\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-27T01:43:13.933826Z","iopub.status.idle":"2025-11-27T01:43:13.934365Z","shell.execute_reply.started":"2025-11-27T01:43:13.934170Z","shell.execute_reply":"2025-11-27T01:43:13.934187Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# %%\n# ========================================\n# CELL 8: EVALUATION - ALL 3 MODELS (FIXED)\n# ========================================\n\ndef evaluate_t5_model(model, tokenizer, test_dataset, model_name):\n    \"\"\"Evaluate T5/mT5\"\"\"\n    \n    print(f\"\\n{'='*70}\")\n    print(f\"üìä EVALUATING: {model_name}\")\n    print(f\"{'='*70}\")\n    \n    model.eval()\n    predictions = []\n    references = []\n    \n    from torch.utils.data import DataLoader\n    dataloader = DataLoader(test_dataset, batch_size=8)\n    \n    total_time = 0\n    \n    with torch.no_grad():\n        for batch in tqdm(dataloader, desc=\"Generating\"):\n            input_ids = batch['input_ids'].to(\"cuda\")\n            attention_mask = batch['attention_mask'].to(\"cuda\")\n    \n            start_time = datetime.now()\n            outputs = model.generate(\n                input_ids=input_ids,\n                attention_mask=attention_mask,\n                max_length=config.MAX_TARGET_LENGTH,\n                num_beams=4,\n                early_stopping=True\n            )\n            total_time += (datetime.now() - start_time).total_seconds()\n    \n            preds = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n            refs = tokenizer.batch_decode(batch['labels'], skip_special_tokens=True)\n    \n            predictions.extend(preds)\n            references.extend(refs)\n    \n    # SIMPLE BLEU\n    from collections import Counter\n    def simple_bleu(pred, ref):\n        pred_words = pred.split()\n        ref_words = ref.split()\n        if len(pred_words) == 0:\n            return 0.0\n        matches = sum((Counter(pred_words) & Counter(ref_words)).values())\n        return matches / len(pred_words)\n    \n    bleu_scores = [simple_bleu(p, r) for p, r in zip(predictions, references)]\n    avg_bleu = sum(bleu_scores) / len(bleu_scores) if bleu_scores else 0.0\n    avg_time = total_time / len(test_dataset)\n    \n    print(f\"‚úÖ BLEU: {avg_bleu:.4f}\")\n    print(f\"‚úÖ Inference Time: {avg_time:.4f}s/sample\")\n    \n    return {\n        \"bleu\": avg_bleu,\n        \"inference_time\": avg_time,\n        \"predictions\": predictions[:10],\n        \"references\": references[:10]\n    }\n\n\ndef evaluate_gpt_model(model, tokenizer, test_df, model_name):\n    \"\"\"Evaluate GPT\"\"\"\n    \n    print(f\"\\n{'='*70}\")\n    print(f\"üìä EVALUATING: {model_name}\")\n    print(f\"{'='*70}\")\n    \n    model.eval()\n    predictions = []\n    references = test_df['target_text'].tolist()[:100]\n    \n    total_time = 0\n    \n    with torch.no_grad():\n        for idx, row in tqdm(test_df.head(100).iterrows(), total=100, desc=\"Generating\"):\n            input_text = row['input_text']\n            \n            inputs = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\n            \n            start_time = datetime.now()\n            outputs = model.generate(\n                **inputs,\n                max_new_tokens=config.MAX_TARGET_LENGTH,\n                num_beams=2,\n                early_stopping=True,\n                pad_token_id=tokenizer.pad_token_id,\n                eos_token_id=tokenizer.eos_token_id\n            )\n            total_time += (datetime.now() - start_time).total_seconds()\n            \n            input_length = inputs['input_ids'].shape[1]\n            generated = outputs[0][input_length:]\n            pred_text = tokenizer.decode(generated, skip_special_tokens=True)\n            predictions.append(pred_text)\n    \n    # SIMPLE BLEU\n    from collections import Counter\n    def simple_bleu(pred, ref):\n        pred_words = pred.split()\n        ref_words = ref.split()\n        if len(pred_words) == 0:\n            return 0.0\n        matches = sum((Counter(pred_words) & Counter(ref_words)).values())\n        return matches / len(pred_words)\n    \n    bleu_scores = [simple_bleu(p, r) for p, r in zip(predictions, references)]\n    avg_bleu = sum(bleu_scores) / len(bleu_scores) if bleu_scores else 0.0\n    avg_time = total_time / len(predictions)\n    \n    print(f\"‚úÖ BLEU: {avg_bleu:.4f}\")\n    print(f\"‚úÖ Inference Time: {avg_time:.4f}s/sample\")\n    \n    return {\n        \"bleu\": avg_bleu,\n        \"inference_time\": avg_time,\n        \"predictions\": predictions[:10],\n        \"references\": references[:10]\n    }\n\n\n# ========================================\n# üî• FIXED LOADING FUNCTIONS\n# ========================================\n\ndef load_mt5_for_eval(model_path):\n    print(f\"\\nüîß Loading mT5 from: {model_path}\")\n    \n    tokenizer = MT5Tokenizer.from_pretrained(model_path)\n    \n    model = MT5ForConditionalGeneration.from_pretrained(\n        model_path,\n        torch_dtype=torch.bfloat16\n    ).to(\"cuda\")\n    \n    if len(tokenizer) != model.config.vocab_size:\n        print(\"‚ö†Ô∏è Vocab mismatch ‚Üí resizing...\")\n        model.resize_token_embeddings(len(tokenizer))\n    \n    print(\"‚úÖ mT5 loaded successfully!\")\n    return model, tokenizer\n\n\ndef load_vit5_for_eval(model_path):\n    print(f\"\\nüîß Loading ViT5 from: {model_path}\")\n\n    # Load tokenizer ƒë√∫ng c√°ch (local)\n    tokenizer = T5Tokenizer.from_pretrained(\n        model_path,\n        local_files_only=True\n    )\n\n    # Load base model\n    base_model = T5ForConditionalGeneration.from_pretrained(\n        model_path,\n        torch_dtype=torch.bfloat16\n    )\n\n    # Wrap advanced architecture\n    model = T5WithAdvancedArchitecture(\n        base_model=base_model,\n        hidden_size=768,\n        num_heads=12\n    )\n\n    # Load custom layers\n    custom_layers_path = f\"{model_path}/advanced_layers.pt\"\n    if os.path.exists(custom_layers_path):\n        print(\"   Loading custom layers...\")\n        state = torch.load(custom_layers_path, map_location=\"cuda\")\n        model.multi_head_attention.load_state_dict(state[\"multi_head_attention\"])\n        model.bilstm.load_state_dict(state[\"bilstm\"])\n        model.rnn.load_state_dict(state[\"rnn\"])\n        model.ffn.load_state_dict(state[\"ffn\"])\n        model.gate.load_state_dict(state[\"gate\"])\n\n    model = model.to(\"cuda\")\n    print(\"‚úÖ ViT5 loaded successfully!\")\n    return model, tokenizer\n\n\ndef load_gpt_for_eval(model_path):\n    print(f\"\\nüîß Loading GPT-Neo from: {model_path}\")\n    \n    tokenizer = AutoTokenizer.from_pretrained(model_path)\n    model = AutoModelForCausalLM.from_pretrained(\n        model_path,\n        torch_dtype=torch.bfloat16\n    ).to(\"cuda\")\n    \n    print(\"‚úÖ GPT-Neo loaded successfully!\")\n    return model, tokenizer\n\n\n# ========================================\n# üöÄ EVALUATE ALL MODELS\n# ========================================\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"üìä EVALUATING ALL 3 MODELS\")\nprint(\"=\"*70)\n\n# Model 1: ViT5 (ƒëang c√≥ trong RAM)\nmetrics1 = evaluate_t5_model(model1, tokenizer1, tokenized_test1, \"ViT5-Base\")\n\n# Model 2: GPT-Neo\nmodel2, tokenizer2_reload = load_gpt_for_eval(f\"{config.OUTPUT_DIR}/gpt-neo-vi/final\")\nmetrics2 = evaluate_gpt_model(model2, tokenizer2_reload, test_df, \"GPT-Neo\")\ndel model2, tokenizer2_reload\ntorch.cuda.empty_cache()\n\n# Model 3: mT5-Base\nmodel3, tokenizer3_reload = load_mt5_for_eval(f\"{config.OUTPUT_DIR}/mt5-base/final\")\nmetrics3 = evaluate_t5_model(model3, tokenizer3_reload, tokenized_test3, \"mT5-Base\")\ndel model3, tokenizer3_reload\ntorch.cuda.empty_cache()\n\nprint(\"\\n‚úÖ All Evaluations Complete!\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-27T01:51:40.720150Z","iopub.execute_input":"2025-11-27T01:51:40.720982Z","iopub.status.idle":"2025-11-27T01:51:40.760917Z","shell.execute_reply.started":"2025-11-27T01:51:40.720928Z","shell.execute_reply":"2025-11-27T01:51:40.759822Z"}},"outputs":[{"name":"stdout","text":"\n======================================================================\nüìä EVALUATING ALL 3 MODELS\n======================================================================\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_47/2547316455.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[0;31m# Model 1: ViT5 (ƒëang c√≥ trong RAM)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 210\u001b[0;31m \u001b[0mmetrics1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_t5_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenized_test1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ViT5-Base\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m \u001b[0;31m# Model 2: GPT-Neo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'model1' is not defined"],"ename":"NameError","evalue":"name 'model1' is not defined","output_type":"error"}],"execution_count":6},{"cell_type":"code","source":"# ========================================\n# CELL 9: FINAL COMPARISON & VISUALIZATION\n# ========================================\n\n# Compile results\nresults_df = pd.DataFrame({\n    \"Model\": [\"ViT5-Base\", \"GPT-Neo-1.3B\", \"mT5-Base\"],\n    \"Architecture\": [\n        \"T5 Enc-Dec + Advanced\",\n        \"GPT Decoder-Only\",\n        \"mT5 Enc-Dec Vanilla\"\n    ],\n    \"Params\": [\"220M\", \"1.3B\", \"580M\"],\n    \"Train Loss\": [\n        result1.metrics['train_loss'],\n        result2.metrics['train_loss'],\n        result3.metrics['train_loss']\n    ],\n    \"BLEU Score\": [\n        metrics1['bleu'],\n        metrics2['bleu'],\n        metrics3['bleu']\n    ],\n    \"Inference Time (s)\": [\n        metrics1['inference_time'],\n        metrics2['inference_time'],\n        metrics3['inference_time']\n    ],\n    \"Training Time (min)\": [\n        result1.metrics['train_runtime'] / 60,\n        result2.metrics['train_runtime'] / 60,\n        result3.metrics['train_runtime'] / 60\n    ]\n})\n\nprint(\"\\n\" + \"=\"*100)\nprint(\"üèÜ FINAL COMPARISON - 3 DIFFERENT MODELS\")\nprint(\"=\"*100)\nprint(results_df.to_string(index=False))\nprint(\"=\"*100)\n\n# Save\nresults_df.to_csv(f\"{config.OUTPUT_DIR}/comparison_results.csv\", index=False)\n\n# Best model\nbest_idx = results_df['BLEU Score'].idxmax()\nprint(f\"\\nü•á BEST MODEL: {results_df.loc[best_idx, 'Model']}\")\nprint(f\"   Architecture: {results_df.loc[best_idx, 'Architecture']}\")\nprint(f\"   BLEU: {results_df.loc[best_idx, 'BLEU Score']:.4f}\")\n\n# Sample predictions\nprint(\"\\n\" + \"=\"*100)\nprint(\"üìù SAMPLE PREDICTIONS\")\nprint(\"=\"*100)\n\nfor i in range(min(5, len(metrics1['predictions']))):\n    print(f\"\\n{'‚îÄ'*100}\")\n    print(f\"Example {i+1}\")\n    print(f\"{'‚îÄ'*100}\")\n    print(f\"Reference:\\n  {metrics1['references'][i]}\")\n    print(f\"\\nViT5-Base (Advanced):\\n  {metrics1['predictions'][i]}\")\n    print(f\"\\nGPT-Neo (Decoder):\\n  {metrics2['predictions'][i]}\")\n    print(f\"\\nmT5-Base (Vanilla):\\n  {metrics3['predictions'][i]}\")\n\n# Visualization\nplt.style.use('seaborn-v0_8-darkgrid')\nfig, axes = plt.subplots(2, 2, figsize=(16, 12))\n\ncolors = ['#3498db', '#e74c3c', '#2ecc71']\n\n# 1. BLEU\nax1 = axes[0, 0]\nbars = ax1.bar(results_df['Model'], results_df['BLEU Score'], color=colors)\nax1.set_title('BLEU Score', fontsize=14, fontweight='bold')\nax1.set_ylabel('BLEU')\nfor bar in bars:\n    height = bar.get_height()\n    ax1.text(bar.get_x() + bar.get_width()/2., height,\n             f'{height:.4f}', ha='center', va='bottom', fontsize=10)\n\n# 2. Loss\nax2 = axes[0, 1]\nbars = ax2.bar(results_df['Model'], results_df['Train Loss'], color=colors)\nax2.set_title('Training Loss', fontsize=14, fontweight='bold')\nax2.set_ylabel('Loss')\nfor bar in bars:\n    height = bar.get_height()\n    ax2.text(bar.get_x() + bar.get_width()/2., height,\n             f'{height:.4f}', ha='center', va='bottom', fontsize=10)\n\n# 3. Inference Speed\nax3 = axes[1, 0]\nbars = ax3.bar(results_df['Model'], results_df['Inference Time (s)'], color=colors)\nax3.set_title('Inference Speed', fontsize=14, fontweight='bold')\nax3.set_ylabel('Time (s)')\nfor bar in bars:\n    height = bar.get_height()\n    ax3.text(bar.get_x() + bar.get_width()/2., height,\n             f'{height:.4f}s', ha='center', va='bottom', fontsize=10)\n\n# 4. Training Time\nax4 = axes[1, 1]\nbars = ax4.bar(results_df['Model'], results_df['Training Time (min)'], color=colors)\nax4.set_title('Training Duration', fontsize=14, fontweight='bold')\nax4.set_ylabel('Time (min)')\nfor bar in bars:\n    height = bar.get_height()\n    ax4.text(bar.get_x() + bar.get_width()/2., height,\n             f'{height:.1f}m', ha='center', va='bottom', fontsize=10)\n\nplt.tight_layout()\nplt.savefig(f\"{config.OUTPUT_DIR}/comparison_charts.png\", dpi=300, bbox_inches='tight')\nplt.show()\n\nprint(f\"\\n‚úÖ Charts saved!\")\nprint(f\"\\nüéâ ALL DONE! Results in: {config.OUTPUT_DIR}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-27T01:43:13.937407Z","iopub.status.idle":"2025-11-27T01:43:13.937762Z","shell.execute_reply.started":"2025-11-27T01:43:13.937593Z","shell.execute_reply":"2025-11-27T01:43:13.937608Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}