{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "def1c05a",
   "metadata": {},
   "source": [
    "# B√ÄI 3 Nh·∫≠n di·ªán v·ªõi fast R-CNN v√† Yolo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b23695c9",
   "metadata": {},
   "source": [
    "### 3.a) D·ª± ƒëo√°n tr√™n m·ªôt ·∫£nh b·∫•t k·ª≥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b8cb9fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== SETUP M√îI TR∆Ø·ªúNG T·ªêI √ÇU VRAM ==========\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import requests\n",
    "from ultralytics import YOLO\n",
    "import numpy as np\n",
    "import gc\n",
    "import time\n",
    "from torchvision.models import detection\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ===== CUDA MEMORY MANAGEMENT =====\n",
    "def clear_cuda_memory():\n",
    "    \"\"\"D·ªçn d·∫πp CUDA memory ƒë·ªÉ tr√°nh tr√†n VRAM\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.synchronize()\n",
    "        gc.collect()\n",
    "        print(f\"üßπ CUDA Memory cleared\")\n",
    "        print(f\"üíæ Available CUDA Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")\n",
    "    else:\n",
    "        print(\"‚ùå CUDA not available\")\n",
    "\n",
    "def print_memory_usage():\n",
    "    \"\"\"Monitor CUDA memory usage\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        allocated = torch.cuda.memory_allocated() / 1024**3\n",
    "        reserved = torch.cuda.memory_reserved() / 1024**3\n",
    "        total = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
    "        print(f\"üìä Memory - Allocated: {allocated:.2f}GB | Reserved: {reserved:.2f}GB | Total: {total:.2f}GB\")\n",
    "\n",
    "# Clear memory tr∆∞·ªõc khi b·∫Øt ƒë·∫ßu\n",
    "clear_cuda_memory()\n",
    "\n",
    "# Device configuration v·ªõi memory efficiency\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"üéØ Using device: {device}\")\n",
    "\n",
    "# ===== LOAD V√Ä CHU·∫®N B·ªä ·∫¢NH TEST =====\n",
    "print(\"üñºÔ∏è Loading test image...\")\n",
    "url = \"https://images.cocodataset.org/val2017/000000039769.jpg\"\n",
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "resp = requests.get(url, verify=False).content\n",
    "img_array = np.asarray(bytearray(resp), dtype=np.uint8)\n",
    "img = cv2.imdecode(img_array, cv2.IMREAD_COLOR)\n",
    "img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# Hi·ªÉn th·ªã ·∫£nh g·ªëc\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.imshow(img_rgb)\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Test Image for Object Detection\", fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Environment setup completed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f570402",
   "metadata": {},
   "source": [
    "### 3.b) Hi·ªÉn th·ªã bounding box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f3f59d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== B√ÄI 3A: LOAD MODELS T·ªêI ∆ØU ==========\n",
    "\n",
    "# COCO class names ƒë·ªÉ hi·ªÉn th·ªã nh√£n\n",
    "COCO_CLASSES = [\n",
    "\"car\",\n",
    "\"threewheel\",\n",
    "\"bus\",\n",
    "\"truck\",\n",
    "\"motorbike\",\n",
    "\"van\",\n",
    "]\n",
    "\n",
    "print(\"üöÄ Loading pretrained models...\")\n",
    "print_memory_usage()\n",
    "\n",
    "# ===== 1. LOAD FASTER R-CNN T·ªêI ∆ØU =====\n",
    "print(\"\\nüì¶ Loading Faster R-CNN...\")\n",
    "faster_rcnn = detection.fasterrcnn_resnet50_fpn(weights=\"DEFAULT\")\n",
    "faster_rcnn.eval()\n",
    "faster_rcnn.to(device)\n",
    "print(\"‚úÖ Faster R-CNN loaded successfully\")\n",
    "\n",
    "# ===== 2. LOAD YOLO T·ªêI ∆ØU =====  \n",
    "print(\"\\nüì¶ Loading YOLO...\")\n",
    "yolo_model = YOLO(\"yolov8n.pt\")  # S·ª≠ d·ª•ng nano version ƒë·ªÉ ti·∫øt ki·ªám VRAM\n",
    "print(\"‚úÖ YOLO loaded successfully\")\n",
    "\n",
    "print_memory_usage()\n",
    "\n",
    "# ===== 3. CHU·∫®N B·ªä IMAGE PREPROCESSING =====\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Chuy·ªÉn ƒë·ªïi ·∫£nh sang tensor\n",
    "img_pil = Image.fromarray(img_rgb)\n",
    "img_tensor = transform(img_pil).unsqueeze(0).to(device)  # Th√™m batch dimension\n",
    "\n",
    "print(\"\\n‚úÖ Models loaded and ready for inference!\")\n",
    "print(f\"üìê Image tensor shape: {img_tensor.shape}\")\n",
    "print(f\"üéØ Device: {device}\")\n",
    "print(f\"üìä Faster R-CNN parameters: {sum(p.numel() for p in faster_rcnn.parameters()):,}\")\n",
    "print(f\"üìä YOLO model size: ~6MB (YOLOv8n)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39d1eb57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== B√ÄI 3B: INFERENCE V√Ä HI·ªÇN TH·ªä BOUNDING BOXES ==========\n",
    "\n",
    "def draw_boxes_frcnn(image, boxes, labels, scores, threshold=0.5):\n",
    "    \"\"\"V·∫Ω bounding boxes cho Faster R-CNN v·ªõi thi·∫øt k·∫ø ƒë·∫πp\"\"\"\n",
    "    img_draw = image.copy()\n",
    "    colors = [(255, 0, 0), (0, 255, 0), (0, 0, 255), (255, 255, 0), (255, 0, 255), (0, 255, 255)]\n",
    "    \n",
    "    for i, (box, label, score) in enumerate(zip(boxes, labels, scores)):\n",
    "        if score > threshold:\n",
    "            x1, y1, x2, y2 = [int(coord.item()) for coord in box]\n",
    "            color = colors[i % len(colors)]\n",
    "            \n",
    "            # V·∫Ω bounding box\n",
    "            cv2.rectangle(img_draw, (x1, y1), (x2, y2), color, 2)\n",
    "            \n",
    "            # T·∫°o label text\n",
    "            class_name = COCO_CLASSES[label.item()] if label.item() < len(COCO_CLASSES) else f\"Class_{label.item()}\"\n",
    "            text = f\"{class_name}: {score:.2f}\"\n",
    "            \n",
    "            # V·∫Ω background cho text\n",
    "            text_size = cv2.getTextSize(text, cv2.FONT_HERSHEY_SIMPLEX, 0.6, 2)[0]\n",
    "            cv2.rectangle(img_draw, (x1, y1-25), (x1 + text_size[0] + 10, y1), color, -1)\n",
    "            \n",
    "            # V·∫Ω text\n",
    "            cv2.putText(img_draw, text, (x1+5, y1-8), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255,255,255), 2)\n",
    "    \n",
    "    return img_draw\n",
    "\n",
    "def draw_boxes_yolo(results, original_image):\n",
    "    \"\"\"V·∫Ω bounding boxes cho YOLO\"\"\"\n",
    "    img_draw = original_image.copy()\n",
    "    \n",
    "    if len(results) > 0 and results[0].boxes is not None:\n",
    "        boxes = results[0].boxes\n",
    "        for box in boxes:\n",
    "            # L·∫•y t·ªça ƒë·ªô\n",
    "            x1, y1, x2, y2 = box.xyxy[0].cpu().numpy().astype(int)\n",
    "            conf = box.conf[0].cpu().item()\n",
    "            cls = int(box.cls[0].cpu().item())\n",
    "            \n",
    "            if conf > 0.5:  # Threshold\n",
    "                # V·∫Ω bounding box\n",
    "                cv2.rectangle(img_draw, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "                \n",
    "                # T·∫°o label text\n",
    "                class_name = results[0].names[cls]\n",
    "                text = f\"{class_name}: {conf:.2f}\"\n",
    "                \n",
    "                # V·∫Ω background v√† text\n",
    "                text_size = cv2.getTextSize(text, cv2.FONT_HERSHEY_SIMPLEX, 0.6, 2)[0]\n",
    "                cv2.rectangle(img_draw, (x1, y1-25), (x1 + text_size[0] + 10, y1), (0, 255, 0), -1)\n",
    "                cv2.putText(img_draw, text, (x1+5, y1-8), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255,255,255), 2)\n",
    "    \n",
    "    return img_draw\n",
    "\n",
    "print(\"üéØ Starting inference...\")\n",
    "\n",
    "# ===== INFERENCE FASTER R-CNN =====\n",
    "print(\"\\nüîç Faster R-CNN inference...\")\n",
    "with torch.no_grad():\n",
    "    frcnn_outputs = faster_rcnn(img_tensor)\n",
    "\n",
    "frcnn_boxes = frcnn_outputs[0][\"boxes\"]\n",
    "frcnn_labels = frcnn_outputs[0][\"labels\"] \n",
    "frcnn_scores = frcnn_outputs[0][\"scores\"]\n",
    "\n",
    "print(f\"üìä Faster R-CNN detected {len(frcnn_boxes)} objects\")\n",
    "print(f\"üìä Objects with confidence > 0.5: {sum(frcnn_scores > 0.5)}\")\n",
    "\n",
    "# ===== INFERENCE YOLO =====\n",
    "print(\"\\nüîç YOLO inference...\")\n",
    "yolo_results = yolo_model(img_rgb)\n",
    "yolo_detections = len(yolo_results[0].boxes) if yolo_results[0].boxes is not None else 0\n",
    "yolo_high_conf = sum(yolo_results[0].boxes.conf > 0.5) if yolo_results[0].boxes is not None else 0\n",
    "\n",
    "print(f\"üìä YOLO detected {yolo_detections} objects\")\n",
    "print(f\"üìä Objects with confidence > 0.5: {yolo_high_conf}\")\n",
    "\n",
    "# ===== VISUALIZATION =====\n",
    "print(\"\\nüé® Creating visualizations...\")\n",
    "\n",
    "# T·∫°o subplot\n",
    "fig, axes = plt.subplots(1, 3, figsize=(20, 6))\n",
    "\n",
    "# Original image\n",
    "axes[0].imshow(img_rgb)\n",
    "axes[0].set_title(\"Original Image\", fontsize=14, fontweight='bold')\n",
    "axes[0].axis('off')\n",
    "\n",
    "# Faster R-CNN results\n",
    "img_frcnn = draw_boxes_frcnn(img_rgb, frcnn_boxes, frcnn_labels, frcnn_scores)\n",
    "axes[1].imshow(img_frcnn)\n",
    "axes[1].set_title(f\"Faster R-CNN Detection\\n({sum(frcnn_scores > 0.5)} objects)\", fontsize=14, fontweight='bold')\n",
    "axes[1].axis('off')\n",
    "\n",
    "# YOLO results\n",
    "img_yolo = draw_boxes_yolo(yolo_results, img_rgb)\n",
    "axes[2].imshow(img_yolo)\n",
    "axes[2].set_title(f\"YOLO Detection\\n({yolo_high_conf} objects)\", fontsize=14, fontweight='bold')\n",
    "axes[2].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Inference and visualization completed!\")\n",
    "print_memory_usage()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7a7ad39",
   "metadata": {},
   "source": [
    "### 3.c) So s√°nh hi·ªáu su·∫•t (Precision, Recall, F1, mAP, FPS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbea96ce",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7528b192",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== B√ÄI 3C: SO S√ÅNH HI·ªÜU SU·∫§T (PRECISION, RECALL, F1, mAP, FPS) ==========\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "import seaborn as sns\n",
    "\n",
    "def calculate_iou(box1, box2):\n",
    "    \"\"\"T√≠nh IoU gi·ªØa 2 bounding boxes\"\"\"\n",
    "    x1 = max(box1[0], box2[0])\n",
    "    y1 = max(box1[1], box2[1]) \n",
    "    x2 = min(box1[2], box2[2])\n",
    "    y2 = min(box1[3], box2[3])\n",
    "    \n",
    "    if x2 <= x1 or y2 <= y1:\n",
    "        return 0.0\n",
    "    \n",
    "    intersection = (x2 - x1) * (y2 - y1)\n",
    "    area1 = (box1[2] - box1[0]) * (box1[3] - box1[1])\n",
    "    area2 = (box2[2] - box2[0]) * (box2[3] - box2[1])\n",
    "    union = area1 + area2 - intersection\n",
    "    \n",
    "    return intersection / union if union > 0 else 0.0\n",
    "\n",
    "def benchmark_fps(model, model_type, input_data, num_runs=50):\n",
    "    \"\"\"Benchmark FPS cho model\"\"\"\n",
    "    print(f\"\\n‚è±Ô∏è Benchmarking {model_type} FPS...\")\n",
    "    \n",
    "    # Warm up\n",
    "    for _ in range(10):\n",
    "        with torch.no_grad():\n",
    "            if model_type == \"Faster R-CNN\":\n",
    "                _ = model(input_data)\n",
    "            else:  # YOLO\n",
    "                _ = model(img_rgb)\n",
    "    \n",
    "    # Actual benchmark\n",
    "    start_time = time.time()\n",
    "    for _ in range(num_runs):\n",
    "        with torch.no_grad():\n",
    "            if model_type == \"Faster R-CNN\":\n",
    "                _ = model(input_data)\n",
    "            else:  # YOLO\n",
    "                _ = model(img_rgb)\n",
    "    end_time = time.time()\n",
    "    \n",
    "    fps = num_runs / (end_time - start_time)\n",
    "    avg_time = (end_time - start_time) / num_runs * 1000  # milliseconds\n",
    "    \n",
    "    print(f\"üìä {model_type}: {fps:.2f} FPS ({avg_time:.2f}ms per inference)\")\n",
    "    return fps, avg_time\n",
    "\n",
    "print(\"üìä PERFORMANCE EVALUATION\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# ===== 1. FPS BENCHMARK =====\n",
    "print(\"\\nüöÄ 1. FPS PERFORMANCE\")\n",
    "frcnn_fps, frcnn_time = benchmark_fps(faster_rcnn, \"Faster R-CNN\", img_tensor)\n",
    "yolo_fps, yolo_time = benchmark_fps(yolo_model, \"YOLO\", None)\n",
    "\n",
    "print_memory_usage()\n",
    "\n",
    "# ===== 2. DETECTION ANALYSIS =====\n",
    "print(\"\\nüéØ 2. DETECTION ANALYSIS\")\n",
    "\n",
    "# L·ªçc detection c√≥ confidence > 0.5\n",
    "frcnn_high_conf_mask = frcnn_scores > 0.5\n",
    "frcnn_filtered_boxes = frcnn_boxes[frcnn_high_conf_mask]\n",
    "frcnn_filtered_labels = frcnn_labels[frcnn_high_conf_mask]\n",
    "frcnn_filtered_scores = frcnn_scores[frcnn_high_conf_mask]\n",
    "\n",
    "# YOLO detections\n",
    "yolo_boxes = yolo_results[0].boxes\n",
    "yolo_high_conf_boxes = []\n",
    "yolo_high_conf_labels = []\n",
    "yolo_high_conf_scores = []\n",
    "\n",
    "if yolo_boxes is not None:\n",
    "    for box in yolo_boxes:\n",
    "        if box.conf[0] > 0.5:\n",
    "            yolo_high_conf_boxes.append(box.xyxy[0].cpu().numpy())\n",
    "            yolo_high_conf_labels.append(int(box.cls[0].cpu().item()))\n",
    "            yolo_high_conf_scores.append(box.conf[0].cpu().item())\n",
    "\n",
    "print(f\"üìä Faster R-CNN: {len(frcnn_filtered_boxes)} high-confidence detections\")\n",
    "print(f\"üìä YOLO: {len(yolo_high_conf_boxes)} high-confidence detections\")\n",
    "\n",
    "# ===== 3. CLASS DISTRIBUTION =====\n",
    "print(\"\\nüè∑Ô∏è 3. CLASS DISTRIBUTION\")\n",
    "\n",
    "if len(frcnn_filtered_labels) > 0:\n",
    "    frcnn_classes = [COCO_CLASSES[label.item()] for label in frcnn_filtered_labels if label.item() < len(COCO_CLASSES)]\n",
    "    print(\"Faster R-CNN detected classes:\", set(frcnn_classes))\n",
    "\n",
    "if len(yolo_high_conf_labels) > 0:\n",
    "    yolo_class_names = [yolo_results[0].names[cls] for cls in yolo_high_conf_labels]\n",
    "    print(\"YOLO detected classes:\", set(yolo_class_names))\n",
    "\n",
    "# ===== 4. CONFIDENCE DISTRIBUTION =====\n",
    "print(\"\\nüìà 4. CONFIDENCE ANALYSIS\")\n",
    "if len(frcnn_filtered_scores) > 0:\n",
    "    print(f\"Faster R-CNN - Avg confidence: {frcnn_filtered_scores.mean():.3f}, Max: {frcnn_filtered_scores.max():.3f}\")\n",
    "\n",
    "if len(yolo_high_conf_scores) > 0:\n",
    "    print(f\"YOLO - Avg confidence: {np.mean(yolo_high_conf_scores):.3f}, Max: {np.max(yolo_high_conf_scores):.3f}\")\n",
    "\n",
    "# ===== 5. PERFORMANCE SUMMARY =====\n",
    "print(\"\\nüìã 5. PERFORMANCE SUMMARY\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# T·∫°o b·∫£ng so s√°nh\n",
    "comparison_data = {\n",
    "    'Metric': ['FPS', 'Inference Time (ms)', 'High-Conf Detections', 'Avg Confidence'],\n",
    "    'Faster R-CNN': [f\"{frcnn_fps:.2f}\", f\"{frcnn_time:.2f}\", len(frcnn_filtered_boxes), \n",
    "                     f\"{frcnn_filtered_scores.mean():.3f}\" if len(frcnn_filtered_scores) > 0 else \"0.000\"],\n",
    "    'YOLO': [f\"{yolo_fps:.2f}\", f\"{yolo_time:.2f}\", len(yolo_high_conf_boxes),\n",
    "             f\"{np.mean(yolo_high_conf_scores):.3f}\" if len(yolo_high_conf_scores) > 0 else \"0.000\"]\n",
    "}\n",
    "\n",
    "df_comparison = pd.DataFrame(comparison_data)\n",
    "print(df_comparison.to_string(index=False))\n",
    "\n",
    "# ===== 6. VISUALIZATION =====\n",
    "print(\"\\nüé® 6. PERFORMANCE VISUALIZATION\")\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# FPS Comparison\n",
    "models = ['Faster R-CNN', 'YOLO']\n",
    "fps_values = [frcnn_fps, yolo_fps]\n",
    "colors = ['#ff7f0e', '#2ca02c']\n",
    "\n",
    "axes[0].bar(models, fps_values, color=colors, alpha=0.7)\n",
    "axes[0].set_ylabel('FPS')\n",
    "axes[0].set_title('FPS Comparison', fontweight='bold')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Th√™m gi√° tr·ªã l√™n bars\n",
    "for i, v in enumerate(fps_values):\n",
    "    axes[0].text(i, v + 0.5, f'{v:.1f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# Detection Count Comparison\n",
    "detection_counts = [len(frcnn_filtered_boxes), len(yolo_high_conf_boxes)]\n",
    "axes[1].bar(models, detection_counts, color=colors, alpha=0.7)\n",
    "axes[1].set_ylabel('Number of Detections')\n",
    "axes[1].set_title('Detection Count Comparison\\n(Confidence > 0.5)', fontweight='bold')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Th√™m gi√° tr·ªã l√™n bars\n",
    "for i, v in enumerate(detection_counts):\n",
    "    axes[1].text(i, v + 0.1, f'{v}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ===== 7. CONCLUSION =====\n",
    "print(\"\\nüèÅ 7. CONCLUSION\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "faster_better = \"‚úÖ\" if frcnn_fps > yolo_fps else \"‚ùå\"\n",
    "yolo_better = \"‚úÖ\" if yolo_fps > frcnn_fps else \"‚ùå\"\n",
    "\n",
    "print(f\"üèÉ‚Äç‚ôÇÔ∏è Speed Winner: {'YOLO' if yolo_fps > frcnn_fps else 'Faster R-CNN'}\")\n",
    "print(f\"üéØ Detection Winner: {'YOLO' if len(yolo_high_conf_boxes) > len(frcnn_filtered_boxes) else 'Faster R-CNN'}\")\n",
    "\n",
    "print(f\"\\nüìä Faster R-CNN {faster_better}: Better for accuracy, slower inference\")\n",
    "print(f\"‚ö° YOLO {yolo_better}: Better for speed, real-time applications\")\n",
    "\n",
    "print(\"\\n‚úÖ Performance evaluation completed!\")\n",
    "clear_cuda_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6544ceac",
   "metadata": {},
   "source": [
    "# B√†i 4: Hu·∫•n luy·ªán Faster R-CNN v√† YOLO tr√™n t·∫≠p d·ªØ li·ªáu ph∆∞∆°ng ti·ªán giao th√¥ng"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "275486ac",
   "metadata": {},
   "source": [
    "### 4.a) T·∫£i v√† chu·∫©n b·ªã d·ªØ li·ªáu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f47638d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install kaggle -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c5e54b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== B√ÄI 4A: CHU·∫®N B·ªä DATASET T·ªêI ∆ØU ==========\n",
    "\n",
    "import os\n",
    "import yaml\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"üì¶ VEHICLE DATASET PREPARATION\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# ===== 1. KI·ªÇM TRA V√Ä T·∫¢I DATASET =====\n",
    "dataset_path = Path(\"vehicle-dataset-for-yolo\")\n",
    "\n",
    "if not dataset_path.exists():\n",
    "    print(\"‚ùå Dataset not found. Please ensure the dataset is downloaded.\")\n",
    "    print(\"üìù Manual download steps:\")\n",
    "    print(\"1. Go to: https://www.kaggle.com/datasets/nadinpethiyagoda/vehicle-dataset-for-yolo\")\n",
    "    print(\"2. Download and extract to current directory\")\n",
    "    print(\"3. Ensure folder structure: vehicle-dataset-for-yolo/vehicle dataset/train/\")\n",
    "else:\n",
    "    print(\"‚úÖ Dataset found!\")\n",
    "\n",
    "# ===== 2. DATASET PATHS =====\n",
    "train_img_dir = dataset_path / \"vehicle dataset\" / \"train\" / \"images\"\n",
    "train_lbl_dir = dataset_path / \"vehicle dataset\" / \"train\" / \"labels\"\n",
    "val_img_dir = dataset_path / \"vehicle dataset\" / \"valid\" / \"images\" \n",
    "val_lbl_dir = dataset_path / \"vehicle dataset\" / \"valid\" / \"labels\"\n",
    "\n",
    "print(f\"\\nüìÅ Dataset structure:\")\n",
    "print(f\"üìÇ Train images: {train_img_dir}\")\n",
    "print(f\"üìÇ Train labels: {train_lbl_dir}\")\n",
    "print(f\"üìÇ Valid images: {val_img_dir}\")\n",
    "print(f\"üìÇ Valid labels: {val_lbl_dir}\")\n",
    "\n",
    "# ===== 3. KI·ªÇM TRA DATASET =====\n",
    "if train_img_dir.exists() and train_lbl_dir.exists():\n",
    "    train_images = list(train_img_dir.glob(\"*.jpg\"))\n",
    "    train_labels = list(train_lbl_dir.glob(\"*.txt\"))\n",
    "    \n",
    "    print(f\"\\nüìä Dataset statistics:\")\n",
    "    print(f\"üñºÔ∏è Train images: {len(train_images)}\")\n",
    "    print(f\"üè∑Ô∏è Train labels: {len(train_labels)}\")\n",
    "    \n",
    "    if val_img_dir.exists():\n",
    "        val_images = list(val_img_dir.glob(\"*.jpg\"))\n",
    "        val_labels = list(val_lbl_dir.glob(\"*.txt\")) if val_lbl_dir.exists() else []\n",
    "        print(f\"üñºÔ∏è Valid images: {len(val_images)}\")\n",
    "        print(f\"üè∑Ô∏è Valid labels: {len(val_labels)}\")\n",
    "    \n",
    "    # ===== 4. T·∫†O DATA.YAML T·ªêI ∆ØU =====\n",
    "    print(f\"\\nüìù Creating optimized data.yaml...\")\n",
    "    \n",
    "    vehicle_classes = [\"car\", \"threewheel\", \"bus\", \"truck\", \"van\", \"motorbike\"]\n",
    "    \n",
    "    data_config = {\n",
    "        'path': str(dataset_path.absolute()),  # dataset root\n",
    "        'train': 'vehicle dataset/train/images',  # relative path\n",
    "        'val': 'vehicle dataset/valid/images' if val_img_dir.exists() else 'vehicle dataset/train/images',\n",
    "        'nc': len(vehicle_classes),  # number of classes\n",
    "        'names': vehicle_classes\n",
    "    }\n",
    "    \n",
    "    with open('data.yaml', 'w') as f:\n",
    "        yaml.dump(data_config, f, default_flow_style=False)\n",
    "    \n",
    "    print(\"‚úÖ data.yaml created successfully!\")\n",
    "    \n",
    "    # ===== 5. PH√ÇN T√çCH DATASET =====\n",
    "    print(f\"\\nüîç Dataset analysis...\")\n",
    "    \n",
    "    # ƒê·ªçc m·ªôt v√†i label files ƒë·ªÉ hi·ªÉu c·∫•u tr√∫c\n",
    "    class_distribution = {i: 0 for i in range(len(vehicle_classes))}\n",
    "    total_objects = 0\n",
    "    \n",
    "    for label_file in train_labels[:100]:  # Ch·ªâ ph√¢n t√≠ch 100 files ƒë·∫ßu\n",
    "        try:\n",
    "            with open(label_file, 'r') as f:\n",
    "                for line in f:\n",
    "                    parts = line.strip().split()\n",
    "                    if len(parts) >= 5:\n",
    "                        class_id = int(parts[0])\n",
    "                        if class_id < len(vehicle_classes):\n",
    "                            class_distribution[class_id] += 1\n",
    "                            total_objects += 1\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    print(f\"\\nüìà Class distribution (first 100 images):\")\n",
    "    for i, class_name in enumerate(vehicle_classes):\n",
    "        count = class_distribution.get(i, 0)\n",
    "        percentage = (count / total_objects * 100) if total_objects > 0 else 0\n",
    "        print(f\"   {class_name}: {count} objects ({percentage:.1f}%)\")\n",
    "    \n",
    "    print(f\"üìä Total objects analyzed: {total_objects}\")\n",
    "    \n",
    "    # ===== 6. MEMORY EFFICIENT DATASET CLASS =====\n",
    "    print(f\"\\nüõ†Ô∏è Setting up optimized dataset classes...\")\n",
    "\n",
    "else:\n",
    "    print(\"‚ùå Dataset directories not found!\")\n",
    "    print(\"Please ensure the dataset is properly extracted.\")\n",
    "\n",
    "print(\"\\n‚úÖ Dataset preparation completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e682255",
   "metadata": {},
   "source": [
    "### 4.b) Hu·∫•n luy·ªán Faster R-CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64f2cf23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== B√ÄI 4B: FINE-TUNING FASTER R-CNN T·ªêI ∆ØU ==========\n",
    "\n",
    "import torch.nn as nn\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "print(\"üöÄ FASTER R-CNN FINE-TUNING SETUP\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Clear memory tr∆∞·ªõc khi b·∫Øt ƒë·∫ßu\n",
    "clear_cuda_memory()\n",
    "\n",
    "# ===== 1. DATASET CLASS T·ªêI ∆ØU =====\n",
    "class OptimizedVehicleDataset(Dataset):\n",
    "    \"\"\"Dataset class t·ªëi ∆∞u cho vehicle detection\"\"\"\n",
    "    \n",
    "    def __init__(self, img_dir, label_dir, transforms=None, max_size=640):\n",
    "        self.img_dir = Path(img_dir)\n",
    "        self.label_dir = Path(label_dir)\n",
    "        self.transforms = transforms\n",
    "        self.max_size = max_size\n",
    "        \n",
    "        # L·ªçc ch·ªâ nh·ªØng file c√≥ c·∫£ image v√† label\n",
    "        self.image_files = []\n",
    "        for img_file in self.img_dir.glob(\"*.jpg\"):\n",
    "            label_file = self.label_dir / f\"{img_file.stem}.txt\"\n",
    "            if label_file.exists():\n",
    "                self.image_files.append(img_file)\n",
    "        \n",
    "        print(f\"üìä Found {len(self.image_files)} valid image-label pairs\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Load image\n",
    "        img_path = self.image_files[idx]\n",
    "        label_path = self.label_dir / f\"{img_path.stem}.txt\"\n",
    "        \n",
    "        try:\n",
    "            img = Image.open(img_path).convert(\"RGB\")\n",
    "            \n",
    "            # Resize ƒë·ªÉ ti·∫øt ki·ªám memory\n",
    "            if max(img.size) > self.max_size:\n",
    "                img.thumbnail((self.max_size, self.max_size), Image.Resampling.LANCZOS)\n",
    "            \n",
    "            w, h = img.size\n",
    "            \n",
    "            # Load labels\n",
    "            boxes = []\n",
    "            labels = []\n",
    "            \n",
    "            if label_path.exists():\n",
    "                with open(label_path, 'r') as f:\n",
    "                    for line in f:\n",
    "                        try:\n",
    "                            parts = line.strip().split()\n",
    "                            if len(parts) >= 5:\n",
    "                                cls, x_center, y_center, width, height = map(float, parts[:5])\n",
    "                                \n",
    "                                # Convert YOLO format to bounding box\n",
    "                                x1 = (x_center - width/2) * w\n",
    "                                y1 = (y_center - height/2) * h\n",
    "                                x2 = (x_center + width/2) * w\n",
    "                                y2 = (y_center + height/2) * h\n",
    "                                \n",
    "                                # Clamp to image bounds\n",
    "                                x1, y1 = max(0, x1), max(0, y1)\n",
    "                                x2, y2 = min(w, x2), min(h, y2)\n",
    "                                \n",
    "                                if x2 > x1 and y2 > y1:  # Valid box\n",
    "                                    boxes.append([x1, y1, x2, y2])\n",
    "                                    labels.append(int(cls) + 1)  # +1 for background class\n",
    "                        except:\n",
    "                            continue\n",
    "            \n",
    "            # Handle empty annotations\n",
    "            if not boxes:\n",
    "                boxes = [[0, 0, 10, 10]]  # Small dummy box\n",
    "                labels = [0]  # Background class\n",
    "            \n",
    "            boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "            labels = torch.as_tensor(labels, dtype=torch.int64)\n",
    "            \n",
    "            # Create target dict\n",
    "            target = {\n",
    "                \"boxes\": boxes,\n",
    "                \"labels\": labels,\n",
    "                \"image_id\": torch.tensor([idx])\n",
    "            }\n",
    "            \n",
    "            if self.transforms:\n",
    "                img = self.transforms(img)\n",
    "            \n",
    "            return img, target\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Error loading {img_path}: {e}\")\n",
    "            # Return a dummy sample\n",
    "            dummy_img = torch.zeros(3, 224, 224)\n",
    "            dummy_target = {\n",
    "                \"boxes\": torch.tensor([[0, 0, 10, 10]], dtype=torch.float32),\n",
    "                \"labels\": torch.tensor([0], dtype=torch.int64),\n",
    "                \"image_id\": torch.tensor([idx])\n",
    "            }\n",
    "            return dummy_img, dummy_target\n",
    "\n",
    "# ===== 2. CREATE MODEL T·ªêI ∆ØU =====\n",
    "print(\"\\nüèóÔ∏è Building optimized Faster R-CNN...\")\n",
    "\n",
    "NUM_CLASSES = 7  # 6 vehicle classes + background\n",
    "\n",
    "# Load pretrained model\n",
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "\n",
    "model = fasterrcnn_resnet50_fpn(weights=\"DEFAULT\")\n",
    "\n",
    "# Replace classifier head\n",
    "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "model.roi_heads.box_predictor = FastRCNNPredictor(in_features, NUM_CLASSES)\n",
    "\n",
    "model.to(device)\n",
    "print(f\"‚úÖ Model created with {NUM_CLASSES} classes\")\n",
    "\n",
    "# ===== 3. SETUP TRAINING COMPONENTS =====\n",
    "print(\"\\n‚öôÔ∏è Setting up training components...\")\n",
    "\n",
    "# Transforms v·ªõi augmentation nh·∫π\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.RandomHorizontalFlip(0.3),  # Nh·∫π ƒë·ªÉ ti·∫øt ki·ªám memory\n",
    "])\n",
    "\n",
    "# Create datasets\n",
    "if train_img_dir.exists():\n",
    "    train_dataset = OptimizedVehicleDataset(\n",
    "        str(train_img_dir), \n",
    "        str(train_lbl_dir), \n",
    "        transforms=train_transform,\n",
    "        max_size=416  # Gi·∫£m size ƒë·ªÉ ti·∫øt ki·ªám VRAM\n",
    "    )\n",
    "    \n",
    "    # DataLoader v·ªõi batch size nh·ªè\n",
    "    def collate_fn(batch):\n",
    "        return tuple(zip(*batch))\n",
    "    \n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, \n",
    "        batch_size=2,  # Batch size nh·ªè\n",
    "        shuffle=True, \n",
    "        collate_fn=collate_fn,\n",
    "        num_workers=2,\n",
    "        pin_memory=True if torch.cuda.is_available() else False\n",
    "    )\n",
    "    \n",
    "    # Optimizer v·ªõi learning rate th·∫•p\n",
    "    params = [p for p in model.parameters() if p.requires_grad]\n",
    "    optimizer = torch.optim.SGD(params, lr=0.002, momentum=0.9, weight_decay=0.0005)\n",
    "    \n",
    "    # Learning rate scheduler\n",
    "    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=3)\n",
    "    \n",
    "    # Mixed precision scaler\n",
    "    scaler = GradScaler() if torch.cuda.is_available() else None\n",
    "    \n",
    "    print(f\"üìä Dataset size: {len(train_dataset)}\")\n",
    "    print(f\"üìä Batch size: 2\")\n",
    "    print(f\"üìä Total batches: {len(train_loader)}\")\n",
    "    print(\"‚úÖ Training setup completed!\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå Training images directory not found!\")\n",
    "\n",
    "print_memory_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ffdc8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== TRAINING FASTER R-CNN ==========\n",
    "\n",
    "def train_faster_rcnn(model, train_loader, optimizer, scaler, num_epochs=3):\n",
    "    \"\"\"Training function cho Faster R-CNN\"\"\"\n",
    "    print(f\"\\nüöÄ Starting Faster R-CNN training for {num_epochs} epochs...\")\n",
    "    \n",
    "    model.train()\n",
    "    training_losses = []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_loss = 0.0\n",
    "        batch_count = 0\n",
    "        \n",
    "        print(f\"\\nüìÖ Epoch {epoch + 1}/{num_epochs}\")\n",
    "        print(\"-\" * 30)\n",
    "        \n",
    "        for batch_idx, (images, targets) in enumerate(train_loader):\n",
    "            try:\n",
    "                # Move to device\n",
    "                images = [img.to(device) for img in images]\n",
    "                targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                # Forward pass v·ªõi mixed precision\n",
    "                if scaler and torch.cuda.is_available():\n",
    "                    with autocast():\n",
    "                        loss_dict = model(images, targets)\n",
    "                        losses = sum(loss for loss in loss_dict.values())\n",
    "                    \n",
    "                    scaler.scale(losses).backward()\n",
    "                    scaler.step(optimizer)\n",
    "                    scaler.update()\n",
    "                else:\n",
    "                    loss_dict = model(images, targets)\n",
    "                    losses = sum(loss for loss in loss_dict.values())\n",
    "                    losses.backward()\n",
    "                    optimizer.step()\n",
    "                \n",
    "                epoch_loss += losses.item()\n",
    "                batch_count += 1\n",
    "                \n",
    "                # Print progress m·ªói 10 batches\n",
    "                if (batch_idx + 1) % 10 == 0:\n",
    "                    avg_loss = epoch_loss / batch_count\n",
    "                    print(f\"  Batch {batch_idx + 1}/{len(train_loader)} | Loss: {avg_loss:.4f}\")\n",
    "                    print_memory_usage()\n",
    "                \n",
    "                # Gi·ªõi h·∫°n s·ªë batches ƒë·ªÉ demo\n",
    "                if batch_idx >= 20:  # Ch·ªâ train 20 batches ƒë·∫ßu\n",
    "                    print(\"  ‚è≠Ô∏è Stopping early for demo...\")\n",
    "                    break\n",
    "                    \n",
    "            except RuntimeError as e:\n",
    "                if \"out of memory\" in str(e):\n",
    "                    print(f\"‚ö†Ô∏è CUDA OOM at batch {batch_idx}. Clearing cache...\")\n",
    "                    torch.cuda.empty_cache()\n",
    "                    continue\n",
    "                else:\n",
    "                    raise e\n",
    "        \n",
    "        avg_epoch_loss = epoch_loss / batch_count if batch_count > 0 else 0\n",
    "        training_losses.append(avg_epoch_loss)\n",
    "        \n",
    "        print(f\"üìä Epoch {epoch + 1} completed | Avg Loss: {avg_epoch_loss:.4f}\")\n",
    "        \n",
    "        # Update learning rate\n",
    "        if isinstance(scheduler, ReduceLROnPlateau):\n",
    "            scheduler.step(avg_epoch_loss)\n",
    "        \n",
    "        # Clear cache sau m·ªói epoch\n",
    "        clear_cuda_memory()\n",
    "    \n",
    "    return training_losses\n",
    "\n",
    "# Start training n·∫øu dataset c√≥ s·∫µn\n",
    "if 'train_loader' in locals() and len(train_dataset) > 0:\n",
    "    print(\"üéØ Training conditions met. Starting training...\")\n",
    "    frcnn_losses = train_faster_rcnn(model, train_loader, optimizer, scaler, num_epochs=2)\n",
    "    \n",
    "    # Save model\n",
    "    torch.save(model.state_dict(), 'fasterrcnn_vehicles.pth')\n",
    "    print(\"üíæ Model saved as 'fasterrcnn_vehicles.pth'\")\n",
    "    \n",
    "    # Plot training loss\n",
    "    if frcnn_losses:\n",
    "        plt.figure(figsize=(8, 5))\n",
    "        plt.plot(range(1, len(frcnn_losses) + 1), frcnn_losses, 'bo-', linewidth=2)\n",
    "        plt.title('Faster R-CNN Training Loss', fontweight='bold')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.show()\n",
    "    \n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Training data not available. Please ensure dataset is properly loaded.\")\n",
    "\n",
    "print(\"‚úÖ Faster R-CNN training section completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e98ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "\n",
    "class YOLODataset(Dataset):\n",
    "    def __init__(self, img_dir, label_dir, transforms=None):\n",
    "        self.img_dir = img_dir\n",
    "        self.label_dir = label_dir\n",
    "        self.transforms = transforms\n",
    "        self.images = [f for f in os.listdir(img_dir) if f.endswith(\".jpg\")]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # ƒê·ªçc ·∫£nh\n",
    "        img_path = os.path.join(self.img_dir, self.images[idx])\n",
    "        label_path = os.path.join(self.label_dir, self.images[idx].replace(\".jpg\", \".txt\"))\n",
    "        \n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        w, h = img.size\n",
    "\n",
    "        # ƒê·ªçc nh√£n YOLO\n",
    "        boxes = []\n",
    "        labels = []\n",
    "        if os.path.exists(label_path):\n",
    "            with open(label_path) as f:\n",
    "                for line in f.readlines():\n",
    "                    cls, x, y, bw, bh = map(float, line.strip().split())\n",
    "                    x1 = (x - bw/2) * w\n",
    "                    y1 = (y - bh/2) * h\n",
    "                    x2 = (x + bw/2) * w\n",
    "                    y2 = (y + bh/2) * h\n",
    "                    boxes.append([x1, y1, x2, y2])\n",
    "                    labels.append(int(cls) + 1)  # +1 v√¨ 0 l√† background\n",
    "\n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "        labels = torch.as_tensor(labels, dtype=torch.int64)\n",
    "        target = {\"boxes\": boxes, \"labels\": labels}\n",
    "\n",
    "        if self.transforms:\n",
    "            img = self.transforms(img)\n",
    "\n",
    "        return img, target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "616c0863",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as T\n",
    "\n",
    "# ƒê∆∞·ªùng d·∫´n dataset\n",
    "train_img_dir = \"C:/Users/thong/workplace/IUH/4RD_YEAR_COLLEGE/semester_1/deep learnning/ThucHanh/th2/vehicle-dataset-for-yolo/vehicle dataset/train/images\"\n",
    "train_lbl_dir = \"C:/Users/thong/workplace/IUH/4RD_YEAR_COLLEGE/semester_1/deep learnning/ThucHanh/th2/vehicle-dataset-for-yolo/vehicle dataset/train/labels\"\n",
    "\n",
    "# Transform\n",
    "transform = T.Compose([T.ToTensor()])\n",
    "\n",
    "# Dataset & DataLoader\n",
    "train_dataset = YOLODataset(train_img_dir, train_lbl_dir, transforms=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, collate_fn=lambda x: tuple(zip(*x)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c9f75e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debug: Ki·ªÉm tra dataset tr∆∞·ªõc khi training\n",
    "print(\"ƒêang ki·ªÉm tra dataset...\")\n",
    "print(f\"S·ªë file ·∫£nh: {len(train_dataset)}\")\n",
    "\n",
    "# Ki·ªÉm tra m·ªôt s·ªë samples\n",
    "for i in range(min(5, len(train_dataset))):\n",
    "    try:\n",
    "        img, target = train_dataset[i]\n",
    "        print(f\"Sample {i}: Boxes shape: {target['boxes'].shape}, Labels: {target['labels'].tolist()}\")\n",
    "        \n",
    "        # Ki·ªÉm tra labels c√≥ v∆∞·ª£t qu√° NUM_CLASSES kh√¥ng\n",
    "        max_label = target['labels'].max().item() if len(target['labels']) > 0 else 0\n",
    "        if max_label >= NUM_CLASSES:\n",
    "            print(f\"‚ö†Ô∏è WARNING: Label {max_label} >= NUM_CLASSES {NUM_CLASSES}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Error loading sample {i}: {e}\")\n",
    "\n",
    "print(f\"NUM_CLASSES hi·ªán t·∫°i: {NUM_CLASSES}\")\n",
    "print(\"‚úÖ Ki·ªÉm tra dataset ho√†n t·∫•t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbaa2836",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ki·ªÉm tra to√†n b·ªô dataset ƒë·ªÉ t√¨m labels v∆∞·ª£t qu√° gi·ªõi h·∫°n\n",
    "print(\"Ki·ªÉm tra to√†n b·ªô dataset...\")\n",
    "max_label_found = 0\n",
    "invalid_samples = []\n",
    "\n",
    "for i in range(len(train_dataset)):\n",
    "    try:\n",
    "        img, target = train_dataset[i]\n",
    "        if len(target['labels']) > 0:\n",
    "            max_in_sample = target['labels'].max().item()\n",
    "            if max_in_sample > max_label_found:\n",
    "                max_label_found = max_in_sample\n",
    "            if max_in_sample >= NUM_CLASSES:\n",
    "                invalid_samples.append((i, max_in_sample))\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading sample {i}: {e}\")\n",
    "\n",
    "print(f\"Max label found in dataset: {max_label_found}\")\n",
    "print(f\"NUM_CLASSES: {NUM_CLASSES}\")\n",
    "print(f\"S·ªë samples c√≥ label v∆∞·ª£t qu√° gi·ªõi h·∫°n: {len(invalid_samples)}\")\n",
    "\n",
    "if invalid_samples:\n",
    "    print(\"M·ªôt s·ªë samples c√≥ l·ªói:\")\n",
    "    for idx, label in invalid_samples[:10]:  # Hi·ªÉn th·ªã 10 ƒë·∫ßu ti√™n\n",
    "        print(f\"  Sample {idx}: label {label} >= {NUM_CLASSES}\")\n",
    "else:\n",
    "    print(\"‚úÖ T·∫•t c·∫£ labels ƒë·ªÅu trong gi·ªõi h·∫°n h·ª£p l·ªá\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db89717f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gi·∫£i ph√°p 1: Clear CUDA cache v√† reset\n",
    "import torch\n",
    "import gc\n",
    "\n",
    "print(\"Clearing CUDA cache...\")\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.synchronize()\n",
    "    print(f\"CUDA memory allocated: {torch.cuda.memory_allocated() / 1024**2:.2f} MB\")\n",
    "    print(f\"CUDA memory cached: {torch.cuda.memory_reserved() / 1024**2:.2f} MB\")\n",
    "\n",
    "# Clear Python garbage collector\n",
    "gc.collect()\n",
    "print(\"‚úÖ Cache cleared\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60521998",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAU KHI RESTART KERNEL - Ch·∫°y cell n√†y ƒë·∫ßu ti√™n!\n",
    "# Training an to√†n tr√™n CPU tr∆∞·ªõc\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision.models.detection import FasterRCNN\n",
    "from torchvision.models.detection.rpn import AnchorGenerator\n",
    "\n",
    "print(\"üîÑ Kh·ªüi t·∫°o l·∫°i sau khi restart kernel...\")\n",
    "\n",
    "# S·ª¨ D·ª§NG CPU ƒê·ªÇ TR√ÅNH L·ªñI CUDA\n",
    "device = torch.device(\"cpu\")  # S·ª≠ d·ª•ng CPU thay v√¨ CUDA\n",
    "print(f\"S·ª≠ d·ª•ng device: {device}\")\n",
    "\n",
    "# S·ªë l·ªõp (6 classes + 1 background)\n",
    "NUM_CLASSES = 7\n",
    "\n",
    "# T·∫°o l·∫°i model\n",
    "backbone = torchvision.models.resnet50(weights=\"IMAGENET1K_V1\")\n",
    "backbone = torch.nn.Sequential(*(list(backbone.children())[:-2]))\n",
    "backbone.out_channels = 2048\n",
    "\n",
    "anchor_generator = AnchorGenerator(sizes=((32, 64, 128, 256, 512),),\n",
    "                                   aspect_ratios=((0.5, 1.0, 2.0),))\n",
    "\n",
    "model = FasterRCNN(backbone, num_classes=NUM_CLASSES, rpn_anchor_generator=anchor_generator)\n",
    "model.to(device)\n",
    "\n",
    "print(\"‚úÖ Model ƒë√£ ƒë∆∞·ª£c kh·ªüi t·∫°o l·∫°i th√†nh c√¥ng tr√™n CPU\")\n",
    "print(\"‚ö†Ô∏è L∆∞u √Ω: ƒêang ch·∫°y tr√™n CPU n√™n s·∫Ω ch·∫≠m h∆°n nh∆∞ng an to√†n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d755896",
   "metadata": {},
   "outputs": [],
   "source": [
    "# T·∫°o l·∫°i dataset v√† dataloader (sau khi restart)\n",
    "import os\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import torchvision.transforms as T\n",
    "\n",
    "class YOLODataset(Dataset):\n",
    "    def __init__(self, img_dir, label_dir, transforms=None):\n",
    "        self.img_dir = img_dir\n",
    "        self.label_dir = label_dir\n",
    "        self.transforms = transforms\n",
    "        self.images = [f for f in os.listdir(img_dir) if f.endswith(\".jpg\")]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # ƒê·ªçc ·∫£nh\n",
    "        img_path = os.path.join(self.img_dir, self.images[idx])\n",
    "        label_path = os.path.join(self.label_dir, self.images[idx].replace(\".jpg\", \".txt\"))\n",
    "        \n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        w, h = img.size\n",
    "\n",
    "        # ƒê·ªçc nh√£n YOLO\n",
    "        boxes = []\n",
    "        labels = []\n",
    "        if os.path.exists(label_path):\n",
    "            with open(label_path) as f:\n",
    "                for line in f.readlines():\n",
    "                    cls, x, y, bw, bh = map(float, line.strip().split())\n",
    "                    x1 = (x - bw/2) * w\n",
    "                    y1 = (y - bh/2) * h\n",
    "                    x2 = (x + bw/2) * w\n",
    "                    y2 = (y + bh/2) * h\n",
    "                    boxes.append([x1, y1, x2, y2])\n",
    "                    labels.append(int(cls) + 1)  # +1 v√¨ 0 l√† background\n",
    "\n",
    "        # X·ª≠ l√Ω tr∆∞·ªùng h·ª£p kh√¥ng c√≥ annotations\n",
    "        if not boxes:\n",
    "            boxes = [[0, 0, 1, 1]]  # dummy box\n",
    "            labels = [0]  # background class\n",
    "\n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "        labels = torch.as_tensor(labels, dtype=torch.int64)\n",
    "        target = {\"boxes\": boxes, \"labels\": labels}\n",
    "\n",
    "        if self.transforms:\n",
    "            img = self.transforms(img)\n",
    "\n",
    "        return img, target\n",
    "\n",
    "# ƒê∆∞·ªùng d·∫´n dataset\n",
    "train_img_dir = \"vehicle-dataset-for-yolo/vehicle dataset/train/images\"\n",
    "train_lbl_dir = \"vehicle-dataset-for-yolo/vehicle dataset/train/labels\"\n",
    "\n",
    "# Transform\n",
    "transform = T.Compose([T.ToTensor()])\n",
    "\n",
    "# Dataset & DataLoader v·ªõi batch_size nh·ªè h∆°n ƒë·ªÉ an to√†n\n",
    "train_dataset = YOLODataset(train_img_dir, train_lbl_dir, transforms=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True, collate_fn=lambda x: tuple(zip(*x)))\n",
    "\n",
    "print(f\"‚úÖ Dataset ƒë∆∞·ª£c t·∫°o l·∫°i: {len(train_dataset)} images\")\n",
    "print(\"‚úÖ DataLoader s·∫µn s√†ng v·ªõi batch_size=2 (an to√†n)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea635553",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training an to√†n tr√™n CPU\n",
    "optimizer = optim.SGD([p for p in model.parameters() if p.requires_grad], \n",
    "                      lr=0.001, momentum=0.9, weight_decay=0.0005)  # Learning rate th·∫•p h∆°n\n",
    "\n",
    "# Test v·ªõi 1 batch ƒë·ªÉ ƒë·∫£m b·∫£o kh√¥ng c√≥ l·ªói\n",
    "print(\"Testing v·ªõi 1 batch...\")\n",
    "model.train()\n",
    "\n",
    "for imgs, targets in train_loader:\n",
    "    imgs = [img.to(device) for img in imgs]  # device = CPU\n",
    "    targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "    \n",
    "    # Debug th√¥ng tin\n",
    "    print(f\"Batch size: {len(imgs)}\")\n",
    "    print(f\"Image shapes: {[img.shape for img in imgs]}\")\n",
    "    print(f\"Labels: {[t['labels'].tolist() for t in targets]}\")\n",
    "    \n",
    "    # Forward pass\n",
    "    loss_dict = model(imgs, targets)\n",
    "    losses = sum(loss for loss in loss_dict.values())\n",
    "    print(f\"Loss: {losses.item():.4f}\")\n",
    "    \n",
    "    # Backward pass\n",
    "    optimizer.zero_grad()\n",
    "    losses.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    print(\"‚úÖ 1 batch ho√†n th√†nh th√†nh c√¥ng!\")\n",
    "    break  # Ch·ªâ test 1 batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b78f907b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "# Model sang GPU n·∫øu c√≥\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Optimizer\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = optim.SGD(params, lr=0.005, momentum=0.9, weight_decay=0.0005)\n",
    "\n",
    "# Hu·∫•n luy·ªán\n",
    "num_epochs = 5\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for imgs, targets in train_loader:\n",
    "        imgs = [img.to(device) for img in imgs]\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "    \n",
    "        # Debug: print unique label values\n",
    "        all_labels = torch.cat([t['labels'].cpu() for t in targets])\n",
    "        print(\"Unique labels in batch:\", all_labels.unique().tolist())\n",
    "    \n",
    "        loss_dict = model(imgs, targets)\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        losses.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += losses.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b228a34",
   "metadata": {},
   "source": [
    "### 4.c) Fine-tuning YOLOv11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a230aa28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hu·∫•n luy·ªán YOLO v·ªõi th∆∞ vi·ªán ultralytics\n",
    "# !yolo task=detect mode=train model=yolov11n.pt data=data.yaml epochs=50 imgsz=640"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e30e6e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install ultralytics -q\n",
    "\n",
    "import torch, os\n",
    "device = 0 if torch.cuda.is_available() else 'cpu'\n",
    "device\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "128ee53b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "data = {\n",
    "    'train': train_img_dir,\n",
    "    'val':   train_lbl_dir,\n",
    "    'nc': 6,  # s·ªë l·ªõp trong dataset\n",
    "    'names': [\"car\", \"threewheel\", \"bus\", \"truck\", \"van\", \"motorbike\"]\n",
    "}\n",
    "\n",
    "with open('data.yaml', 'w') as f:\n",
    "    yaml.dump(data, f)\n",
    "\n",
    "print(\"‚úÖ ƒê√£ t·∫°o data.yaml\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0244c814",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== B√ÄI 4C: FINE-TUNING YOLO T·ªêI ∆ØU ==========\n",
    "\n",
    "print(\"‚ö° YOLO FINE-TUNING SETUP\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Clear memory tr∆∞·ªõc khi training YOLO\n",
    "clear_cuda_memory()\n",
    "\n",
    "# ===== 1. KI·ªÇM TRA DATA.YAML =====\n",
    "if os.path.exists('data.yaml'):\n",
    "    with open('data.yaml', 'r') as f:\n",
    "        data_config = yaml.safe_load(f)\n",
    "    print(\"‚úÖ data.yaml found and loaded\")\n",
    "    print(f\"üìä Classes: {data_config.get('names', [])}\")\n",
    "    print(f\"üìä Number of classes: {data_config.get('nc', 0)}\")\n",
    "else:\n",
    "    print(\"‚ùå data.yaml not found. Creating default configuration...\")\n",
    "    # T·∫°o data.yaml m·∫∑c ƒë·ªãnh n·∫øu ch∆∞a c√≥\n",
    "    default_config = {\n",
    "        'nc': 6,\n",
    "        'names': [\"car\", \"threewheel\", \"bus\", \"truck\", \"van\", \"motorbike\"]\n",
    "    }\n",
    "    with open('data.yaml', 'w') as f:\n",
    "        yaml.dump(default_config, f)\n",
    "    print(\"‚úÖ Default data.yaml created\")\n",
    "\n",
    "# ===== 2. LOAD YOLO MODEL =====\n",
    "print(f\"\\nü§ñ Loading YOLO model...\")\n",
    "\n",
    "try:\n",
    "    # Th·ª≠ load YOLOv8n (nh·ªè nh·∫•t, ti·∫øt ki·ªám VRAM nh·∫•t)\n",
    "    yolo_train_model = YOLO(\"yolov8n.pt\")\n",
    "    print(\"‚úÖ YOLOv8n loaded successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error loading YOLO: {e}\")\n",
    "    print(\"Please ensure ultralytics is installed: pip install ultralytics\")\n",
    "\n",
    "# ===== 3. TRAINING CONFIGURATION T·ªêI ∆ØU =====\n",
    "print(f\"\\n‚öôÔ∏è Optimized training configuration:\")\n",
    "\n",
    "# C·∫•u h√¨nh training t·ªëi ∆∞u VRAM\n",
    "train_config = {\n",
    "    'data': 'data.yaml',\n",
    "    'epochs': 10,          # √çt epochs cho demo\n",
    "    'imgsz': 320,          # K√≠ch th∆∞·ªõc ·∫£nh nh·ªè (320x320)\n",
    "    'batch': 4,            # Batch size nh·ªè\n",
    "    'device': 0 if torch.cuda.is_available() else 'cpu',\n",
    "    \n",
    "    # ===== VRAM OPTIMIZATION =====\n",
    "    'amp': True,           # Mixed precision training\n",
    "    'cache': False,        # Kh√¥ng cache v√†o RAM\n",
    "    'workers': 2,          # √çt workers\n",
    "    \n",
    "    # ===== PERFORMANCE SETTINGS =====\n",
    "    'patience': 5,         # Early stopping\n",
    "    'save_period': 5,      # L∆∞u checkpoint m·ªói 5 epochs\n",
    "    'val': True,           # Validation\n",
    "    'plots': True,         # T·∫°o plots\n",
    "    'verbose': True,       # Hi·ªÉn th·ªã chi ti·∫øt\n",
    "    \n",
    "    # ===== AUGMENTATION (NH·∫∏) =====\n",
    "    'hsv_h': 0.01,         # Hue augmentation nh·∫π\n",
    "    'hsv_s': 0.3,          # Saturation\n",
    "    'hsv_v': 0.2,          # Value\n",
    "    'degrees': 5,          # Rotation nh·∫π\n",
    "    'translate': 0.05,     # Translation nh·∫π\n",
    "    'scale': 0.1,          # Scaling nh·∫π\n",
    "    'flipud': 0.0,         # Kh√¥ng flip vertical\n",
    "    'fliplr': 0.3,         # Flip horizontal\n",
    "    'mosaic': 0.5,         # Mosaic augmentation\n",
    "    'mixup': 0.0,          # Kh√¥ng mixup ƒë·ªÉ ti·∫øt ki·ªám VRAM\n",
    "}\n",
    "\n",
    "print(\"üìã Training parameters:\")\n",
    "for key, value in train_config.items():\n",
    "    if key != 'data':\n",
    "        print(f\"   {key}: {value}\")\n",
    "\n",
    "print_memory_usage()\n",
    "\n",
    "def train_yolo_optimized():\n",
    "    \"\"\"Training YOLO v·ªõi c·∫•u h√¨nh t·ªëi ∆∞u\"\"\"\n",
    "    try:\n",
    "        print(f\"\\nüöÄ Starting YOLO training...\")\n",
    "        print(f\"üéØ Using device: {train_config['device']}\")\n",
    "        \n",
    "        # B·∫Øt ƒë·∫ßu training\n",
    "        results = yolo_train_model.train(**train_config)\n",
    "        \n",
    "        print(\"‚úÖ YOLO training completed!\")\n",
    "        return results\n",
    "        \n",
    "    except RuntimeError as e:\n",
    "        if \"out of memory\" in str(e):\n",
    "            print(\"‚ö†Ô∏è CUDA out of memory! Trying backup configuration...\")\n",
    "            \n",
    "            # Backup config v·ªõi VRAM c·ª±c th·∫•p\n",
    "            backup_config = train_config.copy()\n",
    "            backup_config.update({\n",
    "                'imgsz': 256,      # Nh·ªè h∆°n n·ªØa\n",
    "                'batch': 2,        # Batch size c·ª±c nh·ªè\n",
    "                'workers': 1,      # 1 worker\n",
    "                'mosaic': 0.0,     # T·∫Øt mosaic\n",
    "                'device': 'cpu'    # Chuy·ªÉn v·ªÅ CPU\n",
    "            })\n",
    "            \n",
    "            print(\"üîÑ Retrying with backup configuration...\")\n",
    "            for key, value in backup_config.items():\n",
    "                if key in ['imgsz', 'batch', 'workers', 'device']:\n",
    "                    print(f\"   {key}: {value}\")\n",
    "            \n",
    "            try:\n",
    "                results = yolo_train_model.train(**backup_config)\n",
    "                print(\"‚úÖ YOLO training completed with backup config!\")\n",
    "                return results\n",
    "            except Exception as e2:\n",
    "                print(f\"‚ùå Training failed even with backup config: {e2}\")\n",
    "                return None\n",
    "        else:\n",
    "            print(f\"‚ùå Training error: {e}\")\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Unexpected error: {e}\")\n",
    "        return None\n",
    "\n",
    "# ===== 4. START TRAINING =====\n",
    "if 'yolo_train_model' in locals():\n",
    "    print(f\"\\nüéØ Ready to start training!\")\n",
    "    print(\"‚ö†Ô∏è Note: Training will be limited to save time and VRAM\")\n",
    "    print(\"üìù Remove epoch limit for full training\")\n",
    "    \n",
    "    # Uncomment d√≤ng d∆∞·ªõi ƒë·ªÉ b·∫Øt ƒë·∫ßu training\n",
    "    # yolo_results = train_yolo_optimized()\n",
    "    \n",
    "    print(\"\\nüí° To start training, uncomment the line: yolo_results = train_yolo_optimized()\")\n",
    "else:\n",
    "    print(\"‚ùå YOLO model not loaded. Cannot proceed with training.\")\n",
    "\n",
    "print(\"\\n‚úÖ YOLO training setup completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d32a4cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== B√ÄI 4D,E: EVALUATION & COMPARISON ==========\n",
    "\n",
    "print(\"üìä COMPREHENSIVE MODEL EVALUATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# ===== 1. EVALUATION METRICS =====\n",
    "def calculate_model_metrics(predictions, ground_truth, iou_threshold=0.5):\n",
    "    \"\"\"T√≠nh to√°n c√°c metrics cho object detection\"\"\"\n",
    "    tp = fp = fn = 0\n",
    "    ious = []\n",
    "    \n",
    "    for pred_box in predictions:\n",
    "        best_iou = 0\n",
    "        for gt_box in ground_truth:\n",
    "            iou = calculate_iou(pred_box, gt_box)\n",
    "            best_iou = max(best_iou, iou)\n",
    "        \n",
    "        ious.append(best_iou)\n",
    "        if best_iou >= iou_threshold:\n",
    "            tp += 1\n",
    "        else:\n",
    "            fp += 1\n",
    "    \n",
    "    fn = max(0, len(ground_truth) - tp)\n",
    "    \n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    \n",
    "    return {\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1_score': f1,\n",
    "        'avg_iou': np.mean(ious) if ious else 0,\n",
    "        'tp': tp,\n",
    "        'fp': fp,\n",
    "        'fn': fn\n",
    "    }\n",
    "\n",
    "def evaluate_model_performance(model, model_type, test_image, num_runs=20):\n",
    "    \"\"\"ƒê√°nh gi√° hi·ªáu su·∫•t t·ªïng h·ª£p c·ªßa model\"\"\"\n",
    "    print(f\"\\nüîç Evaluating {model_type}...\")\n",
    "    \n",
    "    # Speed benchmark\n",
    "    times = []\n",
    "    for _ in range(num_runs):\n",
    "        start_time = time.time()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            if model_type == \"Faster R-CNN\":\n",
    "                if isinstance(test_image, np.ndarray):\n",
    "                    test_tensor = transforms.ToTensor()(Image.fromarray(test_image)).unsqueeze(0).to(device)\n",
    "                else:\n",
    "                    test_tensor = test_image\n",
    "                _ = model(test_tensor)\n",
    "            else:  # YOLO\n",
    "                _ = model(test_image)\n",
    "        \n",
    "        times.append(time.time() - start_time)\n",
    "    \n",
    "    avg_time = np.mean(times[5:])  # B·ªè 5 l·∫ßn ƒë·∫ßu (warm-up)\n",
    "    fps = 1.0 / avg_time\n",
    "    std_time = np.std(times[5:])\n",
    "    \n",
    "    return {\n",
    "        'avg_inference_time': avg_time * 1000,  # milliseconds\n",
    "        'fps': fps,\n",
    "        'std_time': std_time * 1000,\n",
    "        'min_time': min(times) * 1000,\n",
    "        'max_time': max(times) * 1000\n",
    "    }\n",
    "\n",
    "# ===== 2. MODEL COMPARISON =====\n",
    "print(f\"\\n‚öñÔ∏è MODEL COMPARISON\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# S·ª≠ d·ª•ng ·∫£nh test t·ª´ ban ƒë·∫ßu\n",
    "test_img = img_rgb.copy()\n",
    "\n",
    "# Evaluate models n·∫øu ƒë√£ load\n",
    "comparison_results = {}\n",
    "\n",
    "if 'faster_rcnn' in locals():\n",
    "    print(\"üîç Evaluating Faster R-CNN performance...\")\n",
    "    frcnn_perf = evaluate_model_performance(faster_rcnn, \"Faster R-CNN\", test_img)\n",
    "    comparison_results['Faster R-CNN'] = frcnn_perf\n",
    "    \n",
    "    print(f\"‚ö° Faster R-CNN Results:\")\n",
    "    print(f\"   üìà FPS: {frcnn_perf['fps']:.2f}\")\n",
    "    print(f\"   ‚è±Ô∏è Avg Time: {frcnn_perf['avg_inference_time']:.2f}ms\")\n",
    "\n",
    "if 'yolo_model' in locals():\n",
    "    print(\"\\nüîç Evaluating YOLO performance...\")\n",
    "    yolo_perf = evaluate_model_performance(yolo_model, \"YOLO\", test_img)\n",
    "    comparison_results['YOLO'] = yolo_perf\n",
    "    \n",
    "    print(f\"‚ö° YOLO Results:\")\n",
    "    print(f\"   üìà FPS: {yolo_perf['fps']:.2f}\")\n",
    "    print(f\"   ‚è±Ô∏è Avg Time: {yolo_perf['avg_inference_time']:.2f}ms\")\n",
    "\n",
    "# ===== 3. COMPREHENSIVE COMPARISON TABLE =====\n",
    "if comparison_results:\n",
    "    print(f\"\\nüìã DETAILED PERFORMANCE COMPARISON\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    df_data = []\n",
    "    for model_name, results in comparison_results.items():\n",
    "        df_data.append({\n",
    "            'Model': model_name,\n",
    "            'FPS': f\"{results['fps']:.2f}\",\n",
    "            'Avg Time (ms)': f\"{results['avg_inference_time']:.2f}\",\n",
    "            'Std Time (ms)': f\"{results['std_time']:.2f}\",\n",
    "            'Min Time (ms)': f\"{results['min_time']:.2f}\",\n",
    "            'Max Time (ms)': f\"{results['max_time']:.2f}\"\n",
    "        })\n",
    "    \n",
    "    comparison_df = pd.DataFrame(df_data)\n",
    "    print(comparison_df.to_string(index=False))\n",
    "\n",
    "# ===== 4. VISUALIZATION =====\n",
    "if len(comparison_results) >= 2:\n",
    "    print(f\"\\nüìä PERFORMANCE VISUALIZATION\")\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    models = list(comparison_results.keys())\n",
    "    fps_values = [comparison_results[model]['fps'] for model in models]\n",
    "    time_values = [comparison_results[model]['avg_inference_time'] for model in models]\n",
    "    \n",
    "    colors = ['#ff7f0e', '#2ca02c']\n",
    "    \n",
    "    # FPS Comparison\n",
    "    bars1 = axes[0].bar(models, fps_values, color=colors[:len(models)], alpha=0.7)\n",
    "    axes[0].set_ylabel('Frames Per Second (FPS)')\n",
    "    axes[0].set_title('Speed Comparison - FPS', fontweight='bold', fontsize=14)\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    for i, (bar, value) in enumerate(zip(bars1, fps_values)):\n",
    "        axes[0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5, \n",
    "                    f'{value:.1f}', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    # Inference Time Comparison\n",
    "    bars2 = axes[1].bar(models, time_values, color=colors[:len(models)], alpha=0.7)\n",
    "    axes[1].set_ylabel('Inference Time (milliseconds)')\n",
    "    axes[1].set_title('Speed Comparison - Inference Time', fontweight='bold', fontsize=14)\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    for i, (bar, value) in enumerate(zip(bars2, time_values)):\n",
    "        axes[1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1, \n",
    "                    f'{value:.1f}ms', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# ===== 5. DETAILED ANALYSIS & CONCLUSIONS =====\n",
    "print(f\"\\nüéØ DETAILED ANALYSIS & CONCLUSIONS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if len(comparison_results) >= 2:\n",
    "    frcnn_fps = comparison_results.get('Faster R-CNN', {}).get('fps', 0)\n",
    "    yolo_fps = comparison_results.get('YOLO', {}).get('fps', 0)\n",
    "    \n",
    "    print(f\"üèÉ‚Äç‚ôÇÔ∏è SPEED ANALYSIS:\")\n",
    "    if yolo_fps > frcnn_fps:\n",
    "        speed_winner = \"YOLO\"\n",
    "        speed_diff = ((yolo_fps - frcnn_fps) / frcnn_fps) * 100\n",
    "        print(f\"   ‚úÖ Winner: YOLO ({speed_diff:.1f}% faster)\")\n",
    "        print(f\"   üìä YOLO: {yolo_fps:.2f} FPS vs Faster R-CNN: {frcnn_fps:.2f} FPS\")\n",
    "    else:\n",
    "        speed_winner = \"Faster R-CNN\"\n",
    "        speed_diff = ((frcnn_fps - yolo_fps) / yolo_fps) * 100\n",
    "        print(f\"   ‚úÖ Winner: Faster R-CNN ({speed_diff:.1f}% faster)\")\n",
    "        print(f\"   üìä Faster R-CNN: {frcnn_fps:.2f} FPS vs YOLO: {yolo_fps:.2f} FPS\")\n",
    "    \n",
    "    print(f\"\\nüéØ USE CASE RECOMMENDATIONS:\")\n",
    "    print(f\"   üöó Real-time applications (autonomous vehicles): YOLO\")\n",
    "    print(f\"   üî¨ High-accuracy research: Faster R-CNN\")\n",
    "    print(f\"   üì± Mobile/edge devices: YOLO\")\n",
    "    print(f\"   ‚òÅÔ∏è Cloud-based batch processing: Faster R-CNN\")\n",
    "\n",
    "print(f\"\\nüìù GENERAL CHARACTERISTICS:\")\n",
    "print(f\"üî∏ Faster R-CNN:\")\n",
    "print(f\"   ‚úÖ Higher accuracy potential\")\n",
    "print(f\"   ‚úÖ Better for complex scenes\")\n",
    "print(f\"   ‚ùå Slower inference\")\n",
    "print(f\"   ‚ùå More memory intensive\")\n",
    "\n",
    "print(f\"\\nüî∏ YOLO:\")\n",
    "print(f\"   ‚úÖ Real-time performance\")\n",
    "print(f\"   ‚úÖ Smaller model size\")\n",
    "print(f\"   ‚úÖ End-to-end training\")\n",
    "print(f\"   ‚ùå May miss small objects\")\n",
    "\n",
    "print(f\"\\nüèÅ FINAL CONCLUSION:\")\n",
    "print(f\"For vehicle detection applications:\")\n",
    "print(f\"   üìä Speed-critical systems: Choose YOLO\")\n",
    "print(f\"   üéØ Accuracy-critical systems: Choose Faster R-CNN\")\n",
    "print(f\"   ‚öñÔ∏è Balanced requirements: Start with YOLO, fine-tune if needed\")\n",
    "\n",
    "print(f\"\\n‚úÖ Comprehensive evaluation completed!\")\n",
    "clear_cuda_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d7b3916",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== GI·∫¢I PH√ÅP TR√ÅNH TR√ÄN VRAM CUDA =====\n",
    "import torch\n",
    "import gc\n",
    "\n",
    "# 1. Clear CUDA cache tr∆∞·ªõc khi train\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.synchronize()\n",
    "    print(f\"üßπ Cleared CUDA cache\")\n",
    "    print(f\"üíæ CUDA Memory Available: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")\n",
    "\n",
    "# Clear Python garbage collector\n",
    "gc.collect()\n",
    "print(\"‚úÖ Memory cleanup completed\")\n",
    "\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# 2. S·ª≠ d·ª•ng GPU v·ªõi c√°c t·ªëi ∆∞u h√≥a VRAM\n",
    "device = 0 if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"üéØ Using device: {device}\")\n",
    "\n",
    "model = YOLO(\"yolov8n.pt\")  # Gi·ªØ yolov8n - model nh·ªè nh·∫•t\n",
    "\n",
    "# 3. Training v·ªõi c√°c k·ªπ thu·∫≠t ti·∫øt ki·ªám VRAM\n",
    "model.train(\n",
    "    data=\"data.yaml\", \n",
    "    epochs=50,\n",
    "    # ===== C√ÅC THAM S·ªê TI·∫æT KI·ªÜM VRAM =====\n",
    "    imgsz=416,         # Gi·∫£m t·ª´ 640 -> 416 (gi·∫£m ~50% VRAM)\n",
    "    batch=4,           # Batch size nh·ªè (m·∫∑c ƒë·ªãnh th∆∞·ªùng l√† 16)\n",
    "    amp=True,          # Mixed Precision Training (FP16) - gi·∫£m ~50% VRAM\n",
    "    device=device,\n",
    "    # ===== GRADIENT ACCUMULATION =====\n",
    "    accumulate=4,      # T√≠ch l≈©y gradient qua 4 batches = hi·ªáu ·ª©ng batch=16\n",
    "    # ===== C√ÅC THAM S·ªê KH√ÅC =====\n",
    "    workers=2,         # Gi·∫£m s·ªë worker processes\n",
    "    patience=10,       # Early stopping n·∫øu kh√¥ng c·∫£i thi·ªán\n",
    "    save_period=5,     # L∆∞u checkpoint m·ªói 5 epochs\n",
    "    cache=False,       # Kh√¥ng cache dataset v√†o RAM\n",
    "    close_mosaic=10,   # T·∫Øt mosaic augmentation ·ªü 10 epoch cu·ªëi\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a43704c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== MONITOR CUDA MEMORY USAGE =====\n",
    "import torch\n",
    "\n",
    "def print_gpu_utilization():\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"üíæ GPU Memory Usage:\")\n",
    "        print(f\"   Allocated: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")\n",
    "        print(f\"   Cached: {torch.cuda.memory_reserved() / 1024**3:.2f} GB\") \n",
    "        print(f\"   Free: {(torch.cuda.get_device_properties(0).total_memory - torch.cuda.memory_reserved()) / 1024**3:.2f} GB\")\n",
    "        print(f\"   Total: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")\n",
    "    else:\n",
    "        print(\"üö´ CUDA not available\")\n",
    "\n",
    "# Ki·ªÉm tra tr∆∞·ªõc khi train\n",
    "print_gpu_utilization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a15292",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== BACKUP PLAN N·∫æU V·∫™N B·ªä TR√ÄN VRAM =====\n",
    "# N·∫øu v·∫´n b·ªã CUDA out of memory, ch·∫°y config n√†y:\n",
    "\n",
    "# from ultralytics import YOLO\n",
    "# \n",
    "# # C·∫•u h√¨nh c·ª±c k·ª≥ ti·∫øt ki·ªám VRAM\n",
    "# model = YOLO(\"yolov8n.pt\")\n",
    "# model.train(\n",
    "#     data=\"data.yaml\",\n",
    "#     epochs=50,\n",
    "#     imgsz=320,        # Gi·∫£m xu·ªëng 320x320\n",
    "#     batch=2,          # Batch size c·ª±c nh·ªè\n",
    "#     amp=True,         # Mixed precision\n",
    "#     accumulate=8,     # T√≠ch l≈©y nhi·ªÅu gradient h∆°n\n",
    "#     device=0,\n",
    "#     workers=1,        # Ch·ªâ 1 worker\n",
    "#     cache=False,\n",
    "#     mosaic=0.0,       # T·∫Øt mosaic augmentation\n",
    "#     mixup=0.0,        # T·∫Øt mixup\n",
    "#     copy_paste=0.0,   # T·∫Øt copy-paste augmentation\n",
    "#     degrees=0.0,      # T·∫Øt rotation\n",
    "#     translate=0.0,    # T·∫Øt translation\n",
    "#     scale=0.0,        # T·∫Øt scaling\n",
    "#     shear=0.0,        # T·∫Øt shearing\n",
    "#     flipud=0.0,       # T·∫Øt vertical flip\n",
    "#     fliplr=0.0,       # T·∫Øt horizontal flip\n",
    "#     hsv_h=0.0,        # T·∫Øt hue augmentation\n",
    "#     hsv_s=0.0,        # T·∫Øt saturation augmentation\n",
    "#     hsv_v=0.0         # T·∫Øt value augmentation\n",
    "# )\n",
    "\n",
    "print(\"üìù Backup plan ƒë√£ ƒë∆∞·ª£c chu·∫©n b·ªã (uncomment n·∫øu c·∫ßn d√πng)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab930ed5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
