{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lTVq2aDdz73W"
   },
   "source": [
    "# 1. Introduction\n",
    "\n",
    "Trong bài tập này, ta sẽ tìm hiểu và triển khai một quy trình hệ thống đề xuất end-to-end dựa trên thuật toán phân tích ma trận. Chúng ta sẽ sử dụng tập dữ liệu MovieLens để huấn luyện mô hình và tiến hành các thử nghiệm. Nhiệm vụ của tập dữ liệu này là đề xuất những bộ phim mà người dùng có khả năng cao sẽ thích, dựa trên sở thích phim trước đây của họ. Đây cũng là một nhiệm vụ quan trọng, vì nó vẫn được sử dụng rộng rãi để đánh giá tiến độ trong cộng đồng nghiên cứu hệ thống đề xuất."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_FJKuH-Rz73Z"
   },
   "source": [
    "## 1.1 Installing libraries\n",
    "\n",
    "Before we begin, we must make sure to install the libraries for the tutorial. To do this, we can use Python's package installer `pip`. Now go ahead and execute the following cell by selecting it and clicking `shift`+`Enter`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Data vizualisation\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import utilities as utl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TOiy1nZ9z73l"
   },
   "source": [
    "We have also prepared some boilerplate functions that we have grouped together in the `utilities` module. It is not necessary for you to look at these in order to complete the tutorial, but if you are curious, it is definitely a good idea to take a look at them!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9YA2A1vqz73m"
   },
   "source": [
    "## 1.2 Task setup and Data\n",
    "\n",
    "The objective of a recommender system is to model users' historical behaviors such that we can predict what an individual user will most likely enjoy in the future. In short, we would like to create recommendations that are personalized to each user's interest.\n",
    "\n",
    "The concept of an item might seem a bit fuzzy at first. But in fact, in the world of recommender systems, we always speak about users and items. Items can be anything a user interacts with, ranging from: products, movies, social media posts, news articles, search results, Instagram photos, restaurants, Pinterest pins and the list goes on. Not so surprisingly, majority of internet companies we interact with on a daily basis run some sort of recommender system in the background.\n",
    "\n",
    "These recommender systems try to predict what will the most relevant item to show us given our user-item interactions. These interactions are often categorized as follows: implicit signals (e.g. likes, views, searches, purchases, installations, music listening behavior etc.) and explicit singals (e.g. ratings, reviews). Both types of signals can be used to help us define how to capture the users' interest, i.e. the user preference. And naturally, some signals suggest stronger proxies for user preferences (e.g. purchases, ratings) than others (e.g. views, clicks, time spent looking at an item).\n",
    "\n",
    "<img src = \"https://user-images.githubusercontent.com/13997178/90336890-0f8fba00-dfdf-11ea-9e32-d6b00988bd10.png\" width = \"350\">\n",
    "\n",
    "We can collect these user-item interactions to form a user preference vector, i.e. a vector that contains all historical interactions of the user with items that are relevant to our given task.\n",
    "\n",
    "But how do we then use these user preference vectors to generate personalized recommendations?\n",
    "\n",
    "Do we need to use all historical interactions? Doesn't user taste change over time? What if recommendations reinforce a user's previous choices?\n",
    "Do we want to recommend complementary, sustitute or independent items from what the user has interacted with previously? \n",
    "Shall we try instead to show her items that she has not yet been exposed to? \n",
    "How do we balance how much exposure items get that are already very popular? What to do in cold-start scenarios when a new user or new item joins our platform?\n",
    "\n",
    "These are just few of the many considerations that one would have to think about before designing a recommender system for a downstream use case. In fact, there is an outstanding community that actively conducts research on some of these questions (e.g. see [RecSys](https://recsys.acm.org/)). There is not a single answer to these considerations - it really just depends on your individual use case. However, the answer will affect not only which models you will use, but also what data you will feed to your model, and how you will set up your objective.\n",
    "\n",
    "In the context of this tutorial, we will focus on recommending movies that users will like according to their past movie ratings. In order to carry out this task, we will use users' movie ratings, some relevant sociodemographic data as well as various features describing movies. Finally, we can refine the diagram as follows:\n",
    "\n",
    "<img src = \"https://user-images.githubusercontent.com/13997178/90336926-4b2a8400-dfdf-11ea-8379-0f38fc104e85.png\" width = \"750\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "E-h67-Loz73n"
   },
   "source": [
    "## 1.3 The MoviesLens dataset(s)\n",
    "\n",
    "The data used here consist of more or less 100k movie evaluations by 943 users. Over 1,6k movies are available. In addition to the 100k evaluations, additional information related to users and movies is available.\n",
    "\n",
    "We will use three different datasets to carry out our analyses:\n",
    "\n",
    "<ul>\n",
    "<li> Users : related to users' characteristics,\n",
    "<li> Movies : related to movies' characteristics,\n",
    "<li> Ratings : containing over 100k evaluations.\n",
    "</ul>\n",
    "\n",
    "We used the <a href=\"https://pandas.pydata.org/\">Pandas</a> library in order to download and manipulate the datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OKlYRrdyz73n"
   },
   "source": [
    "### 1.3.1 Users: Download and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read dataset\n",
    "users = pd.read_csv('ml-100k/u.user' , sep='|', header=None, engine='python', encoding='latin-1')\n",
    "\n",
    "# Columns describing user characteristics\n",
    "users.columns = ['Index', 'Age', 'Gender', 'Occupation', 'Zip code']\n",
    "\n",
    "# Quick overview\n",
    "users.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of users x features:', users.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ImaeouHkz78d"
   },
   "source": [
    "Before presenting some descriptive statistics related to the population, we transform the users' data in a <a href=\"https://en.wikipedia.org/wiki/List_(abstract_data_type)\">list</a> in order to be able to handle them more easily. \n",
    "\n",
    "We first encode users' gender - originally stored as a string - with either 0 or 1. \n",
    "Then similarly, since occupations are also recorded as a string value, we would like to instead encode this into a binary array indicating the presence of each occupation.\n",
    "Finally, we concatanate each user feature (i.e. age, gender and occupation) into a list per each user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of users\n",
    "nb_users = len(users)\n",
    "\n",
    "# Gender: Convert 'M' and 'F' to 0 and 1\n",
    "gender = np.where(np.matrix(users['Gender']) == 'M', 0, 1)[0]\n",
    "\n",
    "print('Shape of gender features:', gender.shape)\n",
    "\n",
    "# Occupation\n",
    "occupation_name = np.array(pd.read_csv('ml-100k/u.occupation', \n",
    "                                            sep='|', header=None, engine='python', encoding='latin-1').loc[:, 0])\n",
    "\n",
    "# Boolean transformation of user's occupation\n",
    "occupation_matrix = np.zeros((nb_users, len(occupation_name)))\n",
    "\n",
    "for k in np.arange(nb_users):\n",
    "    occupation_matrix[k, occupation_name.tolist().index(users['Occupation'][k])] = 1\n",
    "\n",
    "print('Shape of user occupation matrix (num of users x num of occupations):', occupation_matrix.shape)\n",
    "\n",
    "# Concatenation of the sociodemographic variables \n",
    "user_attributes = np.concatenate((np.matrix(users['Age']), np.matrix(gender), occupation_matrix.T)).T.tolist()\n",
    "\n",
    "print('Shape of final user attribute matrix: (list of users with 23 features):', len(user_attributes), len(user_attributes[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "98e7SN9qz78v"
   },
   "source": [
    "We then explore the descriptive statistics of the users. These include information related to age (continuous variable), gender (binary variable) and occupation of each user (21, all binary)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "t3jJXODCz78w"
   },
   "source": [
    "#### Descriptive statistics related to users'  *age*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(users['Age'].describe()).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CcDMUpJZz7-m"
   },
   "source": [
    "#### Percentage of users per *gender*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utl.barplot(['Women', 'Men'], np.array([np.mean(gender) , 1 - np.mean(gender)]) * 100, \n",
    "            'Sex', 'Percentage (%)', \"User's gender\", 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "S7WfQi04z7-q"
   },
   "source": [
    "#### Percentage of users per *occupation*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attributes, scores = utl.rearrange(occupation_name, np.mean(occupation_matrix, axis=0) * 100)\n",
    "utl.barplot(attributes, scores, 'Occupation', 'Percentage (%)', \"Users' occupation\", 90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is now a good time to pause and reflect what these various user characteristics tell us. We see that majority of our users are men, and 20+ % of them are students, with a median age of 31. What does that mean for our model?\n",
    "Ideally, if we were to use this dataset to train a model on it, it would reflect the overall popular of movie watchers we will serve recommendations to. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1\n",
    "> Bạn nghĩ sao nếu ngày mai chúng ta triển khai một chiến dịch mới, cho phép xem tất cả phim miễn phí? Bạn nghĩ các đề xuất của chúng ta được đào tạo dựa trên tập dữ liệu này sẽ thay đổi như thế nào? Nếu chúng ta triển khai một chiến dịch khác, tăng giá thuê phim lên gấp 3 lần thì sao? Lượng người xem phim cơ bản sẽ thay đổi như thế nào?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AMXYOKLOz7-t"
   },
   "source": [
    "### 1.3.2 Movies: Download and preprocesssing\n",
    "\n",
    "In the same way, we will now process and explore the data associated with movies. For each movie, we have the *title*, the *release date* in North America, as well as the corresponding *genres*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read dataset\n",
    "movies = pd.read_csv('ml-100k/u.item', sep='|', header=None, engine='python', encoding='latin-1')\n",
    "\n",
    "# Number of movies\n",
    "nb_movies = len(movies)\n",
    "print('The number of movies is: ', nb_movies)\n",
    "\n",
    "# Genres\n",
    "movies_genre = np.matrix(movies.loc[:, 5:])\n",
    "movies_genre_name = np.array(pd.read_csv('ml-100k/u.genre', sep='|', header=None, engine='python', encoding='latin-1').loc[:, 0])\n",
    "\n",
    "# Quick overview\n",
    "movies.columns = ['Index', 'Title', 'Release', 'The Not a Number column', 'Imdb'] + movies_genre_name.tolist()\n",
    "movies.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FFIZ_Wqmz7_E"
   },
   "source": [
    "#### Percentage of movies per *genre*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 365
    },
    "colab_type": "code",
    "id": "WoFlTXY4z7_E",
    "outputId": "0e004e94-7b6b-4eb6-910b-7adf5d131674"
   },
   "outputs": [],
   "source": [
    "attributes, scores = utl.rearrange(movies_genre_name, \n",
    "                                   np.array(np.round(np.mean(movies_genre, axis=0) * 1, 2))[0])\n",
    "utl.barplot(attributes, np.array(scores) * 100, xlabel='Genre', ylabel='Percentage (%)', \n",
    "            title=\" \", rotation = 90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wGJIb9Dhz7_Q"
   },
   "source": [
    "### 1.3.3 Ratings: Download and preprocessing\n",
    "\n",
    "The dataset based on users ratings consists of approximately 100k lines (one evaluation per line) where the following are presented: the *user identification number*, the *identification number of the movie*, its associated *rating* and *a time marker*. The training and test sets were provided as is, that is, we do not need to build them ourselves, and have 80k and 20k evaluations respectively.\n",
    "\n",
    "For practical reasons, we convert the database as a list using our `convert` util function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "nx9oYW5gz7_Q",
    "outputId": "d2182a44-5665-43a4-c82c-ad15b09262d7"
   },
   "outputs": [],
   "source": [
    "training_set = np.array(pd.read_csv('ml-100k/u1.base'), delimiter='\\t'), dtype='int')\n",
    "testing_set = np.array(pd.read_csv('ml-100k/u1.test'), delimiter='\\t'), dtype='int')\n",
    "\n",
    "print('Example sample (user idx, movie idx, rating, timestamp: ', training_set[0])\n",
    "print('Shape of original training and test set with shape:     ', training_set.shape, testing_set.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "jy25j9S0QpR1",
    "outputId": "59409f89-3637-41af-a948-35aabfbcfb1b"
   },
   "outputs": [],
   "source": [
    "train_set = utl.convert(training_set, nb_users, nb_movies)\n",
    "test_set = utl.convert(testing_set, nb_users, nb_movies)\n",
    "\n",
    "print('Shape of final training set: (list of users x list of all movies):', len(train_set), len(train_set[0]))\n",
    "print('Shape of final test set:     (list of users x list of all movies):', len(test_set), len(test_set[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(train_set).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7mvWDUNez7_W"
   },
   "source": [
    "As we did before, we can get descriptive statistics associated with the evaluations. At first, it might be interesting to study the average trends of users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 76
    },
    "colab_type": "code",
    "id": "72s3zgm_z7_W",
    "outputId": "8e20a0c0-1310-42f2-ac0c-cdf6548fd4b2"
   },
   "outputs": [],
   "source": [
    "train_matrix = np.array(train_set)\n",
    "assert train_matrix.shape == (943, 1682)\n",
    "\n",
    "binarized_train_matrix = np.where(train_matrix > 0 , 1, 0)\n",
    "\n",
    "num_movies_watched = np.sum(binarized_train_matrix, axis=1) ## sum across movies for each user\n",
    "pd.DataFrame(num_movies_watched).describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I.e. The median user in our training set has watched 50 movies, with smallest number of movies watched being 4, and the most movies watched being 685."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "opRzwOHIz7_g"
   },
   "source": [
    "#### Histogram of the number of movies watched per user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 521
    },
    "colab_type": "code",
    "id": "y6tRdKbSz7_h",
    "outputId": "216fc169-57cb-478e-f8ac-5bbbc260e94b"
   },
   "outputs": [],
   "source": [
    "sns.set(rc={'figure.figsize':(12,8)})\n",
    "sns.set(font_scale = 1.5)\n",
    "\n",
    "plt.title('Empirical distribution of \\n the number of movies watched per user')\n",
    "plt.xlabel('Number of movies watched')\n",
    "plt.ylabel('Number of users')\n",
    "plt.hist(num_movies_watched, 100);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TpTDzpvBz7_k"
   },
   "source": [
    "#### Percentage of users having seen a movie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 76
    },
    "colab_type": "code",
    "id": "zGmPJgpbz7_l",
    "outputId": "e3c6393d-2d10-4ffa-ffd0-655666401253"
   },
   "outputs": [],
   "source": [
    "movie_popularity = np.mean(binarized_train_matrix, axis=0)  ## axis 0 refers to averaging across users\n",
    "pd.DataFrame(movie_popularity).describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is, 50% of movies were seen by less than 2% of users, whereas there are movies that were seen by either no user or by 51% of all users.\n",
    " \n",
    "We can also plot this as a histogram and see how many movies were seen by what proportion of the population.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 483
    },
    "colab_type": "code",
    "id": "mRgymfcWz7_p",
    "outputId": "dccc0518-9848-4d92-d59e-e605d5e83cb5"
   },
   "outputs": [],
   "source": [
    "plt.xlabel('Proportion of the population who watched the movie')\n",
    "plt.ylabel('Number of Movies')\n",
    "plt.hist(movie_popularity, 100);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2\n",
    "\n",
    "> Biểu đồ ở trên cho chúng ta biết điều gì về training matrix user-item?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "w_WxJwK-z8AD"
   },
   "source": [
    "#### Individual preferences according to the type of movie\n",
    "\n",
    "We could also look at the behavior of a particular individual. Among other things, we could study if there is a bias associated with her evaluation scheme or what are her cinematographic preferences according to the score awarded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GCy_zSQUz8AF"
   },
   "outputs": [],
   "source": [
    "def stats_user(data, movies_genre, user_id):\n",
    "    \n",
    "    ratings = data[user_id]\n",
    "    stats = np.zeros(6)\n",
    "    eva = np.zeros((6, movies_genre.shape[1]))\n",
    "\n",
    "    for k in np.arange(len(ratings)):\n",
    "        index = int(ratings[k])\n",
    "        stats[index] += 1\n",
    "        eva[index, :] = eva[index, :] + movies_genre[k]\n",
    "\n",
    "    return stats, eva"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 307
    },
    "colab_type": "code",
    "id": "eZHVMziCz8AJ",
    "outputId": "3e59ee12-2260-416a-af2c-ab7ae1f0506a"
   },
   "outputs": [],
   "source": [
    "user_id = 0\n",
    "num_of_star_ratings, genre_based_ratings = stats_user(train_set, movies_genre, user_id)\n",
    "utl.barplot(np.arange(5) + 1, num_of_star_ratings[1:6] / sum(num_of_star_ratings[1:6]), xlabel='Number of stars', ylabel='Percentage of movies (%)', \n",
    "            title=\" \", rotation = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nAI0G1Wnz8A6"
   },
   "source": [
    "# Question 3\n",
    "\n",
    "Làm thế nào chúng ta có thể kiểm tra sự tồn tại của bias ​​liên quan đến model đánh giá cá nhân?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "S4nDaiDoz8A7"
   },
   "source": [
    "## 1.4 Construction of the training and validation sets\n",
    "\n",
    "In all machine learning tasks, we always begin with a task in mind. In image classification, this task could be to identify if an image contains a cat or a dog. In recommender systems, it is to predict a set of items that the user will likely interact with. We could break this down as a task where we want to predict that an item is relevant to a user.\n",
    " \n",
    "In order to build a model that will perform this task, we need to come up with a metric that will measure how good our trained model is.\n",
    "For image classification tasks such as recognizing cats or dogs, this metric could be the percentage of images that we correctly classify. In short, we can use accuracy. For the recommendation task, this metric could be the number of retrieved items that our model was able to predict as relevant.\n",
    " \n",
    "We often speak of breaking our data into training, validation and test sets, where we use the training set only for model training, and the validation and test sets for evaluation. But why do we do this? It is because we would like to ensure that our model does not just overfit on characteristics of training dataset, but the model will generalize to observations that it did not encounter in the past.\n",
    " \n",
    "Ultimately, generalizing to unseen datapoints is the very reason we built this model for. Generally, we want to ensure that the characteristics of the training, validation and test sets will resemble what future datapoints will look like. Remember? We talked about this when we looked at the various characteristics of our users, movies and ratings.\n",
    " \n",
    "To make this more concrete, people often build on the assumption that these datasets are *iid*, that is, the datapoints are independent and are identically distributed. That is, datapoints seen in the validation and test sets have the same distribution, but were never seen during training.\n",
    " \n",
    " \n",
    "<b>! Note! </b>\n",
    " \n",
    "Though this iid assumption is widely used in machine learning, evaluation in recommender systems gets somewhat tricky.\n",
    " \n",
    "In fact, due to our choice of the recommender model here, the data we will use for evaluation is not linked to a new set of users, but rather, it will contain user-item interactions by the same set of users, but that were not seen during training.\n",
    " \n",
    "As a result, the data associated with the training, validation and test sets are no longer independent, which complicates things theoretically.\n",
    " \n",
    "Though it is good to be aware of this, it is outside the scope of this tutorial. Here we will just naively assume that each observation is independent of each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Y9Vgrze6z8A8"
   },
   "outputs": [],
   "source": [
    "def split(data, ratio, tensor=False):\n",
    "    train = np.zeros((len(data), len(data[0]))).tolist()\n",
    "    valid = np.zeros((len(data), len(data[0]))).tolist()\n",
    "\n",
    "    for i in range(len(data)):\n",
    "        for j in range(len(data[i])):\n",
    "            if data[i][j] > 0:\n",
    "                if np.random.binomial(1, ratio, 1):\n",
    "                    train[i][j] = data[i][j]\n",
    "                else:\n",
    "                    valid[i][j] = data[i][j]\n",
    "\n",
    "    return [train, valid]\n",
    "\n",
    "train = split(train_set, 0.8)\n",
    "test = test_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pIG0GWqkz8BC"
   },
   "source": [
    "# 2. Recommender systems: Matrix Factorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sF4nUjipz8Bh"
   },
   "source": [
    "## 2.1 Model\n",
    "\n",
    "Matrix factorization (MF) supposes that each observed evaluation $r_{ui}$ for $1 \\leq u \\leq |U|$ and $1 \\leq i \\leq |I|$, where $|U|$ and $|I|$ are respectively the number of users and movies, can be estimated with respect to a latent (hidden) model. This model presents the estimate $\\hat{r}_{ui}$ of the observed evaluation $r_{ui}$ as:\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\hat{r}_{ui} =  \\langle p_{u}, q_{i} \\rangle, \n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where $\\langle \\cdot \\rangle$ is the scalar product and $p_{u}$ and $q_{i}$ are the latent representations associated to user <i>u</i> and item <i>i</i>. The intuition behind this representation suggests that each evaluation can be estimated by considering a latent characterization of users and items.\n",
    "\n",
    "For example, let's fix the number of latent variables to 3, and suppose that they are associated with the popularity of the movie at the box office, its duration and finally its level of romance. Let us define the <i>u</i> user as a 15-year-old teenager who loves popular and relatively short horror movies. We can model the associated latent vector by:\n",
    "$$\n",
    "\\begin{align}\n",
    "p_{u} = [1, 0, 0]^T.\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Suppose now that the movie <i>i</i> turns out to be <i>The Lion King</i> with the following latent modelization:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "q_{i} = [1, 0.5, 0]^T.\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "The estimation of the evaluation for this user and item according to the latent representations will therefore be:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\hat{r}_{ui} =  \\langle p_u, q_i \\rangle = 1.\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "The main challenge in this type of model is to define the set of latent vectors associated with users, grouped in matrix form by $\\mathbf{P}_{|U| \\times k} = [p_1, p_2, .. ., p_k]$, and to items, grouped in matrix form by $ \\mathbf{Q}_{|I| \\times k} = [q_1, q_2, ..., q_k] $.\n",
    " \n",
    "Since the initial problem is to present the most accurate estimates, and thus to calculate $\\mathbf{P}$ and $\\mathbf{Q}$ so as to minimize the distance between the totality of the observed ratings $r_{ui}$ and their estimate $\\hat {r}_{ui}$, we can define the task to accomplish with the following optimization problem:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathbf{P}, \\mathbf{Q} = \\underset{p, q}{\\operatorname{argmin}} \\sum_{r_{ui} \\neq 0} (r_{ui} - \\hat{r}_{ui})^2 = \\underset{p, q}{\\operatorname{argmin}}  \\sum_{r_{ui} \\neq 0} (r_{ui} - \\langle p_u, q_i \\rangle)^2.\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "We may add a regularization on the latent variables, in order to force the associated vectors to have non-zero components:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathbf{P}, \\mathbf{Q} = \\underset{p, q}{\\operatorname{argmin}} \\sum_{r_{ui} \\neq 0} \\{(r_{ui} - \\langle p_u, q_i \\rangle)^2 + \\lambda(||p_u||^2 + ||q_i||^2)\\},\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where $\\lambda$ is the regularization hyperparameter, <i>weigth decay</i> in deep learning, or Lagrange multiplier in math. Although this last remark seems technical, it should be noted that latent vectors with very few zero values will in turn lead to predicted ratings other than zero. Since we are trying to propose new recommendations, this constraint seems useful to avoid a sparse matrix estimate $\\hat{\\mathbf{R}}$.\n",
    "\n",
    "In general, the optimization problem above, which turns out to factorize a sparse matrix, cannot be solved as easily as using the least squares in a linear regression context for example. In this tutorial, we will introduce the stochastic gradient descent algorithm, as one of the approaches to solving this optimization problem to estimate $\\mathbf{P}$ and $ \\mathbf{Q}$ matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-RkDWAtPz8Bj"
   },
   "source": [
    "## 2.2 Implementation\n",
    "\n",
    "In order to build a recommendation system based on MF, we have to define some specific functions necessary for this type of algorithm. Overall, we will break down the implementation process into three steps:\n",
    "\n",
    "1. <b>Learning loop</b>: Iterative optimization process that will estimate how far off the model is from satisfying a given objective, and will make the necessary model updates until a given stopping criterion is reached. \n",
    "\n",
    "2. <b> Loss Function</b>: Calculates how far off our model's prediction is from the true observation.\n",
    "\n",
    "3. <b> Estimation</b>: Estimation of the matrices of factors $\\mathbf{P}$ and $\\mathbf{Q}$ respectively associated with the users and the items."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uuSCo_qfPRFc"
   },
   "source": [
    "### 2.2.1 Learning loop\n",
    "\n",
    "We will now set up the learning loop. This will contain a function that performs a number of iterations to update our model's parameters until a given stopping criterion is reached."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WZJrzBL_z8Bn"
   },
   "source": [
    "## Question 4\n",
    "> Giả sử chúng ta có quyền truy cập vào ba hàm: prediction, sgd và loss. Chúng ta không cần xem cách triển khai chúng, mà chỉ cần tập trung signiture của các hàm này (bạn có thể xem qua các code cells). Vào cuối mỗi epoch, chúng ta nên xem xét số liệu nào để đảm bảo mô hình của chúng ta đang học. Đoạn code bên dưới có 1 lỗi sai, hãy tìm ra nó."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yhxDFNIrz8Bo"
   },
   "outputs": [],
   "source": [
    "def learn_to_recommend(data, features=10, lr=0.0002, epochs=101, weigth_decay=0.02, stopping=0.001):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "       data: every evaluation\n",
    "       features: number of latent variables\n",
    "       lr: learning rate for gradient descent\n",
    "       epochs: number of iterations or maximum loops to perform\n",
    "       weigth_decay: L2 regularization to predict rattings different of 0\n",
    "       stopping: scalar associated with the stopping criterion\n",
    "      \n",
    "     Returns:\n",
    "       P: latent matrix of users\n",
    "       Q: latent matrix of items\n",
    "       loss_train: vector of the different values of the loss function after each iteration on the train\n",
    "       loss_valid: vector of the different values of the loss function after each iteration not on valid\n",
    "       \"\"\"\n",
    "     train, valid = data[0], data[1]\n",
    "    nb_users, nb_items = len(train), len(train[0])\n",
    "\n",
    "    loss_train, loss_valid = [], []\n",
    "\n",
    "    P = np.random.rand(nb_users, features) * 0.1\n",
    "    Q = np.random.rand(nb_items, features) * 0.1\n",
    "    \n",
    "    for e in range(epochs):        \n",
    "        for u in range(nb_users):\n",
    "            for i in range(nb_items):\n",
    "\n",
    "                if train[u][i] > 0:\n",
    "                    error_ui = train[u][i] - prediction(P, Q, u, i)\n",
    "                    P, Q = sgd(error_ui, P, Q, u, i, features, lr, weigth_decay)\n",
    "                               \n",
    "        loss_train.append(loss(train, P, Q))\n",
    "        loss_valid.append(loss(valid, P, Q))\n",
    "        \n",
    "        if e % 10 == 0:\n",
    "            print('Epoch : ', \"{:3.0f}\".format(e+1), ' | Train :', \"{:3.3f}\".format(loss_train[-1]), \n",
    "                  ' | Valid :', \"{:3.3f}\".format(loss_valid[-1]))\n",
    "\n",
    "        if e > 1:\n",
    "            if abs(loss_valid[-1] - loss_valid[-2]) < stopping:\n",
    "                continue\n",
    "        \n",
    "    return P, Q, loss_train, loss_valid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NgT88hU5z8Bu"
   },
   "source": [
    "### 2.2.2 Loss function\n",
    "\n",
    "The loss function plays a decisive role in the construction of a predictive model. In fact, it is this same cost function that we will try to optimize by iteratively adjusting the values of the latent matrices $\\mathbf{P}$ and $\\mathbf{Q}$.\n",
    "\n",
    "Since we consider that the observed evaluations vary between 1 and 5, the mean squared error (MSE) seems an interesting first option. From a recommender system point of view, we will define the MSE as follows:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "MSE(\\mathbf{R}, \\hat{\\mathbf{R}}) = \\frac{1}{n} \\sum_{r_{ui} \\neq 0} (r_{ui} - \\hat{r}_{ui})^2, \n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where $\\mathbf{R}$ and $\\hat{\\mathbf{R}}$ are respectively the matrices of observed and predicted ratings <i>n</i> is the number of evaluations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OAPyYa9QPDpU"
   },
   "source": [
    "## Question 5\n",
    "\n",
    "Giả sử chúng ta muốn dự đoán đánh giá của người dùng <i>u</i> cho bộ phim <i>i</i>, nên làm thế nào? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 6\n",
    "\n",
    "Hàm `loss` sau đây bị sai một chi tiết quan trọng. Hãy sửa lại lỗi đó!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ft_CJd64z8Bu"
   },
   "outputs": [],
   "source": [
    "def prediction(P, Q, u, i):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        P: user matrix\n",
    "        Q: matrix of items\n",
    "        u: index associated with user u\n",
    "        i: index associated with item i\n",
    "    Returns:\n",
    "        pred: the predicted evaluation of the user u for the item i\n",
    "    \"\"\"\n",
    "    return np.dot(P[u,:], Q[i,:])\n",
    "\n",
    "def loss(data, P, Q):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "       data: ratings\n",
    "       P: matrix of users\n",
    "       Q: matrix of items   \n",
    "    Returns:\n",
    "        MSE: observed mean of squared errors \n",
    "    \"\"\"\n",
    "    errors_sum, nb_evaluations = 0., 0\n",
    "    nb_users, nb_items = len(data), len(data[0])\n",
    "\n",
    "    for u in range(nb_users):\n",
    "        for i in range(nb_items):\n",
    "        \n",
    "            if data[u][i] <= 0:\n",
    "                errors_sum += pow(data[u][i] - prediction(P, Q, u, i), 2)\n",
    "                nb_evaluations += 1\n",
    "                \n",
    "    return errors_sum / nb_evaluations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "10bI2dV5z8CQ"
   },
   "source": [
    "### 2.2.3 Estimation\n",
    " \n",
    "Parameters' estimates are directly associated with the loss function we are trying to minimize. With matrix factorization, two estimation techniques are available to calculate latent matrices $\\mathbf{P}$ and $\\mathbf{Q}$ respectively associated to users and items. In both cases, these techniques use the linearity of the matrix factorization model.\n",
    " \n",
    "#### Gradient descent\n",
    " \n",
    "First, we implement stochastic gradient descent (SGD); an iterative method that reviews all non-zero evaluations for each user. Formally, and remembering that the function we are trying to minimize is:\n",
    " \n",
    "$$\n",
    "\\begin{align}\n",
    "\\underset{p, q}{\\operatorname{min}} L(\\mathbf{R}, \\lambda) = \\underset{p, q}{\\operatorname{min}} \\sum_{r_{ui} \\neq 0} \\{(r_{ui} - \\langle p_u, q_i \\rangle)^2 + \\lambda \\cdot (||p_u||^2 + ||q_i||^2)\\},\n",
    "\\end{align}\n",
    "$$\n",
    " \n",
    "we calculate the gradients of the previous equation as a function of $p_u$ and $q_i$:\n",
    " \n",
    "$$\n",
    "\\nabla_{p_{u}} L(\\mathbf{R}, \\lambda) =  -2q_{i} \\cdot \\epsilon_{ui} + 2\\lambda \\cdot p_{u} \\quad \\text{and} \\quad\n",
    "\\nabla_{q_{i}} L(\\mathbf{R}, \\lambda) =  -2p_{u} \\cdot \\epsilon_{ui} + 2\\lambda \\cdot q_{i},\n",
    "$$\n",
    " \n",
    "where we denote the error by:\n",
    " \n",
    "$$\n",
    "\\epsilon_{ui} = r_{ui} - \\hat{r}_{ui}.\n",
    "$$\n",
    " \n",
    "Finally, for each iteration, as long as the user-item rating is non-zero, we perform the following update on the latent vectors as follows:\n",
    " \n",
    "$$\n",
    "p_{u}^{(t+1)} \\leftarrow p_{u}^{(t)} + \\gamma \\cdot (2q_{i}^{(t)} \\cdot \\epsilon_{ui} - 2 \\lambda \\cdot p_{u}^{(t)}) \\\\\n",
    "q_{i}^{(t+1)} \\leftarrow q_{i}^{(t)} + \\gamma \\cdot (2p_{u}^{(t)} \\cdot \\epsilon_{ui} - 2 \\lambda \\cdot q_{i}^{(t)}),\n",
    "$$\n",
    " \n",
    "where $ p_{u}^{(t + 1)}$ is the value of $ p_{u}$ after the $t + 1$ iteration and where $\\gamma$ is the learning rate of the descent. That is, we take a step in the opposite direction of the gradients such that we minimize the loss function.\n",
    " \n",
    "#### Note on alternate least squares\n",
    " \n",
    "The second technique is based on Alternate Least Squares (ALS). This method is elegant in that it allows an analytical form. We will not implement it in this workshop.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8R7uiumPPLaq"
   },
   "source": [
    "Với các phương trình trên, ta có thể hoàn thành hàm `sgd` bên dưới để cập nhật các tham số mô hình $\\mathbf{P}$ và $\\mathbf{Q}$ như sau:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8ic_KU6uz8Ca"
   },
   "outputs": [],
   "source": [
    "def sgd(error, P, Q, id_user, id_item, features, lr, weigth_decay):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        error: difference between observed and predicted evaluation (in that order)\n",
    "        P: matrix of users\n",
    "        Q: matrix of items\n",
    "        id_user: id_user\n",
    "        id_item: id_item\n",
    "        features: number of latent variables\n",
    "        lr: learning for the descent of the gradient\n",
    "        weigth_decay: scalar multiplier controlling the influence of the regularization term\n",
    "       \n",
    "     Returns:\n",
    "        P: the new estimate for P\n",
    "        Q: the new estimate for Q\n",
    "     \"\"\"    \n",
    "    for f in range(features):\n",
    "        P[id_user, f] = P[id_user, f] + lr * (2 * Q[id_item, f] * error - 2 * weigth_decay * P[id_user, f])\n",
    "        Q[id_item, f] = Q[id_item, f] + lr * (2 * P[id_user, f] * error - 2 * weigth_decay * Q[id_item, f])\n",
    "    return P, Q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7IwjGI_Bz8Ce"
   },
   "source": [
    "## 2.3 Training\n",
    "\n",
    "Now that matrix factorization is implemented, we can begin to train the model with different parameters and hyperparameters. The idea here is not to adjust the parameters in such a way as to obtain the best model, but simply to understand the role that they can play, both from the point of view of overfitting and computing time. In fact, there are very few wrong answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OOMVmv6-z8DQ"
   },
   "outputs": [],
   "source": [
    "features = 5\n",
    "lr = 0.02\n",
    "epochs = 101\n",
    "weigth_decay = 0.02\n",
    "stopping = 0.01\n",
    "\n",
    "P, Q, loss_train, loss_valid = learn_to_recommend(train, features, lr, epochs, weigth_decay, stopping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XAs0VXIoz8DS"
   },
   "source": [
    "Once the model is trained, we can visualize the different learning curves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qGJBjTy9z8DT"
   },
   "outputs": [],
   "source": [
    "x = list(range(len(loss_train)))\n",
    "k=0\n",
    "\n",
    "sns.set(rc={'figure.figsize':(12,8)})\n",
    "sns.set(font_scale = 1.5)\n",
    "\n",
    "plt.plot(x[-k:], loss_train[-k:], 'r', label=\"Train\")\n",
    "plt.plot(x[-k:], loss_valid[-k:], 'g', label=\"Validation\")\n",
    "plt.title('Learning curves')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('MSE')\n",
    "leg = plt.legend(loc='best', shadow=True, fancybox=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZdVEiUmyz8DV"
   },
   "source": [
    "## Question 7\n",
    " \n",
    "Hỏi nếu vòng lặp thực hiện nhiều lần thì sẽ gặp phải vấn đề gì?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 7."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "N7hI0Mzqz8DV"
   },
   "source": [
    "Bây giờ mô hình của chúng ta đã được đào tạo, hãy đánh giá hiệu suất cuối cùng của mô hình.\n",
    "\n",
    "## Question 8\n",
    " \n",
    "Hãy điền vào dòng code bên dưới để in ra giá trị lost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 8.\n",
    "loss(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VY2re70Nz8DZ"
   },
   "source": [
    "## 2.4 Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "H9vsE0-Iz8Da"
   },
   "source": [
    "### 2.4.1 Exploring latent layers\n",
    " \n",
    "Thanks to matrix factorization, it is possible to explore the various latent variables associated with users and items. By the nature of the matrices $\\mathbf{P}$ and $\\mathbf{Q}$, we can explore the <i>k</i> latent variables by looking at the columns of $\\mathbf{P}$ and $\\mathbf{Q}$.\n",
    " \n",
    "For example, suppose that the first two latent variables in $\\mathbf{Q}$ are the following, where each value in these arrays corresponds to an item:\n",
    " \n",
    "$$\n",
    "\\begin{align}\n",
    "q_1 &= [-1.0, \\ -0.8, \\ 0.0, \\ 1.0, \\ 0.5, \\ ...]\n",
    "\\qquad \\text {and} \\qquad\n",
    "q_2 = [-1.0, \\ 0.8, \\ 1.0, \\ 0.5, \\ -0.8, \\ ...].\n",
    "\\end{align}\n",
    "$$\n",
    " \n",
    "Let's suppose that these underlying items correspond to the following movies:\n",
    " \n",
    "1. The Room (2003),\n",
    "2. Star Wars: Attack of the clones (2002),\n",
    "3. Titanic (1997),\n",
    "4. Citizen Kane (1954),\n",
    "5. The Nigthmare before Christmas (1993).\n",
    " \n",
    "By mapping these movies according to the associated values ​​of the first two latent variables, we obtain the following graph:\n",
    " \n",
    "<img src = \"https://user-images.githubusercontent.com/13997178/91663551-9486d300-eab7-11ea-8e9f-c58398eff9fe.png\" width = \"500\">\n",
    " \n",
    "When doing this, we might begin to see some patterns around what each latent variable might correspond to. In this case, we could imagine that the first latent variable here is associated with how well received the movie was, while the second variable may detect the presence of a superstar.\n",
    " \n",
    "This might be an interesting hypothesis! Let's see if we see similar patterns when we look at the values associated with the matrix of users, $ \\mathbf{P}$.\n",
    " \n",
    "Suppose now that the first two latent variables of the $\\mathbf{P}$ user matrix have the following values:\n",
    " \n",
    "$$\n",
    "\\begin{align}\n",
    "p_1 &= [1.0, \\ 0.0, \\ -0.5, \\ 1.0, \\ -1.0, \\ ...]\n",
    "\\qquad \\text{and} \\qquad\n",
    "p_2 = [1.0, \\ 0.0, \\ 0.5, \\ -1.0, \\ -0.8, \\ ...]\n",
    "\\end{align}\n",
    "$$\n",
    " \n",
    "And that that each value in these two arrays correspond to the following users.\n",
    " \n",
    "1. Serena,\n",
    "2. Kali,\n",
    "3. Neil,\n",
    "4. Mary,\n",
    "5. David.\n",
    " \n",
    "We will now map the users according to the values associated with $ p_1 $ and $ p_2 $ vectors. Note, here we considered the same two latent factors, such that we can investigate how they compare to the characterization of the axes that we obtained before:\n",
    " \n",
    "<img src = \"https://user-images.githubusercontent.com/13997178/91663557-9e103b00-eab7-11ea-9ee1-0de6a5ac3760.png\" width = \"500\">\n",
    " \n",
    " \n",
    " \n",
    "This approach could allow us to suggest new movies that have never been evaluated by users simply based on certain characteristics. For example, there is a good chance that Serena loves the upcoming Scorsese <i> The Irish man </i> movie and that Neil looks forward to the new <i> Cat </i>.\n",
    "We will now propose a function that will facilitate the exploration of latent variables.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yDEWyWn_z8Db"
   },
   "outputs": [],
   "source": [
    "def explore(movie_titles, latent_matrix, frequency_mask, factor_idx, k):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "       movie_titles: Pandas Series containing movie titles\n",
    "        latent_matrix: matrix containing the model parameters for movies\n",
    "        frequency_mask: boolean array masking non-frequent movies\n",
    "        factor_idx: index of the latent variable\n",
    "        k: number of movies to show\n",
    "\n",
    "    Returns:\n",
    "        names: movie titles\n",
    "        scores: associated predicted ratings of movies\n",
    "    \"\"\"\n",
    "\n",
    "    # slice the column to obtain latent variable, then apply mask\n",
    "    latent_variable = latent_matrix[:, factor] * frequency_mask\n",
    "\n",
    "    # filter out infrequent movies\n",
    "    nonzero_indices = np.nonzero(latent_variable)\n",
    "    movies = np.array(movie_titles)[nonzero_indices][:k]\n",
    "    latent_variable = latent_variable[nonzero_indices][:k]\n",
    "\n",
    "    return movies, latent_variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now go ahead and use this function to visualize the results. \n",
    "We will do this while only considering movies that have been watched by\n",
    "at least 10% of all users. To do this, let's use the movie_popularity list we created earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(movie_popularity)\n",
    "# print(movie_popularity.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nqSJGMbfz8Dg"
   },
   "outputs": [],
   "source": [
    "k = 5\n",
    "factor = 0\n",
    "threshold = 0.1\n",
    "names, scores = explore(movies['Title'], Q, np.where(movie_popularity > threshold, 1, 0), factor, k)\n",
    "\n",
    "df = pd.DataFrame(np.matrix((names, scores)).T, (np.arange(len(scores)) + 1).tolist())\n",
    "df.columns = ['Title', 'Latent factor']\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nZuzfJHfz8Dg"
   },
   "source": [
    "## Question 9\n",
    "\n",
    "Hỏi các latent variables có thể diễn giải được không?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 9."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bNCN7hBpz8Di"
   },
   "source": [
    "# 3. Applications\n",
    " \n",
    "Congratulations on making it so far and on building your first recommender system! We can now put our model to use and generate personalized recommendations for users. In this section we will do just that.\n",
    " \n",
    "We will choose a given user, and consider her preferences. Then we will generate recommendations for her using our trained matrix factorization model with the caveat in mind that we only want to recommend movies that she has not seen in the past.\n",
    " \n",
    "Với một người dùng nhất định, hãy tạo 10 đề xuất phim phù hợp nhất dựa trên sở thích của người dùng. Chúng tôi đã xác định các bước cần thiết để bạn thực hiện việc này. Bạn cũng có thể thấy hữu ích khi sử dụng hàm `rank_top_k` mà chúng tôi đã cung cấp bên dưới."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7sFPWtOez8Dj"
   },
   "outputs": [],
   "source": [
    "def rank_top_k(names, ratings, k=10):\n",
    "   \"\"\"\n",
    "   Example:\n",
    "   a, b = np.array(['a', 'b', 'c']), np.array([6, 1, 3])\n",
    "   a, b = rank_top_k(a, b, k=2)\n",
    "   >>> a\n",
    "   np.array('a', 'c')\n",
    "   >>> b\n",
    "   np.array([6, 3])\n",
    "   \"\"\"\n",
    "   # rank indices in descending order\n",
    "   ranked_ids = np.argsort(ratings)[::-1]\n",
    "   return names[ranked_ids][:k], ratings[ranked_ids][:k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PI2exdGgz8Dm"
   },
   "outputs": [],
   "source": [
    "user_id = 0\n",
    "top_k = 10\n",
    "# Step 1: Define the user's preferences using the training set.\n",
    "user_train = np.array(train[0][user_id])\n",
    "# Step 2: Define what movies the user has not seen yet.\n",
    "# We will only generate recommendations for these movies, so make sure we store this in an awway equal to the number of all movies.\n",
    "movies_not_seen = np.where(user_train == 0, 1, 0)\n",
    "# Step 3: Predict the user's ratings across all movies.\n",
    "estimates = np.dot(P[user_id, :], Q.T)\n",
    "# Step 4: Consider the estimated ratings for movies that were not seen by the user.\n",
    "unseen_movie_estimates = estimates * movies_not_seen\n",
    "# Step 5: Retrieve the top k recommendations for that user.\n",
    "recommendations, scores = rank_top_k(np.array(movies['Title']), unseen_movie_estimates, k=top_k)\n",
    "# Step 6: Show the title and associated latent feature of the recommendations\n",
    "df = pd.DataFrame(np.matrix((recommendations, scores)).T, (np.arange(10) + 1).tolist(), \n",
    "                  columns=['Title', 'Predicted rating'])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "16H76jlxz8Dq"
   },
   "source": [
    "It could also be interesting to recommend movies to a user considering not only her past movie ratings, but also her preference for various movie genres.\n",
    " \n",
    "## Question 10\n",
    " \n",
    "Hãy tưởng tượng người dùng của chúng ta đang duyệt qua một danh sách phim hoạt hình. Để cung cấp cho người dùng bộ phim phù hợp nhất, chúng ta có thể muốn giới hạn các đề xuất vào một thể loại cụ thể. Hãy hoàn chỉnh dòng code bên dưới:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pS63d_flz8Dq"
   },
   "outputs": [],
   "source": [
    "def recommend(user_id, data, P, Q, list_of_genre_names, movies_genre, genre):\n",
    "    \"\"\"\n",
    "    args:\n",
    "       user_id: user_id\n",
    "        data: user-item ratings\n",
    "        P: user matrix\n",
    "        Q: item matrix\n",
    "        list_of_genre_names: list of genre names\n",
    "        movies_genre: user's preference for genres\n",
    "        new: Boolean, do we want to make new recommendations or not?\n",
    "\n",
    "    Returns:\n",
    "        the best suggestions based on the genre of movie selected\n",
    "    \"\"\"\n",
    "\n",
    "    # TODO 10.\n",
    "    ranked_ids = ...\n",
    "    return np.array(predictions) * np.array(genre.T)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(movies_genre_name)\n",
    "print(movies_genre.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qVkOjfDhz8Du"
   },
   "outputs": [],
   "source": [
    "genre = \"Animation\"\n",
    "user_id = 1\n",
    "top_k = 5\n",
    " \n",
    "# Estimate recommendations\n",
    "estimates = recommend(user_id, train, P, Q, list_of_genre_names=movies_genre_name, movies_genre=movies_genre, genre=genre)\n",
    "recommendations, scores = rank_top_k(np.array(movies['Title']), estimates, k=top_k)\n",
    " \n",
    "# Presentation\n",
    "df = pd.DataFrame(np.matrix((recommendations, scores)).T, (np.arange(top_k) + 1).tolist(), columns = ['Title', 'Predicted rating'])\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Matrix Factorization tutorial with exercises",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
